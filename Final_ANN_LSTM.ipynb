{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_ANN_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-VbbtgVOqGWb",
        "colab": {}
      },
      "source": [
        "ENV = 'colab'  # 'colab'\n",
        "if ENV == 'colab':\n",
        "    !pip install -q PyDrive imbalanced-learn ipdb hyperopt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-qdJhI2pqGWf"
      },
      "source": [
        "# Rete neurale per extreme returns su 2 azioni - con sentiment\n",
        "\n",
        "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica. Qui faremo *solo* l'ottimizzazione degli iperparametri, l'addestramento finale con i parametri ottimali trovati sarà fatta in un altro notebook.\n",
        "\n",
        "Il flusso è il seguente:\n",
        "\n",
        "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
        "- [x] calcolo dei log returns\n",
        "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
        "- [x] creazione estremi al 95%\n",
        "- [x] oversampling con due possibili strategie: replicare le istanze positive, o replicarle con aggiunta di rumore gaussiano\n",
        "- [x] addestramento rete con hyperparameter optimization\n",
        "- [x] ripetizione di ottimizzazione iperparametri per tutte e due le azioni con aggiunta di sentiment\n",
        "\n",
        "Nell'altro notebook dovrò fare:\n",
        "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
        "- [ ] confronto con i risultati del modello probabilistico\n",
        "- [ ] conclusioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yIWuJMSBqGWg",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "import pickle\n",
        "import copy\n",
        "import pprint\n",
        "import uuid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas.testing as pt\n",
        "import scipy.integrate\n",
        "import sklearn.metrics as sm\n",
        "import sklearn.preprocessing as skpp\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import hyperopt as hy\n",
        "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
        "\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as pl\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "import seaborn as sns\n",
        "\n",
        "import ipdb\n",
        "\n",
        "%pdb on"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImRJwq-Fq4aj",
        "colab": {}
      },
      "source": [
        "if ENV == 'colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "joUuYwuYqGWj"
      },
      "source": [
        "Un po' di dichiarazioni utili per il seguito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRPdSilJqGWk",
        "colab": {}
      },
      "source": [
        "stock_type = ['min_vol', 'max_vol']\n",
        "return_type = ['pos', 'neg', 'abs']\n",
        "impact_type = ['no_sentiment', 'with_sentiment']\n",
        "q_type = '95'\n",
        "rs = 42  # random state\n",
        "MAX_EPOCHS = 1000\n",
        "\n",
        "stock_codes = {\n",
        "    'min_vol': '9CE4C7',\n",
        "    'max_vol': 'E28F22'\n",
        "}  # già trovate in Paper-azioni.ipynb\n",
        "\n",
        "stock_colors = {\n",
        "    'min_vol': 'palegoldenrod',\n",
        "    'max_vol': 'coral',\n",
        "}\n",
        "\n",
        "dataset_colors = {\n",
        "    'train': 'navy',\n",
        "    'validation': 'forestgreen',\n",
        "}\n",
        "\n",
        "# i giorni sono i primi disponibili in quel mese nei dati\n",
        "split_dates = {\n",
        "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
        "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
        "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
        "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
        "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
        "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
        "    'last_train': datetime.datetime(2017, 1, 3), \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIgaOP67qGWm"
      },
      "source": [
        "## 1. Importazione dei dati \n",
        "\n",
        "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R3FzgUr5qGWn",
        "colab": {}
      },
      "source": [
        "if ENV == 'colab':\n",
        "    data_path = '/gdrive/My Drive/OptiRisk Thesis/data'\n",
        "    base_path = '/gdrive/My Drive/OptiRisk Thesis/experiments/11_final_experiment'\n",
        "else:\n",
        "    data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
        "    base_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/experiments/11_final_experiment\"\n",
        "\n",
        "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
        "ta_dir = os.path.join(data_path, 'technical_features', 'features_all_years')\n",
        "impact_path = os.path.join(data_path, 'sentiment', 'impactFinal.csv')\n",
        "print(f\"BASE path: {base_path}\")\n",
        "print(f\"TA dir: {ta_dir}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TsmulRB3qGWp"
      },
      "source": [
        "Conversione delle date e settaggio dell'index del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aX80lWk8qGWq",
        "colab": {}
      },
      "source": [
        "prices = pd.read_csv(prices_path)\n",
        "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
        "prices.index = prices['date']\n",
        "prices.drop(columns=['date'], inplace=True)\n",
        "prices.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlBiVnJIqGWs"
      },
      "source": [
        "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
        "\n",
        "I prezzi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HyNlWyFnqGWt",
        "colab": {}
      },
      "source": [
        "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
        "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
        "prices_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B-TCT_OnqGWv"
      },
      "source": [
        "I volumi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EegGob7qGWv",
        "colab": {}
      },
      "source": [
        "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
        "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
        "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
        "volume_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hNWkwALBDN",
        "colab_type": "text"
      },
      "source": [
        "Ora carico l'impact score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGVl7dDLBDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "impact = pd.read_csv(impact_path)\n",
        "impact.loc[:, 'date'] = pd.to_datetime(impact['date'], format=\"%Y%m%d\")\n",
        "impact.index = impact['date']\n",
        "impact.drop(columns=['date'], inplace=True)\n",
        "impact.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg6LiL7tLBDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "impact_ts = impact.pivot(columns='companyId', values=['PosImpact', 'NegImpact'])\n",
        "impact_ts_no_nan = impact_ts.dropna(axis='columns', how='any', inplace=False)\n",
        "impact_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D11Ov9SBqGWy"
      },
      "source": [
        "Ora calcoliamo i log-returns, le direzioni, i volumi e gli impact:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-hSvXqxqGW0",
        "colab": {}
      },
      "source": [
        "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
        "directions_ts_no_nan = prices_ts_no_nan.diff(periods=1).iloc[1:, :]\n",
        "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
        "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
        "\n",
        "impact_ts_no_nan = impact_ts_no_nan.loc[prices_ts_no_nan.index]\n",
        "\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, directions_ts_no_nan.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, impact_ts_no_nan.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h3N3bQbJkr3c"
      },
      "source": [
        "Mi conviene creare una funzione che standardizzi le features, visto che poi ne avrò più di una (es: returns + volume)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rqZ2KaSJk2Kt",
        "colab": {}
      },
      "source": [
        "def only_train_notime(feature: pd.Series) -> pd.Series:\n",
        "    \"\"\"Just return the training part of a Series.\"\"\"\n",
        "    f = feature[np.logical_or(\n",
        "        feature.index < split_dates['subprime-crisis-halfway'],\n",
        "        np.logical_and(\n",
        "            feature.index >= split_dates['eu-debt-halfway'],\n",
        "            feature.index < split_dates['last_train']\n",
        "        )\n",
        "    )]\n",
        "\n",
        "    return f\n",
        "\n",
        "def standardize(feature: pd.Series) -> pd.Series:\n",
        "    \"\"\"Standardize a feature by computing the statistics on the training set.\"\"\"\n",
        "    # prendo solo la parte di training, perdendo ogni riferimento alla\n",
        "    # sequenza temporale\n",
        "    tmp_feature_train = only_train_notime(feature)\n",
        "\n",
        "    scaler = skpp.RobustScaler()\n",
        "    scaler.fit(tmp_feature_train.values.reshape(-1, 1))\n",
        "\n",
        "    result = pd.Series(\n",
        "        data=scaler.transform(feature.values.reshape(-1, 1)).flatten(),\n",
        "        index=feature.index\n",
        "    )\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVoCHZZurG4j"
      },
      "source": [
        "Ora creo i thresholds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soghP5HhqGW3",
        "colab": {}
      },
      "source": [
        "# ora creo i dati per i returns (non standardizzati), i thresholds e i volumi (standardizzati)\n",
        "lr_train_notime = dict()\n",
        "lr_test_notime = dict()\n",
        "returns_train_notime = dict()\n",
        "\n",
        "# aggiungiamo i dati in modalità taglia-e-cuci\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    # training set\n",
        "    lr_current = log_returns.loc[:, s_code]\n",
        "    lr_train_notime[s_type] = only_train_notime(lr_current)\n",
        "    \n",
        "    # returns train, tutti POSITIVI\n",
        "    returns_train_notime[s_type] = {\n",
        "        'pos': lr_train_notime[s_type][lr_train_notime[s_type] > 0.0],\n",
        "        'neg': -(lr_train_notime[s_type][lr_train_notime[s_type] < 0.0]),\n",
        "        'abs': lr_train_notime[s_type].abs()\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "# ora creo i threshold\n",
        "thresholds = {\n",
        "    s_type: {\n",
        "        ret_type: {\n",
        "            q_type: returns_train_notime[s_type][ret_type].quantile(0.95)\n",
        "        }\n",
        "        for ret_type in return_type\n",
        "    }\n",
        "    for s_type in stock_type\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lr51ts-sjf6b"
      },
      "source": [
        "ed infine creo i DataFrame e gli arrays che contengono tutti gli estremi e tutti i dati.\n",
        "\n",
        "Le features che qui utilizziamo sono:\n",
        "\n",
        "- log-returns standardizzati\n",
        "- volume scambiato standardizzato\n",
        "- tutte le features di TA che ci sono nel white paper di Douglas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzG8axsrrSSa",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {s_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[s_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[s_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[s_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "    print(\"adding positive and negative sentiment impact\")\n",
        "    features[s_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "    features[s_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]\n",
        "    \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90lESpICqGW9",
        "colab": {}
      },
      "source": [
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[s_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[s_type][name] for name in feature_names] + \\\n",
        "              [features[s_type]['pos_impact'], features[s_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names + ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[s_type] = tmp_df\n",
        "    extremes_all[s_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[s_type]['pos'][q_type],\n",
        "        lr <= -thresholds[s_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[s_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SbKxpt4zqGW8"
      },
      "source": [
        "## 2. Creazione dataset train-test per TensorFlow\n",
        "\n",
        "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
        "\n",
        "- X: cubo dati\n",
        "- y: estremo si/no\n",
        "\n",
        "Per prima cosa, creo delle funzioni che mi creano i dati:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sg7SbAHBqGXA",
        "colab": {}
      },
      "source": [
        "# testata, funziona con array, Series e DataFrame\n",
        "def rolling_window(data: np.ndarray,\n",
        "                   start: int,\n",
        "                   end: int,\n",
        "                   lookback: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a rolling window view of data, starting at index start, finishing\n",
        "    at index end, with loockback days of bptt.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: series, dataframe or array\n",
        "        the data, containing one row for each time point and one column for each feature\n",
        "        \n",
        "    start: int\n",
        "        starting index in the data\n",
        "        \n",
        "    end: int\n",
        "        index where the whole thing ends, data[end] is **excluded**\n",
        "        \n",
        "    lookback: int\n",
        "        length of the lookback period\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    X: np.ndarray\n",
        "        array of shape(n_points, lookback, n_features)\n",
        "    \"\"\"\n",
        "    assert lookback < data.shape[0]  # lookback sano\n",
        "    assert start - lookback + 1 >= 0  # lookback sano\n",
        "    \n",
        "    n_features = data.shape[1]\n",
        "    n_points = end - start\n",
        "    \n",
        "    X = np.zeros((n_points, lookback, n_features), dtype = data.dtype)\n",
        "    \n",
        "    # range strano per l'indicizzazione numpy\n",
        "    for i, t in enumerate(range(start + 1, end + 1)):\n",
        "        X[i, :, :] = data[t - lookback:t, :]\n",
        "        \n",
        "    return X\n",
        "\n",
        "\n",
        "# testata, funziona hehehe\n",
        "def rolling_window_xyd(data: Union[pd.Series, pd.DataFrame],\n",
        "                      targets: List[pd.Series],\n",
        "                      start: int,\n",
        "                      end: int,\n",
        "                      lookback: int) -> Tuple[np.ndarray, List[np.ndarray], pd.Series]:\n",
        "    \"\"\"\n",
        "    Create X, y and dates in a single shot.\n",
        "    The returned dates are relative to the y array(s).\n",
        "    \"\"\"\n",
        "    if isinstance(data, pd.Series):\n",
        "        my_data = data.values.reshape(-1, 1)\n",
        "    elif isinstance(data, pd.DataFrame):\n",
        "        my_data = data.values\n",
        "    else:\n",
        "        raise TypeError(\"data should be a pandas Series or Dataframe\")\n",
        "\n",
        "    X = rolling_window(my_data, start, end, lookback)\n",
        "    \n",
        "    if not isinstance(targets, list):\n",
        "        raise TypeError(\"target must be a list of pandas Series\")\n",
        "    if not all(isinstance(t, pd.Series) for t in targets):\n",
        "        raise TypeError(\"all targets should be pandas Series\")\n",
        "    if not all(isinstance(t.index, pd.DatetimeIndex) for t in targets):\n",
        "        raise TypeError(\"index of target should be a pandas DatetimeIndex\")\n",
        "        \n",
        "    y = [t.values[start + 1:end + 1] for t in targets]\n",
        "    dates = pd.Series(data=targets[0].index[start + 1: end + 1])\n",
        "        \n",
        "    return X, y, dates\n",
        "\n",
        "\n",
        "# TESTATO: funziona\n",
        "def create_Xyd(returns: Union[pd.Series, pd.DataFrame],\n",
        "               extremes: pd.Series,\n",
        "               directions: pd.Series,\n",
        "               lookback: int) -> Tuple[\n",
        "    np.ndarray, np.ndarray, List[np.ndarray], List[np.ndarray], pd.Series, pd.Series\n",
        "]:\n",
        "    \"\"\"\n",
        "    Create the X, y and dates arrays for the ANN.\n",
        "    \"\"\"\n",
        "    test_start_1 = returns.index.get_loc(split_dates['subprime-crisis-halfway'])\n",
        "    test_end_1 = returns.index.get_loc(split_dates['eu-debt-halfway'])\n",
        "    test_start_2 = returns.index.get_loc(split_dates['last_train'])\n",
        "\n",
        "    # TRAIN\n",
        "    tmp_X_train_1, tmp_y_train_1, tmp_dates_train_1 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=lookback - 1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_start_1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "\n",
        "    tmp_X_train_2, tmp_y_train_2, tmp_dates_train_2 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_end_1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_start_2,\n",
        "        lookback=lookback\n",
        "    )\n",
        "    \n",
        "    assert len(tmp_y_train_1) == len(tmp_y_train_2)\n",
        "    \n",
        "    X_train = np.concatenate([tmp_X_train_1, tmp_X_train_2])\n",
        "    y_train = [np.concatenate([tmp_y_train_1[i], tmp_y_train_2[i]]) for i in range(len(tmp_y_train_1))]\n",
        "    dates_train = pd.concat([tmp_dates_train_1, tmp_dates_train_2], axis=0, ignore_index=True).values\n",
        "    assert X_train.shape[0] == dates_train.shape[0]\n",
        "    assert all(yy.shape[0] == X_train.shape[0] for yy in y_train)\n",
        "\n",
        "    # TEST\n",
        "    tmp_X_test_1, tmp_y_test_1, tmp_dates_test_1 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_start_1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_end_1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "    \n",
        "    tmp_X_test_2, tmp_y_test_2, tmp_dates_test_2 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_start_2,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=returns.shape[0] - 1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "  \n",
        "    X_test = np.concatenate([tmp_X_test_1, tmp_X_test_2])\n",
        "    y_test = [np.concatenate([tmp_y_test_1[i], tmp_y_test_2[i]]) for i in range(len(tmp_y_test_1))]\n",
        "    dates_test = pd.concat([tmp_dates_test_1, tmp_dates_test_2], axis=0, ignore_index=True).values\n",
        "    assert X_test.shape[0] == dates_test.shape[0]\n",
        "    assert all(yy.shape[0] == X_test.shape[0] for yy in y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "\n",
        "\n",
        "def split_stratified(X: np.ndarray,\n",
        "                     y: List[np.ndarray],\n",
        "                     dates: np.ndarray,\n",
        "                     test_size=0.2,\n",
        "                     random_state=rs,\n",
        "                     verbose=False):\n",
        "    \"\"\"\n",
        "    Split a dataset in a stratified fashion on the target variable y[0].\n",
        "    \"\"\"\n",
        "    assert X.ndim == 3\n",
        "    # divido in train-validation, lo faccio prendendo gli indici dagli estremi y/n con un\n",
        "    # ShuffleSplit che divide a caso\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    \n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[2]\n",
        "    \n",
        "    XX = np.zeros(n_samples, dtype=np.int8)\n",
        "    \n",
        "    if verbose:\n",
        "        for i in range(len(y)):\n",
        "            vals, counts = np.unique(y[i], return_counts=True)\n",
        "            for v, c in zip(vals, counts):\n",
        "                print(f\"y[{i}] has {c} elements of class {v}\")\n",
        "    \n",
        "    train_index, test_index = next(splitter.split(XX, y[0]))\n",
        "    \n",
        "    X_train = X[train_index]\n",
        "    X_validation = X[test_index]\n",
        "    \n",
        "    y_train = [yy[train_index] for yy in y]\n",
        "    y_validation = [yy[test_index] for yy in y]\n",
        "    \n",
        "    dates_train = dates[train_index]\n",
        "    dates_validation = dates[test_index]\n",
        "\n",
        "    return X_train, X_validation, y_train, y_validation, dates_train, dates_validation\n",
        "\n",
        "\n",
        "def oversample_mtl(X: np.ndarray, y: List[np.ndarray], random_state=rs, dt=np.float32):\n",
        "    \"\"\"Oversample a dataset on the positive 1 class.\"\"\"\n",
        "    assert X.dtype == dt\n",
        "    assert X.ndim == 3\n",
        "    assert isinstance(y, list) and all(yy.ndim == 1 for yy in y) and all(yy.dtype == dt for yy in y)\n",
        "    \n",
        "    # oversample\n",
        "    ro = RandomOverSampler(random_state=random_state)\n",
        "    nx = X.shape[0]\n",
        "    indexes = np.arange(nx).reshape(nx, 1)\n",
        "    \n",
        "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y[0])\n",
        "    ir = indexes_resampled.flatten()\n",
        "    \n",
        "    X_resampled = X[ir]\n",
        "    y_resampled = [yy[ir] for yy in y]\n",
        "    \n",
        "    return X_resampled, y_resampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UVy3gkvqGXG"
      },
      "source": [
        "## 3. Addestramento reti\n",
        "\n",
        "Creo delle funzioni che mi aiutino ad addestrare e valutare i diversi modelli:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gyNkr3CjOH6t",
        "colab": {}
      },
      "source": [
        "def create_model_mtl(space: Dict[str, Any],\n",
        "                     bptt: int,\n",
        "                     n_features: int) -> keras.models.Model:\n",
        "    \"\"\"Create a model using the parameters in the search space.\"\"\"\n",
        "    l = space['layers']\n",
        "\n",
        "    input_dropout = float(l['input_dropout'])\n",
        "    assert input_dropout >= 0.0 and input_dropout <= 1.0\n",
        "\n",
        "    n_layers = int(l['num_layers']['how_many'])\n",
        "    assert n_layers <= 2 and n_layers > 0\n",
        "\n",
        "#     n_cells_1 = int(l['num_layers']['n_cells_1'])\n",
        "#     assert n_cells_1 >= 1\n",
        "\n",
        "    # creo il modello\n",
        "    model_input = keras.Input(shape=(bptt, n_features), name='model_input')\n",
        "\n",
        "    if n_layers == 1:\n",
        "        if input_dropout > 0.0:\n",
        "            x = keras.layers.LSTM(n_features, dropout=input_dropout)(model_input)\n",
        "        else:\n",
        "            x = keras.layers.LSTM(n_features)(model_input)\n",
        "    elif n_layers == 2:\n",
        "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
        "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
        "        x = keras.layers.LSTM(n_cells_2)(x)\n",
        "    elif n_layers == 3:\n",
        "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
        "        n_cells_3 = int(l['num_layers']['n_cells_3'])\n",
        "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
        "        x = keras.layers.LSTM(n_cells_2, return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(n_cells_3)(x)\n",
        "\n",
        "    output_is_extreme = keras.layers.Dense(\n",
        "        2, activation='softmax', name='extreme')(x)\n",
        "    output_is_up_down = keras.layers.Dense(\n",
        "        2, activation='softmax', name='up_down')(x)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=model_input,\n",
        "        outputs=[output_is_extreme, output_is_up_down],\n",
        "        name='MTL_model')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd_1r30NQ2BT",
        "colab_type": "text"
      },
      "source": [
        "Creo una funzione che crea ed addestra il modello che voglio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnQnax_lQ1ZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(space,\n",
        "                max_epochs: int,\n",
        "                data: Union[pd.Series, pd.DataFrame],\n",
        "                extremes: pd.Series,\n",
        "                directions: pd.Series,\n",
        "                verbose: int):\n",
        "    \"\"\"Train a model with the supplied parameters\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    space: \n",
        "        hyperopt search space\n",
        "\n",
        "    max_epochs: int\n",
        "        number of max epochs to tun the model for\n",
        "        \n",
        "    data: pd.Series of shape (n_timepoints,), or pd.DataFrame of shape (n_timepoints, n_features)\n",
        "        data containing returns, volume and all other things, where every row is\n",
        "        a timepoint and every column a different feature\n",
        "        \n",
        "    extremes: pd.Series of shape (n_timepoints,)\n",
        "        target for the extremes, binary 1/0, \n",
        "        \n",
        "    directions: pd.Series of shape (n_timepoints,)\n",
        "        target for the directions\n",
        "        \n",
        "    verbose: int\n",
        "        verbosity for Keras\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: keras.Model\n",
        "        the trained model\n",
        "    history: keras.history\n",
        "        training history of the model\n",
        "    \n",
        "    X_train_bal: np.ndarray\n",
        "        balanced training set\n",
        "    \n",
        "    X_validation: np.ndarray\n",
        "        the validation set\n",
        "        \n",
        "    X_test: np.ndarray\n",
        "        the test set\n",
        "        \n",
        "    y_train_bal: list of np.ndarray\n",
        "        balanced targets for the training set\n",
        "        \n",
        "    y_validation: list of np.ndarray\n",
        "        targets for the validation set\n",
        "    \n",
        "    y_test: list of np.ndarray\n",
        "        targets for the test set\n",
        "    \"\"\"\n",
        "    sigmoid_or_softmax = 'softmax'\n",
        "    if data.ndim == 1:\n",
        "        n_features = 1\n",
        "    else:\n",
        "        n_features = data.shape[1]\n",
        "\n",
        "    lookback = bptt = int(space['bptt'])\n",
        "    batch_size = data.shape[0]\n",
        "    \n",
        "    # 1. creazione dataset per questo lookback\n",
        "    X_trv, X_test, y_trv, y_test, dates_trv, dates_test = create_Xyd(\n",
        "        data.astype(np.float32),\n",
        "        extremes.astype(np.float32),\n",
        "        directions.astype(np.float32),\n",
        "        lookback=lookback\n",
        "    )\n",
        "\n",
        "    # divido in train-validation\n",
        "    X_train, X_validation, y_train, y_validation, dates_train, dates_validation = split_stratified(\n",
        "        X_trv,\n",
        "        y_trv,\n",
        "        dates_trv,\n",
        "        test_size=0.2,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # 2. bilancio con oversampling della classe di minoranza (1)\n",
        "    X_train_bal, y_train_bal = oversample_mtl(X_train, y_train)  # bal = balanced\n",
        "\n",
        "    # 3. creo le variabili che servono per il training (dati e parametri)\n",
        "    try:\n",
        "        use_class_weight = space['use_class_weight']\n",
        "        if use_class_weight:\n",
        "            print(\"Using class weight for training\")\n",
        "    except KeyError:\n",
        "        use_class_weight = False\n",
        "        \n",
        "    \n",
        "    y_train_bal_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_train_bal]\n",
        "    y_validation_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_validation]\n",
        "    y_test_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_test]\n",
        "\n",
        "    # 4. inizializza le loss a 0 e crea i tempi di inizio e l'id esperimento\n",
        "    optimizer_name = space['optimizer']['name']\n",
        "    assert optimizer_name in {'adam', 'adadelta'}\n",
        "\n",
        "    start_time = int(round(time.time()))\n",
        "    experiment_id = str(uuid.uuid4())\n",
        "\n",
        "    # 5. addestra il modello\n",
        "    model = create_model_mtl(space, lookback, n_features)\n",
        "\n",
        "    # 5.1 crea l'optimizer\n",
        "    if optimizer_name == 'adam':\n",
        "        learning_rate = space['optimizer']['lr']\n",
        "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    elif optimizer_name == 'adadelta':\n",
        "        optimizer = 'adadelta'\n",
        "        learning_rate = 1.0\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid optimizer name {optimizer_name}\")\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # 5.2 compila il modello\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
        "    )\n",
        "\n",
        "    # 5.3 parametri per l'Early Stopping\n",
        "    min_delta = float(space['early_stop']['min_delta'])\n",
        "    patience = int(space['early_stop']['patience'])\n",
        "\n",
        "    early_stop_cb = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=min_delta,\n",
        "        patience=patience,\n",
        "        restore_best_weights=True)\n",
        "\n",
        "    # 5.4 addestramento\n",
        "    print(\"Fitting model\")\n",
        "    history: keras.callbacks.History = model.fit(  # type: ignore\n",
        "        x=X_train_bal,\n",
        "        y=y_train_bal_cat,\n",
        "        epochs=max_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_validation, y_validation_cat),\n",
        "        callbacks=[early_stop_cb],\n",
        "        shuffle=True,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    return model, history, X_train_bal, X_validation, X_test, y_train_bal, y_validation, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_qSCv4cTQRj",
        "colab_type": "text"
      },
      "source": [
        "creo anche i dict dove salvare i risultati, mentre le funzioni di valutazione delle performance le metto alla fine, nel punto 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZ0kYr6T1Md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann_probabilities = dict()\n",
        "for imp_type in impact_type:\n",
        "    ann_probabilities[imp_type] = dict()\n",
        "\n",
        "best_spaces = copy.deepcopy(ann_probabilities)\n",
        "\n",
        "y_true = copy.deepcopy(ann_probabilities)\n",
        "recalls = copy.deepcopy(ann_probabilities)\n",
        "precisions = copy.deepcopy(ann_probabilities)\n",
        "fprs = copy.deepcopy(ann_probabilities)\n",
        "ksss = copy.deepcopy(ann_probabilities)\n",
        "losses = copy.deepcopy(ann_probabilities)\n",
        "utilities = copy.deepcopy(ann_probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T_tVJY9rOH66"
      },
      "source": [
        "### 3.1 Azione con minima volatilità - NO sentiment - rete migliore\n",
        "\n",
        "Cominciamo con l'azione meno volatile.\n",
        "\n",
        "La migliore combinazione di iperparametri è:\n",
        "\n",
        "- 19 neuroni di entrata\n",
        "- optimizer: Adadelta\n",
        "- early stopping con parametri:\n",
        "    - patience: 14\n",
        "    - min delta: 6e-4\n",
        "- dropout in input con rate 0.07024542157284551\n",
        "- lookback: 53"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BxKDWjIAqGXG",
        "colab": {}
      },
      "source": [
        "s_type = 'min_vol'\n",
        "imp_type = 'no_sentiment'  # one of 'no_sentiment', 'with_sentiment'\n",
        "model_path = os.path.join(base_path, 'results', 'models', f\"final_model_{s_type}_{imp_type}.h5\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_{imp_type}.pickle\")\n",
        "\n",
        "with open(best_filename, 'rb') as infile:\n",
        "    best = pickle.load(infile)\n",
        "\n",
        "print(f\"MODEL path: {model_path}\")\n",
        "print(f\"BEST path: {best_filename}\")\n",
        "print(f\"BEST: {best}\")\n",
        "\n",
        "best_spaces[imp_type][s_type] = {\n",
        "    'bptt': best['bptt_len'],\n",
        "    'early_stop': {\n",
        "        'min_delta': best['early_stop_min_delta'],\n",
        "        'patience': best['early_stop_patience']\n",
        "    },\n",
        "    'layers': {\n",
        "        'input_dropout': best['dropout_kill_rate'],\n",
        "        'num_layers': {\n",
        "            'how_many': 1,\n",
        "            'n_cells_1': 0,\n",
        "            'n_cells_2': 0\n",
        "        }\n",
        "    },\n",
        "    'optimizer': {\n",
        "        'name': 'adadelta'\n",
        "    },\n",
        "    'use_class_weight': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tB0NKm7PyrC",
        "colab_type": "text"
      },
      "source": [
        "Ora creo i dati per questo esperimento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8cizn6TP1nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {st_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[st_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[st_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[st_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "#    print(\"adding positive and negative sentiment impact\")\n",
        "#    features[st_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "#    features[st_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]   \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[st_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[st_type][name] for name in feature_names] #+ \\\n",
        "              #[features[st_type]['pos_impact'], features[st_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names #+ ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[st_type] = tmp_df\n",
        "    extremes_all[st_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[st_type]['pos'][q_type],\n",
        "        lr <= -thresholds[st_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[st_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[st_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSJrqM3GVYBC",
        "colab_type": "text"
      },
      "source": [
        "addestro quindi il modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlVb3a0OVXLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history, X_train_bal, X_validation, X_test, y_train_bal, y_validation, y_test = train_model(\n",
        "    best_spaces[imp_type][s_type],\n",
        "    MAX_EPOCHS,\n",
        "    data_all[s_type],\n",
        "    extremes_all[s_type][q_type],\n",
        "    directions_all[s_type],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\\n\\nSaving model to {model_path}\")\n",
        "model.save(model_path)\n",
        "\n",
        "# salvataggio di quello che mi serve per il confronto in un file numpy\n",
        "npz_path = os.path.join(base_path, 'datasets', f'final_{s_type}_{imp_type}.npz')\n",
        "print(f\"Saving data to {npz_path}\")\n",
        "np.savez(npz_path, **{\n",
        "    'X_train': X_train_bal,\n",
        "    'X_validation': X_validation,\n",
        "    'X_test': X_test,\n",
        "    'y_train': y_train_bal,\n",
        "    'y_validation': y_validation,\n",
        "    'y_test': y_test,\n",
        "})\n",
        "\n",
        "print(\"Adding to y_true\")\n",
        "# aggiunta a y_true e ann_probabilities\n",
        "y_true[imp_type][s_type] = {\n",
        "    'train': y_train_bal,\n",
        "    'validation': y_validation,\n",
        "    'test': y_test\n",
        "}\n",
        "\n",
        "print(\"Computing probabilities...\")\n",
        "ann_probabilities[imp_type][s_type] = {\n",
        "    'train': model.predict(X_train_bal, batch_size=X_train_bal.shape[0]),\n",
        "    'validation': model.predict(X_validation, batch_size=X_validation.shape[0]),\n",
        "    'test': model.predict(X_test, batch_size=X_test.shape[0]),\n",
        "}\n",
        "print(f\"Done for '{imp_type}' with stock '{s_type}'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPaECwjxeCYw",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Azione con massima volatilità - NO sentiment - rete migliore\n",
        "\n",
        "Cominciamo con l'azione meno volatile.\n",
        "\n",
        "La migliore combinazione di iperparametri è:\n",
        "\n",
        "- 19 neuroni di entrata\n",
        "- optimizer: Adadelta\n",
        "- early stopping con parametri:\n",
        "    - patience: 19\n",
        "    - min delta: 6e-4\n",
        "- dropout in input con rate 0.08215272160724836\n",
        "- lookback: 34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARSr8JJFeHbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_type = 'max_vol'\n",
        "imp_type = 'no_sentiment'  # one of 'no_sentiment', 'with_sentiment'\n",
        "model_path = os.path.join(base_path, 'results', 'models', f\"final_model_{s_type}_{imp_type}.h5\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_{imp_type}.pickle\")\n",
        "\n",
        "with open(best_filename, 'rb') as infile:\n",
        "    best = pickle.load(infile)\n",
        "\n",
        "print(f\"MODEL path: {model_path}\")\n",
        "print(f\"BEST path: {best_filename}\")\n",
        "print(f\"BEST: {best}\")\n",
        "\n",
        "best_spaces[imp_type][s_type] = {\n",
        "    'bptt': best['bptt_len'],\n",
        "    'early_stop': {\n",
        "        'min_delta': best['early_stop_min_delta'],\n",
        "        'patience': 30, #best['early_stop_patience']\n",
        "    },\n",
        "    'layers': {\n",
        "        'input_dropout': best['dropout_kill_rate'],\n",
        "        'num_layers': {\n",
        "            'how_many': 1,\n",
        "            'n_cells_1': 0,\n",
        "            'n_cells_2': 0\n",
        "        }\n",
        "    },\n",
        "    'optimizer': {\n",
        "        'name': 'adadelta'\n",
        "    },\n",
        "    'use_class_weight': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX6mWPp9egwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {st_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[st_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[st_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[st_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "#    print(\"adding positive and negative sentiment impact\")\n",
        "#    features[st_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "#    features[st_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]   \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[st_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[st_type][name] for name in feature_names] #+ \\\n",
        "              #[features[st_type]['pos_impact'], features[st_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names #+ ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[st_type] = tmp_df\n",
        "    extremes_all[st_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[st_type]['pos'][q_type],\n",
        "        lr <= -thresholds[st_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[st_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[st_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHccCrpAeolO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history, X_train_bal, X_validation, X_test, y_train_bal, y_validation, y_test = train_model(\n",
        "    best_spaces[imp_type][s_type],\n",
        "    MAX_EPOCHS,\n",
        "    data_all[s_type],\n",
        "    extremes_all[s_type][q_type],\n",
        "    directions_all[s_type],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\\n\\nSaving model to {model_path}\")\n",
        "model.save(model_path)\n",
        "\n",
        "# salvataggio di quello che mi serve per il confronto in un file numpy\n",
        "npz_path = os.path.join(base_path, 'datasets', f'final_{s_type}_{imp_type}.npz')\n",
        "print(f\"Saving data to {npz_path}\")\n",
        "np.savez(npz_path, **{\n",
        "    'X_train': X_train_bal,\n",
        "    'X_validation': X_validation,\n",
        "    'X_test': X_test,\n",
        "    'y_train': y_train_bal,\n",
        "    'y_validation': y_validation,\n",
        "    'y_test': y_test,\n",
        "})\n",
        "\n",
        "print(\"Adding to y_true\")\n",
        "# aggiunta a y_true e ann_probabilities\n",
        "y_true[imp_type][s_type] = {\n",
        "    'train': y_train_bal,\n",
        "    'validation': y_validation,\n",
        "    'test': y_test\n",
        "}\n",
        "\n",
        "print(\"Computing probabilities...\")\n",
        "ann_probabilities[imp_type][s_type] = {\n",
        "    'train': model.predict(X_train_bal, batch_size=X_train_bal.shape[0]),\n",
        "    'validation': model.predict(X_validation, batch_size=X_validation.shape[0]),\n",
        "    'test': model.predict(X_test, batch_size=X_test.shape[0]),\n",
        "}\n",
        "print(f\"Done for '{imp_type}' with stock '{s_type}'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po354xkukn8Y",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Azione con minima volatilità - WITH sentiment - rete migliore\n",
        "\n",
        "Cominciamo con l'azione meno volatile.\n",
        "\n",
        "La migliore combinazione di iperparametri è:\n",
        "\n",
        "- 21 neuroni di entrata (19 + 2 sentiment)\n",
        "- optimizer: Adam, con lr: 0.008646840787170844\n",
        "- early stopping con parametri:\n",
        "    - patience: 21\n",
        "    - min delta: 0.0078000000000000005\n",
        "- dropout in input con rate 0.031203212683827974\n",
        "- lookback: 75"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Ae8hTmlTXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_type = 'min_vol'\n",
        "imp_type = 'with_sentiment'  # one of 'no_sentiment', 'with_sentiment'\n",
        "model_path = os.path.join(base_path, 'results', 'models', f\"final_model_{s_type}_{imp_type}.h5\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_{imp_type}_2_layers.pickle\")\n",
        "\n",
        "with open(best_filename, 'rb') as infile:\n",
        "    best = pickle.load(infile)\n",
        "\n",
        "print(f\"MODEL path: {model_path}\")\n",
        "print(f\"BEST path: {best_filename}\")\n",
        "print(f\"BEST: {best}\")\n",
        "\n",
        "best_spaces[imp_type][s_type] = {\n",
        "    'bptt': best['bptt_len'],\n",
        "    'early_stop': {\n",
        "        'min_delta': best['early_stop_min_delta'],\n",
        "        'patience': best['early_stop_patience'],\n",
        "    },\n",
        "    'layers': {\n",
        "        'input_dropout': best['dropout_kill_rate'],\n",
        "        'num_layers': {\n",
        "            'how_many': 2,\n",
        "            'n_cells_1': 21,\n",
        "            'n_cells_2': best['number_of_cells_2'],\n",
        "        }\n",
        "    },\n",
        "    'optimizer': {\n",
        "        'name': 'adadelta',\n",
        "    },\n",
        "    'use_class_weight': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81styMkvlTsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {st_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[st_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[st_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[st_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "    print(\"adding positive and negative sentiment impact\")\n",
        "    features[st_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "    features[st_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]   \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[st_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[st_type][name] for name in feature_names] + \\\n",
        "              [features[st_type]['pos_impact'], features[st_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names + ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[st_type] = tmp_df\n",
        "    extremes_all[st_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[st_type]['pos'][q_type],\n",
        "        lr <= -thresholds[st_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[st_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[st_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMgRtgFjlT-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history, X_train_bal, X_validation, X_test, y_train_bal, y_validation, y_test = train_model(\n",
        "    best_spaces[imp_type][s_type],\n",
        "    MAX_EPOCHS,\n",
        "    data_all[s_type],\n",
        "    extremes_all[s_type][q_type],\n",
        "    directions_all[s_type],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\\n\\nSaving model to {model_path}\")\n",
        "model.save(model_path)\n",
        "\n",
        "# salvataggio di quello che mi serve per il confronto in un file numpy\n",
        "npz_path = os.path.join(base_path, 'datasets', f'final_{s_type}_{imp_type}.npz')\n",
        "print(f\"Saving data to {npz_path}\")\n",
        "np.savez(npz_path, **{\n",
        "    'X_train': X_train_bal,\n",
        "    'X_validation': X_validation,\n",
        "    'X_test': X_test,\n",
        "    'y_train': y_train_bal,\n",
        "    'y_validation': y_validation,\n",
        "    'y_test': y_test,\n",
        "})\n",
        "\n",
        "print(\"Adding to y_true\")\n",
        "# aggiunta a y_true e ann_probabilities\n",
        "y_true[imp_type][s_type] = {\n",
        "    'train': y_train_bal,\n",
        "    'validation': y_validation,\n",
        "    'test': y_test\n",
        "}\n",
        "\n",
        "print(\"Computing probabilities...\")\n",
        "ann_probabilities[imp_type][s_type] = {\n",
        "    'train': model.predict(X_train_bal, batch_size=X_train_bal.shape[0]),\n",
        "    'validation': model.predict(X_validation, batch_size=X_validation.shape[0]),\n",
        "    'test': model.predict(X_test, batch_size=X_test.shape[0]),\n",
        "}\n",
        "print(f\"Done for '{imp_type}' with stock '{s_type}'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrfZE45mecG",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Azione con massima volatilità - WITH sentiment - rete migliore\n",
        "\n",
        "Cominciamo con l'azione più volatile.\n",
        "\n",
        "La migliore combinazione di iperparametri è:\n",
        "\n",
        "- 21 neuroni di entrata (19 + 2 sentiment)\n",
        "- optimizer: Adadelta\n",
        "- early stopping con parametri:\n",
        "    - patience: 23\n",
        "    - min delta: 0.006\n",
        "- dropout in input con rate 0.05756409954550673\n",
        "- lookback: 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7h_u2I4mlL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_type = 'max_vol'\n",
        "imp_type = 'with_sentiment'  # one of 'no_sentiment', 'with_sentiment'\n",
        "model_path = os.path.join(base_path, 'results', 'models', f\"final_model_{s_type}_{imp_type}.h5\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_{imp_type}_2_layers.pickle\")\n",
        "\n",
        "with open(best_filename, 'rb') as infile:\n",
        "    best = pickle.load(infile)\n",
        "\n",
        "print(f\"MODEL path: {model_path}\")\n",
        "print(f\"BEST path: {best_filename}\")\n",
        "print(f\"BEST: {best}\")\n",
        "\n",
        "best_spaces[imp_type][s_type] = {\n",
        "    'bptt': best['bptt_len'],\n",
        "    'early_stop': {\n",
        "        'min_delta': best['early_stop_min_delta'],\n",
        "        'patience': best['early_stop_patience'],\n",
        "    },\n",
        "    'layers': {\n",
        "        'input_dropout': best['dropout_kill_rate'],\n",
        "        'num_layers': {\n",
        "            'how_many': 1,\n",
        "            'n_cells_1': 0,\n",
        "            'n_cells_2': 0\n",
        "        }\n",
        "    },\n",
        "    'optimizer': {\n",
        "        'name': 'adadelta',\n",
        "    },\n",
        "    'use_class_weight': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNhllTVjmpAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {st_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[st_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[st_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[st_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "    print(\"adding positive and negative sentiment impact\")\n",
        "    features[st_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "    features[st_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]   \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for st_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[st_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[st_type][name] for name in feature_names] + \\\n",
        "              [features[st_type]['pos_impact'], features[st_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names + ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[st_type] = tmp_df\n",
        "    extremes_all[st_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[st_type]['pos'][q_type],\n",
        "        lr <= -thresholds[st_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[st_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[st_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um8cuUn_mtX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history, X_train_bal, X_validation, X_test, y_train_bal, y_validation, y_test = train_model(\n",
        "    best_spaces[imp_type][s_type],\n",
        "    MAX_EPOCHS,\n",
        "    data_all[s_type],\n",
        "    extremes_all[s_type][q_type],\n",
        "    directions_all[s_type],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\\n\\nSaving model to {model_path}\")\n",
        "model.save(model_path)\n",
        "\n",
        "# salvataggio di quello che mi serve per il confronto in un file numpy\n",
        "npz_path = os.path.join(base_path, 'datasets', f'final_{s_type}_{imp_type}.npz')\n",
        "print(f\"Saving data to {npz_path}\")\n",
        "np.savez(npz_path, **{\n",
        "    'X_train': X_train_bal,\n",
        "    'X_validation': X_validation,\n",
        "    'X_test': X_test,\n",
        "    'y_train': y_train_bal,\n",
        "    'y_validation': y_validation,\n",
        "    'y_test': y_test,\n",
        "})\n",
        "\n",
        "print(\"Adding to y_true\")\n",
        "# aggiunta a y_true e ann_probabilities\n",
        "y_true[imp_type][s_type] = {\n",
        "    'train': y_train_bal,\n",
        "    'validation': y_validation,\n",
        "    'test': y_test\n",
        "}\n",
        "\n",
        "print(\"Computing probabilities...\")\n",
        "ann_probabilities[imp_type][s_type] = {\n",
        "    'train': model.predict(X_train_bal, batch_size=X_train_bal.shape[0]),\n",
        "    'validation': model.predict(X_validation, batch_size=X_validation.shape[0]),\n",
        "    'test': model.predict(X_test, batch_size=X_test.shape[0]),\n",
        "}\n",
        "print(f\"Done for '{imp_type}' with stock '{s_type}'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TzTD8h5jAaL6"
      },
      "source": [
        "## 4. Analisi dei risultati\n",
        "\n",
        "Analizziamo allora i risultati per confrontarli con quelli delle probabilità."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5nXyquVVsRW",
        "colab": {}
      },
      "source": [
        "def loss_function(theta, recall, fpr):\n",
        "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
        "    assert theta >= 0.0 and theta <= 1.0\n",
        "    \n",
        "    return theta * (1 - recall) + (1 - theta) * fpr\n",
        "\n",
        "\n",
        "def utility_function(theta, loss):\n",
        "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
        "    return min(theta, 1 - theta) - loss\n",
        "\n",
        "\n",
        "def to_binary(prob: np.ndarray, thresh: float):\n",
        "    assert thresh <= 1.0 and thresh >= 0.0\n",
        "    \n",
        "    return (prob >= thresh).astype(np.int8)\n",
        "\n",
        "\n",
        "#def recall_fpr_kss_precision(y_true, y_pred):\n",
        "#    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
        "#    tp = np.sum(np.logical_and(y_true, y_pred))\n",
        "#    tn = np.sum(np.logical_and(\n",
        "#        np.logical_not(y_true),\n",
        "#        np.logical_not(y_pred)\n",
        "#    ))\n",
        "#    fp = np.sum(np.logical_and(\n",
        "#        y_pred\n",
        "#        np.logical_not(y_true),\n",
        "#    ))\n",
        "#    fn = np.sum(np.logical_and(\n",
        "#        y_true,\n",
        "#        np.logical_not(y_pred)\n",
        "#    ))\n",
        "#    \n",
        "#    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
        "#    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
        "#    precision = tp / (tp + fp)\n",
        "#    \n",
        "#    kss = recall - fpr\n",
        "#    \n",
        "#    return recall, fpr, kss, precision\n",
        "\n",
        "\n",
        "def recall_fpr_kss_precision(y_true, y_pred):\n",
        "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
        "    cm = sm.confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    tp = cm[1, 1]\n",
        "    tn = cm[0, 0]\n",
        "    fp = cm[0, 1]\n",
        "    fn = cm[1, 0]\n",
        "    \n",
        "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
        "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
        "    precision = tp / (tp + fp)\n",
        "    \n",
        "    kss = recall - fpr\n",
        "    \n",
        "    return recall, fpr, kss, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fBtZN0LrZ4_R",
        "colab": {}
      },
      "source": [
        "def optimize_wt(w, theta, probabilities, y_true, verbose=False):\n",
        "    \"\"\"Get the best threshold for the class 1 probability.\"\"\"\n",
        "    recalls = np.zeros((w.shape[0], ), dtype=np.float64)\n",
        "    fprs = copy.deepcopy(recalls)\n",
        "    ksss = copy.deepcopy(recalls)\n",
        "    precisions = copy.deepcopy(recalls)\n",
        "    losses = copy.deepcopy(recalls)\n",
        "    utilities = copy.deepcopy(recalls)\n",
        "\n",
        "    for i, thresh in enumerate(w):\n",
        "        if i % 200 == 0 and verbose:\n",
        "            print(f\"iteration {i} / {len(w_t)}\")\n",
        "\n",
        "        y_pred = to_binary(probabilities, thresh).astype(np.int8)\n",
        "        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
        "        loss = loss_function(theta, recall, fpr)\n",
        "        utility = utility_function(theta, loss)\n",
        "\n",
        "        recalls[i] = recall\n",
        "        precisions[i] = precision\n",
        "        ksss[i] = kss\n",
        "        fprs[i] = fpr\n",
        "        losses[i] = loss\n",
        "        utilities[i] = utility\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Finished!\")\n",
        "\n",
        "    return recalls, fprs, ksss, precisions, losses, utilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdTmqrRbnDo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prima devo ottimizzare il w_t per ogni modello, e poi usarlo\n",
        "dataset_type = ['train', 'validation', 'test']\n",
        "w_t = np.arange(0, 1, 1e-3)\n",
        "theta = 0.5\n",
        "\n",
        "best_indexes = {\n",
        "    imp_type: {\n",
        "        s_type: dict()\n",
        "        for s_type in stock_type\n",
        "    }\n",
        "    for imp_type in impact_type\n",
        "}\n",
        "\n",
        "# per ogni impact (sentiment Y/N)\n",
        "for imp_type in impact_type:\n",
        "\n",
        "    # per ogni tipo di stock (min_vol/max_vol)\n",
        "    for s_type in stock_type:\n",
        "        recalls[imp_type][s_type] = dict()\n",
        "        fprs[imp_type][s_type] = dict()\n",
        "        ksss[imp_type][s_type] = dict()\n",
        "        precisions[imp_type][s_type] = dict()\n",
        "        losses[imp_type][s_type] = dict()\n",
        "        utilities[imp_type][s_type] = dict()\n",
        "        best_indexes[imp_type][s_type] = dict()\n",
        "\n",
        "        # per ogni dataset, ottimizzo sul VALIDATION set, è importante!\n",
        "        for data_type in dataset_type:\n",
        "            print(f\"Impact: {imp_type}\\tStock: {s_type}, dataset: {data_type}\")\n",
        "\n",
        "            probs = ann_probabilities[imp_type][s_type][data_type]\n",
        "            curr_y_true = y_true[imp_type][s_type][data_type][0]\n",
        "\n",
        "            if data_type in {'train', 'validation'}:  # solo su training e validation\n",
        "                tmp_recalls, tmp_fprs, tmp_ksss, tmp_precisions, tmp_losses, tmp_utilities = \\\n",
        "                optimize_wt(w_t, theta, probs[0][:, 1], curr_y_true, verbose=False)\n",
        "\n",
        "                recalls[imp_type][s_type][data_type] = tmp_recalls\n",
        "                fprs[imp_type][s_type][data_type] = tmp_fprs\n",
        "                ksss[imp_type][s_type][data_type] = tmp_ksss\n",
        "                precisions[imp_type][s_type][data_type] = tmp_precisions\n",
        "                losses[imp_type][s_type][data_type] = tmp_losses\n",
        "                utilities[imp_type][s_type][data_type] = tmp_utilities\n",
        "\n",
        "                best_indexes[imp_type][s_type][data_type] = np.argmax(utilities[imp_type][s_type][data_type])\n",
        "            elif data_type == 'test':\n",
        "                best_threshold = w_t[best_indexes[imp_type][s_type]['validation']]\n",
        "                y_pred = to_binary(probs[0][:, 1], best_threshold)\n",
        "                tmp_recall, tmp_fpr, tmp_kss, tmp_precision = recall_fpr_kss_precision(curr_y_true, y_pred)\n",
        "                tmp_loss = loss_function(theta, tmp_recall, tmp_fpr)\n",
        "                tmp_utility = utility_function(theta, tmp_loss)\n",
        "\n",
        "                recalls[imp_type][s_type][data_type] = tmp_recall\n",
        "                fprs[imp_type][s_type][data_type] = tmp_fpr\n",
        "                ksss[imp_type][s_type][data_type] = tmp_kss\n",
        "                precisions[imp_type][s_type][data_type] = tmp_precision\n",
        "                losses[imp_type][s_type][data_type] = tmp_loss\n",
        "                utilities[imp_type][s_type][data_type] = tmp_utility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v784XCniaQf",
        "colab_type": "text"
      },
      "source": [
        "per completezza calcolo anche il classification_report per ogni combinazione:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnx23BYsj_3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# per ogni impact (sentiment Y/N)\n",
        "for imp_type in impact_type:\n",
        "\n",
        "    # per ogni tipo di stock (min_vol/max_vol)\n",
        "    for s_type in stock_type:\n",
        "\n",
        "        # per ogni dataset, ottimizzo sul VALIDATION set, è importante!\n",
        "        for data_type in dataset_type:\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"\\nImpact: {imp_type}\\tStock: {s_type}, dataset: {data_type}\\n\")\n",
        "\n",
        "            probs = ann_probabilities[imp_type][s_type][data_type]\n",
        "            curr_y_true = y_true[imp_type][s_type][data_type][0]\n",
        "            \n",
        "            best_threshold = w_t[best_indexes[imp_type][s_type]['validation']]\n",
        "            y_pred = to_binary(probs[0][:, 1], best_threshold)\n",
        "\n",
        "            print(sm.classification_report(\n",
        "                    curr_y_true,\n",
        "                    y_pred,\n",
        "                    labels=[0, 1]\n",
        "                )\n",
        "            )\n",
        "            print(\"-\" * 60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSaFzRMEffu0",
        "colab_type": "text"
      },
      "source": [
        "già che ci sono calcolo la AUROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLVAkI0fe1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_auroc(fpr, recall):\n",
        "    area = scipy.integrate.trapz(y=recall, x=fpr)\n",
        "    return area"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIp_j3duxE1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(20, 16))\n",
        "\n",
        "for j, s_type in enumerate(stock_type):\n",
        "    for i, imp_type in enumerate(impact_type):\n",
        "        for data_type in dataset_type[:-1]:\n",
        "            i_sorted = np.argsort(fprs[imp_type][s_type][data_type])\n",
        "            \n",
        "            x = fprs[imp_type][s_type][data_type][i_sorted]\n",
        "            y = recalls[imp_type][s_type][data_type][i_sorted]\n",
        "\n",
        "            area = get_auroc(x, y)\n",
        "            row_format = \"{:>15}{:>15}{:>15}{:>15.3f}\"\n",
        "            print(row_format.format(s_type, imp_type, data_type, area))\n",
        "            #print(f\"{s_type} | {imp_type} | {data_type} | AUROC: {area:4.3f}\")\n",
        "\n",
        "            ax[i, j].plot(\n",
        "                x,\n",
        "                y,\n",
        "                color=dataset_colors[data_type],\n",
        "                label=str.title(data_type)\n",
        "            )\n",
        "\n",
        "            i_sweet = np.argmax(utilities[imp_type][s_type]['validation'])\n",
        "            best_x = fprs[imp_type][s_type][data_type][i_sweet]\n",
        "            best_y = recalls[imp_type][s_type][data_type][i_sweet]\n",
        "\n",
        "            ax[i, j].plot(\n",
        "                best_x,\n",
        "                best_y,\n",
        "                marker='s',\n",
        "                markersize=5,\n",
        "                color=dataset_colors[data_type],\n",
        "                label=str.title(f\"{data_type} - best\")\n",
        "            )\n",
        "\n",
        "        ax[i, j].plot([0, 1], [0, 1], color='black', linewidth=0.5, label='__None__')\n",
        "        ax[i, j].legend(loc='lower right', fontsize=14)\n",
        "        ax[i, j].set_xlim([0, 1.1])\n",
        "        ax[i, j].set_ylim([0, 1.1])\n",
        "        ax[i, j].set_xlabel('FPR', fontsize=16)\n",
        "        ax[i, j].set_ylabel('Recall', fontsize=16)\n",
        "        ax[i, j].set_title(f\"{s_type} | {imp_type}\", fontsize=16)\n",
        "\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKZmAwBaHlBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(20, 16))\n",
        "\n",
        "for j, s_type in enumerate(stock_type):\n",
        "    for i, imp_type in enumerate(impact_type):\n",
        "        for data_type in dataset_type[:-1]:\n",
        "            i_sorted = np.argsort(fprs[imp_type][s_type][data_type])\n",
        "            \n",
        "            x = w_t\n",
        "            y = utilities[imp_type][s_type][data_type]\n",
        "\n",
        "            ax[i, j].plot(\n",
        "                x,\n",
        "                y,\n",
        "                color=dataset_colors[data_type],\n",
        "                label=str.title(data_type)\n",
        "            )\n",
        "\n",
        "            i_sweet = np.argmax(utilities[imp_type][s_type]['validation'])\n",
        "            best_x = w_t[i_sweet]\n",
        "            best_y = utilities[imp_type][s_type][data_type][i_sweet]\n",
        "\n",
        "            ax[i, j].plot(\n",
        "                best_x,\n",
        "                best_y,\n",
        "                marker='s',\n",
        "                markersize=5,\n",
        "                color=dataset_colors[data_type],\n",
        "                label=str.title(f\"{data_type} - best\")\n",
        "            )\n",
        "\n",
        "        ax[i, j].legend(loc='lower right', fontsize=14)\n",
        "        #ax[i, j].set_xlim([0, 1.1])\n",
        "        #ax[i, j].set_ylim([0, 0.21])\n",
        "        ax[i, j].set_xlabel(r'$p_t$', fontsize=16)\n",
        "        ax[i, j].set_ylabel('Utility', fontsize=16)\n",
        "        ax[i, j].set_title(f\"{s_type} | {imp_type}\", fontsize=16)\n",
        "\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9tzqrP1Fopx",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Creazione tabelle performance\n",
        "\n",
        "Ora creo le tabelle esattamente come nel caso delle probabilità:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmld1ZHu1AeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_single_results_table(recalls, precisions, fprs, ksss, utilities, s_type):\n",
        "    columns = impact_type\n",
        "    index = pd.Index([\n",
        "        'in: FPR', 'out: FPR', 'in: Recall', 'out: Recall',\n",
        "        'in: Prec', 'out: Prec',\n",
        "        'in: U', 'out: U', 'in: KSS', 'out: KSS',\n",
        "    ])\n",
        "    table = pd.DataFrame(\n",
        "        data=np.zeros((len(index), len(columns)), dtype=np.float64),\n",
        "        columns=columns,\n",
        "        index=index\n",
        "    )\n",
        "\n",
        "    for imp_type in columns:\n",
        "        table.loc['in: FPR', imp_type] = fprs[imp_type][s_type]['train']\n",
        "        table.loc['out: FPR', imp_type] = fprs[imp_type][s_type]['test']\n",
        "        \n",
        "        table.loc['in: Recall', imp_type] = recalls[imp_type][s_type]['train']\n",
        "        table.loc['out: Recall', imp_type] = recalls[imp_type][s_type]['test']\n",
        "        \n",
        "        table.loc['in: Prec', imp_type] = precisions[imp_type][s_type]['train']\n",
        "        table.loc['out: Prec', imp_type] = precisions[imp_type][s_type]['test']\n",
        "        \n",
        "        table.loc['in: U', imp_type] = utilities[imp_type][s_type]['train']\n",
        "        table.loc['out: U', imp_type] = utilities[imp_type][s_type]['test']\n",
        "        \n",
        "        table.loc['in: KSS', imp_type] = ksss[imp_type][s_type]['train']\n",
        "        table.loc['out: KSS', imp_type] = ksss[imp_type][s_type]['test']\n",
        "    \n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoBnmaaKObWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_recalls = {\n",
        "    imp_type: {\n",
        "        s_type: {\n",
        "            data_type: 0\n",
        "            for data_type in dataset_type\n",
        "        }\n",
        "        for s_type in stock_type\n",
        "    }\n",
        "    for imp_type in impact_type\n",
        "}\n",
        "all_precisions = copy.deepcopy(all_recalls)\n",
        "all_fprs = copy.deepcopy(all_recalls)\n",
        "all_ksss = copy.deepcopy(all_recalls)\n",
        "all_utilities = copy.deepcopy(all_recalls)\n",
        "\n",
        "for imp_type in impact_type:\n",
        "    for s_type in stock_type:\n",
        "        for data_type in dataset_type:\n",
        "\n",
        "            if data_type == 'test':\n",
        "                all_recalls[imp_type][s_type][data_type] = recalls[imp_type][s_type][data_type]\n",
        "                all_precisions[imp_type][s_type][data_type] = precisions[imp_type][s_type][data_type]\n",
        "                all_fprs[imp_type][s_type][data_type] = fprs[imp_type][s_type][data_type]\n",
        "                all_ksss[imp_type][s_type][data_type] = ksss[imp_type][s_type][data_type]\n",
        "                all_utilities[imp_type][s_type][data_type] = utilities[imp_type][s_type][data_type]\n",
        "            else:\n",
        "                i_best = best_indexes[imp_type][s_type][data_type]\n",
        "\n",
        "                all_recalls[imp_type][s_type][data_type] = recalls[imp_type][s_type][data_type][i_best]\n",
        "                all_precisions[imp_type][s_type][data_type] = precisions[imp_type][s_type][data_type][i_best]\n",
        "                all_fprs[imp_type][s_type][data_type] = fprs[imp_type][s_type][data_type][i_best]\n",
        "                all_ksss[imp_type][s_type][data_type] = ksss[imp_type][s_type][data_type][i_best]\n",
        "                all_utilities[imp_type][s_type][data_type] = utilities[imp_type][s_type][data_type][i_best]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjyz06-WYOgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_vol = get_single_results_table(all_recalls, all_precisions, all_fprs, all_ksss, all_utilities, 'min_vol')\n",
        "max_vol = get_single_results_table(all_recalls, all_precisions, all_fprs, all_ksss, all_utilities, 'max_vol')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi0VMRaFYnXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.concat([min_vol, max_vol], axis='columns', keys=['min_vol', 'max_vol'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RbzadFrZdQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.to_csv(os.path.join(base_path, 'confronto3.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce2-WmsdhGaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-22Wx5uQmJIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}