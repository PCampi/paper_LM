{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi statistica intervalli di ricorrenza extreme returns\n",
    "\n",
    "Questo notebook contiene le analisi statistiche fatte per il paper.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *Dow Jones* con la massima ampiezza storica disponibile (1985 - 2019)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] plot degli intervalli di ricorrenza per il Dow Jones, con metodi diversi:\n",
    "    - [x] quantile threshold al\n",
    "        - [x] 95%\n",
    "        - [x] 97.5%\n",
    "        - [x] 99%\n",
    "        - [x] verifica della relazione $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio, e confronto con l'evidenza dei dati\n",
    "    - [x] peak-over-threshold definito come $pot = \\mu \\pm m \\cdot \\sigma$: ricavare analiticamente m come $m = \\frac{q_x - \\mu}{\\sigma}$ dove $q_x$ è il quantile di ordine $x$\n",
    "        \n",
    "Infine, tutto dovrà essere rifatto per una azione dell'*S&P500*, non per il Dow Jones come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import List\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import scipy.special as sfun\n",
    "from scipy.stats import genextreme as gev\n",
    "\n",
    "from statsmodels.tsa import stattools\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "\n",
    "import numba\n",
    "\n",
    "# import stimatore di Hill\n",
    "import tail_estimation\n",
    "from my_timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_type = ['pos', 'neg', 'abs']\n",
    "quantile_type = ['95', '97.5', '99', 'evt']\n",
    "distribution_type = ['weibull', 's-exp', 'q-exp']\n",
    "\n",
    "colors = {\n",
    "    'pos': 'seagreen',\n",
    "    'neg': 'darkred',\n",
    "    'abs': 'royalblue',\n",
    "}\n",
    "\n",
    "legend_labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Caricamento dei dati e divisione train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "djia_path = os.path.join(data_path, 'DJIA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia = pd.read_csv(djia_path)\n",
    "djia.loc[:, 'Date'] = pd.to_datetime(djia['Date'], format=\"%Y-%m-%d\")\n",
    "djia.index = djia['Date']\n",
    "djia.drop(columns=['Date'], inplace=True)\n",
    "djia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora dichiariamo le date in cui bisogna dividere i dati, visto che ci sono state crisi nei mesi/anni successivi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dates = {\n",
    "    'insurance': datetime.datetime(1987, 1, 1), # insurance companies crisis\n",
    "    'dot-com': datetime.datetime(2000, 1, 1), # dot-com bubble explodes\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 1), # subprime crisis\n",
    "    'eu-debt': datetime.datetime(2011, 1, 1), # EU sovereign debt crisis\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcolo log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns = np.log(djia.loc[:, ['Adj Close']]).diff(periods=1).iloc[1:, :]\n",
    "log_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(18, 7))\n",
    "\n",
    "ax[0].plot(djia['Adj Close'])\n",
    "ax[0].set(xlabel='Date', ylabel='DJIA', title='Dow Jones index value')\n",
    "sns.despine()\n",
    "\n",
    "ax[1].plot(log_returns, label='Log Returns')\n",
    "ax[1].set(xlabel='Date', ylabel='Log Returns', title='Dow Jones Log Returns')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funzione per dividere il dataset prima e dopo gli eventi critici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(data: pd.DataFrame, before_date: datetime.datetime):\n",
    "    \"\"\"Split the data before and after the before_date.\"\"\"\n",
    "    before = data[data.index < before_date]\n",
    "    after = data[data.index >= before_date]\n",
    "    \n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_before_after = {\n",
    "    event: divide(log_returns, split_dates[event])\n",
    "    for event in split_dates.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora scelgo quale sia la data di splitting e faccio le analisi con quella:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_key = 'subprime-crisis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stima dei parametri della distribuzione GEV\n",
    "\n",
    "In questa sezione si replica la sezione 4.1 del paper.\n",
    "\n",
    "Secondo la [Extreme Value Theory](https://en.wikipedia.org/wiki/Extreme_value_theory), trovare gli estremi significa trovare un gruppo di dati $x \\geq x_t$ dove $x_t$ è l'*extreme value threshold*, e che soddisfi la GEV ([Generalized Extreme Values Distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution)) che ha distribuzione cumulativa:\n",
    "\n",
    "\\begin{align*}\n",
    "    G(x) &= exp\\left[- \\left(1 + \\xi \\frac{x - \\mu}{\\sigma}\\right)^{-1/\\xi}\\right] \\; for \\; \\xi \\neq 0\\\\ \n",
    "    G(x) &= exp\\left[-exp\\left(\\frac{x - \\mu}{\\sigma}\\right)\\right] \\; for \\; \\xi = 0\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "dove $\\xi$ è lo *shape parameter* che determina la forma della coda, $1/\\xi$ è il *tail exponent* della distribuzione.\n",
    "\n",
    "Per trovare il threshold $x_t$, usiamo questo metodo:\n",
    "\n",
    "1. sort dei dati (tutti i log-returns) in ordine discendente (o non-ascendente) per avere la sequenza $x_1 \\geq x_2 \\geq \\ldots \\geq x_n$\n",
    "2. applicare lo [stimatore di Hill](https://en.wikipedia.org/wiki/Heavy-tailed_distribution#Hill's_tail-index_estimator) dove $n$ è il numero di samples, $k$ l'indice del k-esimo dato più grande (posizione k nella sequenza ordinata) chiamato *k-th order statistic*\n",
    "\n",
    "$$\\hat{\\xi}_{k,n} = \\frac{1}{\\gamma - 1} = \\frac{1}{k}\\sum_{i=1}^{k}log\\left(\\frac{x_i}{x_{k+1}}\\right)$$\n",
    "\n",
    "3. calcolare la statistica di [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic) per quantificare il fitting tra la distribuzione GEV così ricavata (con $\\hat{\\xi}_{k,n}$ come esponente) e quella empirica della coda, dove la coda è rappresentata dai returns che sono $x \\geq x_t$, cioé quelli ordinati discendenti con indice $i < k$\n",
    "4. scegliere $k$ (e di conseguenza $x_t$ che non è altro che il k-esimo elemento dei return ordinati discendenti) come il valore associato alla *minima* statistica di Kolmogorov-Smirnov\n",
    "\n",
    "Questo flusso viene applicato a tutti e 3 i tipi di returns considerati: positivi, negativi e assoluti.\n",
    "\n",
    "Cominciamo:\n",
    "\n",
    "### 3.1 Stima di $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def estimate_shape_param(x: np.ndarray, get_optimum=False, n_times=1):\n",
    "    \"\"\"Estimate the shape parameter gamma for the GEV using Hill estimator.\n",
    "    Input MUST be sorted in descending order.\n",
    "    \"\"\"\n",
    "    # xi_estimation[0]: order statistics\n",
    "    # xi_estimation[1]: tail index estimates (xi su Wikipedia, gamma nel paper)\n",
    "    # xi_estimation[2]: optimal order statistics (k)\n",
    "    # xi_estimation[3]: tail index for the optimal order statistics\n",
    "\n",
    "    # xi_estimation[4]: array of fractions of order statistics used for the 1st bootstrap sample\n",
    "    # xi_estimation[5]: corresponding AMSE values\n",
    "    # xi_estimation[6]: fraction of order statistics corresponding to the minimum of AMSE for the 1st bootstrap\n",
    "    # xi_estimation[7]: index of the 1st bootstrap sample's order statistics array corresponding to the\n",
    "    #                   minimization boundary set by eps_stop parameter\n",
    "\n",
    "    # xi_estimation[8]: array of fractions of order statistics used for the 2nd bootstrap sample\n",
    "    # xi_estimation[9]: corresponding AMSE values\n",
    "    # xi_estimation[10]: fraction of order statistics corresponding to the minimum of AMSE for the 2nd bootstrap\n",
    "    # xi_estimation[11]: index of the 2nd bootstrap sample's order statistics array corresponding to the\n",
    "    #                   minimization boundary set by eps_stop parameter\n",
    "    assert isinstance(get_optimum, bool)\n",
    "    \n",
    "    # check descending\n",
    "    assert np.all(np.diff(x) <= 0.0)\n",
    "    \n",
    "    xis = np.zeros((n_times, x.shape[0] - 1))\n",
    "    optimal_kappas = np.zeros((n_times, ), dtype=np.int)\n",
    "    optimal_xis = np.zeros((n_times, ), dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        xi_estimation = tail_estimation.hill_estimator(x, bootstrap=get_optimum)\n",
    "\n",
    "        kappas = xi_estimation[0]\n",
    "        xi_hill = xi_estimation[1]\n",
    "        \n",
    "        xis[i, :] = xi_hill[:]\n",
    "        \n",
    "        if get_optimum:\n",
    "            optimal_kappas[i] = xi_estimation[2]\n",
    "            optimal_xis[i] = xi_estimation[3]\n",
    "            \n",
    "    xis = np.mean(xis, axis=0)  # per ogni colonna (k) la media delle xi che ha trovato\n",
    "    \n",
    "    if get_optimum:\n",
    "        optimal_k = int(round(np.mean(optimal_kappas)))\n",
    "        optimal_xi = np.mean(optimal_xis)\n",
    "        optimal_gamma = 1.0 + (1.0 / optimal_xi)\n",
    "        \n",
    "        return {\n",
    "            'kappas': kappas,\n",
    "            'xis': xis,\n",
    "            'gammas': 1.0 + (1.0 / xis),\n",
    "            'k_opt': optimal_k,\n",
    "            'xi_opt': optimal_xi,\n",
    "            'gamma_opt': optimal_gamma,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'kappas': kappas,\n",
    "            'xis': xis,\n",
    "            'gammas': 1.0 + (1.0 / xis),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo i return ordinati dal più grande al più piccolo per i positivi, negativi e assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. scelta di dove fermarsi, qui prima della crisi finanziaria 2007-2008\n",
    "lr_before, lr_after = returns_before_after[split_key]\n",
    "\n",
    "# returns contiene il training set\n",
    "# 1. sorting dei returns - va fatto sia per il positivo, che per il negativo, che per gli assoluti\n",
    "returns = {\n",
    "    'pos': lr_before['Adj Close'][lr_before['Adj Close'] > 0.0],\n",
    "    'neg': lr_before['Adj Close'][lr_before['Adj Close'] < 0.0],\n",
    "    'abs': lr_before['Adj Close'].abs(),\n",
    "}\n",
    "\n",
    "sorted_positive_lr = returns['pos'].sort_values(ascending=False)\n",
    "sorted_negative_lr = (-returns['neg']).sort_values(ascending=False)\n",
    "sorted_absolute_lr = returns['abs'].sort_values(ascending=False)\n",
    "\n",
    "eps = 5e-6\n",
    "\n",
    "sorted_lr = {\n",
    "    'pos': sorted_positive_lr[sorted_positive_lr >= eps],\n",
    "    'neg': sorted_negative_lr[sorted_negative_lr >= eps],\n",
    "    'abs': sorted_absolute_lr[sorted_absolute_lr >= eps],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo la stima di $\\xi = \\frac{1}{\\gamma - 1}$ con lo stimatore di Hill per i tre returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_estimation = {\n",
    "    ret_type: estimate_shape_param(sorted_lr[ret_type].values, get_optimum=True, n_times=30)\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "title_format = \"{:>15}\"*4\n",
    "row_format = \"{:>15}\" + \"{:>15.3f}\" * 2 + \"{:>15}\"\n",
    "print(title_format.format('Return type', 'Xi', 'gamma = -c', 'k'))\n",
    "print(\"-\"*60)\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    print(row_format.format(ret_type, est['xi_opt'], est['gamma_opt'], est['k_opt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plot di $\\xi_{k, n}$ e $\\gamma$ al variare di k e dei returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot di $\\xi$ e $\\gamma$ al variare di $k$, cioè del threshold, e plot di $\\xi$ e $\\gamma$ al variare dei return, per vedere come cambia a seconda di quale return si prenda come threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot delle stime di xi e gamma al variare di k\n",
    "truncation = 100\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(18, 14))\n",
    "\n",
    "# Xi\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = est['kappas'][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[0][0].plot(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "    ax[0][0].plot(est['k_opt'], est['xi_opt'], marker='s', markersize=5, color=colors[ret_type])\n",
    "\n",
    "ax[0][0].set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\frac{1}{\\hat{\\gamma} - 1}$',\n",
    "    title=r'Hill estimation of $\\hat{\\xi}_{k,n}$ as function of $k$'\n",
    ")\n",
    "ax[0][0].legend()\n",
    "\n",
    "\n",
    "# gamma\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = est['kappas'][truncation:]\n",
    "    y = est['gammas'][truncation:]\n",
    "\n",
    "    ax[0][1].plot(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "    ax[0][1].plot(est['k_opt'], est['gamma_opt'], marker='s', markersize=5, color=colors[ret_type])\n",
    "\n",
    "ax[0][1].set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'$\\hat{\\gamma}_{k,n}$',\n",
    "    title=r'Hill estimation of $\\hat{\\gamma}_{k,n}$ as function of $k$'\n",
    ")\n",
    "ax[0][1].legend()\n",
    "\n",
    "\n",
    "# seconda riga, in funzione dei log-returns\n",
    "# Xi\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = sorted_lr[ret_type].values[1:][::-1][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[1][0].semilogx(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax[1][0].set(\n",
    "    xlabel=r'$r$, $-r$, $|r|$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\frac{1}{\\hat{\\gamma} - 1}$',\n",
    "    title=r'Hill estimation of $\\hat{\\xi}_{k,n}$ as function of the returns'\n",
    ")\n",
    "ax[1][0].legend()\n",
    "\n",
    "# gamma\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = sorted_lr[ret_type].values[1:][::-1][truncation:]\n",
    "    y = est['gammas'][truncation:]\n",
    "\n",
    "    ax[1][1].semilogx(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax[1][1].set(\n",
    "    xlabel=r'$r$, $-r$, $|r|$',\n",
    "    ylabel=r'$\\hat{\\gamma}_{k,n}$',\n",
    "    title=r'Hill estimation of $\\hat{\\gamma}_{k,n}$ as function of sorted returns'\n",
    ")\n",
    "ax[1][1].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fitting della GEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora biogna passare a fittare la GEV e calcolare la statistica KS per ogni valore di $k$ (e quindi del return che funge da threshold, $x_t$).\n",
    "\n",
    "Andremo poi a scegliere il valore di $k$ che minimizza la KS.\n",
    "\n",
    "Creiamo allora una funzione apposita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=False, parallel=True, nogil=True)\n",
    "def fit_gev(lr, xi, k, size=None, n_tries=1):\n",
    "    \"\"\"Find the best fitting GEV to the tail distribution of data in lr.\"\"\"\n",
    "#     assert isinstance(xi, np.ndarray)\n",
    "#     assert isinstance(k, np.ndarray)\n",
    "#     assert xi.shape == k.shape\n",
    "    \n",
    "    # check descending\n",
    "#     assert np.all(np.diff(lr) <= 0.0)\n",
    "    nk = k.shape[0]\n",
    "    n_xi = xi.shape[0]\n",
    "    \n",
    "    ks = np.zeros((n_xi, ))\n",
    "    pvals = np.zeros((n_xi, ))\n",
    "    fits = []\n",
    "    \n",
    "    print(\"Start fitting\")\n",
    "    for i in numba.prange(nk):\n",
    "        current_k = k[i]\n",
    "        current_xi = xi[i]\n",
    "        \n",
    "        threshold = lr[current_k]\n",
    "        tail_data = lr[:current_k]\n",
    "        \n",
    "        if size:\n",
    "            ss = size\n",
    "        else:\n",
    "            ss = current_k\n",
    "        \n",
    "        fit = gev.fit(tail_data, fix_c=-current_xi)\n",
    "        fits.append(fit)\n",
    "        \n",
    "        k_stat_temp = np.zeros((n_tries, ))\n",
    "        pval_temp = np.zeros((n_tries, ))\n",
    "        \n",
    "        c = fit[0]\n",
    "        loc = fit[1]\n",
    "        scale = fit[2]\n",
    "        \n",
    "        for j in range(n_tries):\n",
    "            rvs = gev.rvs(c, loc, scale, ss)\n",
    "\n",
    "            kk, pv = scipy.stats.ks_2samp(tail_data, rvs)\n",
    "            k_stat_temp[j] = kk\n",
    "            pval_temp[j] = pv\n",
    "        \n",
    "        ks[i] = np.mean(k_stat_temp)\n",
    "        pvals[i] = np.mean(pval_temp)\n",
    "        \n",
    "    print(\"Fitting done\")\n",
    "    \n",
    "    return {\n",
    "        'ks': ks,\n",
    "        'p': pvals,\n",
    "        'fits': fits,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed applichiamola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "npz_filename = f\"ks_stat-pvals_{split_key}.npz\"\n",
    "pickle_filename = f\"./fits_{split_key}.pickle\"\n",
    "\n",
    "if load:  # just load the already computed KS and p-values\n",
    "    loaded = np.load(npz_filename)\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as infile:\n",
    "        fits = pickle.load(infile)\n",
    "    \n",
    "    kolmog_smirn = {\n",
    "        'pos': {\n",
    "            'ks': loaded['ks_pos'],\n",
    "            'p': loaded['pvals_pos'],\n",
    "            'fits': fits['pos'],\n",
    "        },\n",
    "        'neg': {\n",
    "            'ks': loaded['ks_neg'],\n",
    "            'p': loaded['pvals_neg'],\n",
    "            'fits': fits['neg'],\n",
    "        },\n",
    "        'abs': {\n",
    "            'ks': loaded['ks_abs'],\n",
    "            'p': loaded['pvals_abs'],\n",
    "            'fits': fits['abs'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "else:  # compute them and save them\n",
    "    size = 10000\n",
    "    n_tries = 10\n",
    "    print(\"\\nFitting returns\")\n",
    "\n",
    "    kolmog_smirn = {\n",
    "        ret_type: fit_gev(\n",
    "            sorted_lr[ret_type].values,\n",
    "            hill_estimation[ret_type]['xis'],\n",
    "            hill_estimation[ret_type]['kappas'],\n",
    "            size=size,\n",
    "            n_tries=n_tries,\n",
    "        )\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    np.savez_compressed(npz_filename, **{\n",
    "        'ks_pos': kolmog_smirn['pos']['ks'],\n",
    "        'pvals_pos': kolmog_smirn['pos']['p'],\n",
    "        'ks_neg': kolmog_smirn['neg']['ks'],\n",
    "        'pvals_neg': kolmog_smirn['neg']['p'],\n",
    "        'ks_abs': kolmog_smirn['abs']['ks'],\n",
    "        'pvals_abs': kolmog_smirn['abs']['p'],\n",
    "    })\n",
    "    \n",
    "    fits = {\n",
    "        ret_type: kolmog_smirn[ret_type]['fits']\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    with open(pickle_filename, 'wb') as outfile:\n",
    "        pickle.dump(fits, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che ho la statistica KS per ogni valore di k e del return, plot delle statistiche rispetto ai returns e a $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pval = 0.05\n",
    "\n",
    "valid_pvals = {\n",
    "    ret_type: kolmog_smirn[ret_type]['p'] <= min_pval\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le figure seguenti mostrano l'andamento di $d_{KS}$ in funzione di k e dei returns ordinati, su scala $x$ semilogaritmica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot della statistica KS al variare del sorted return e di k\n",
    "truncation = 100\n",
    "end = -170\n",
    "p_labels = {\n",
    "    'pos': r'valid $p$ for $r$',\n",
    "    'neg': r'valid $p$ for $-r$',\n",
    "    'abs': r'valid $p$ for $|r|$',\n",
    "}\n",
    "\n",
    "p_height = {\n",
    "    'pos': 0.0,\n",
    "    'neg': -0.01,\n",
    "    'abs': -0.02\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(18, 7))\n",
    "\n",
    "# d_KS in funzione dei returns\n",
    "for ret_type in return_type:\n",
    "    x = sorted_lr[ret_type].values[1:][::-1][truncation:end]\n",
    "    y = kolmog_smirn[ret_type]['ks'][truncation:end]\n",
    "    mask = valid_pvals[ret_type]\n",
    "    x_ok = x[mask[::-1][truncation:end]]\n",
    "    \n",
    "    i_min = np.argmin(y)\n",
    "    \n",
    "    ax[0].semilogx(\n",
    "        x,\n",
    "        y,\n",
    "        color=colors[ret_type],\n",
    "        label=legend_labels[ret_type])\n",
    "    \n",
    "    ax[0].semilogx(\n",
    "        x_ok,\n",
    "        p_height[ret_type] * np.ones((len(x_ok, ))),\n",
    "        color=colors[ret_type],\n",
    "        marker='.',\n",
    "        linestyle='',\n",
    "        label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[0].axvline(\n",
    "        x[i_min],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "ax[0].set(\n",
    "    xlabel=r'$r$, $-r$, $|r|$',\n",
    "    ylabel=r'$d_{KS}$',\n",
    "    title=r'KS statistics as function of the sorted returns'\n",
    ")\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "# d_KS in funzione di k\n",
    "for ret_type in return_type:\n",
    "    x = hill_estimation[ret_type]['kappas'][truncation:end]\n",
    "    y = kolmog_smirn[ret_type]['ks'][truncation:end]\n",
    "    mask = valid_pvals[ret_type]\n",
    "    k_ok = x[mask[::-1][truncation:end]]\n",
    "    \n",
    "    i_min = np.argmin(y)\n",
    "    \n",
    "    ax[1].semilogx(\n",
    "        x,\n",
    "        y,\n",
    "        color=colors[ret_type],\n",
    "        label=legend_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[1].semilogx(\n",
    "        k_ok,\n",
    "        p_height[ret_type] * np.ones((len(k_ok, ))),\n",
    "        color=colors[ret_type],\n",
    "        marker='.',\n",
    "        linestyle='',\n",
    "        label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[1].axvline(\n",
    "        x[i_min],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "ax[1].set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'$d_{KS}$',\n",
    "    title=r'KS statistics as function of $k$'\n",
    ")\n",
    "ax[1].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le linee sottostanti le figure, composte in realtà da punti, identificano quei valori per cui il test di Kolmogorov-Smirnov ha dato un _p_-value $p \\leq 0.05$, ed è quindi ritenuto statisticamente valido. Si nota come agli estremi di $k$ e dei returns il *p*-value non sia significativo e ci siano grosse oscillazioni, probabilmente dovute a instabilità numeriche nel calcolo della maximum likelihood.\n",
    "\n",
    "### 3.4 Minima $d_{KS}$ per trovare il miglior fit della GEV\n",
    "\n",
    "Ottimo, ora bisogna selezionare il minimo valore della statistica KS $d_{KS}$ che abbia un p-value valido ($p < 0.05$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_dks(d_ks, pvals, min_pval=min_pval):\n",
    "    dks = d_ks.copy()\n",
    "    invalid = pvals > min_pval\n",
    "    \n",
    "    dks[invalid] = np.min(dks) + 1\n",
    "    i_min = np.argmin(dks)\n",
    "    \n",
    "    return dks[i_min], i_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_kolmog_smirn = {\n",
    "    ret_type: find_min_dks(kolmog_smirn[ret_type]['ks'], kolmog_smirn[ret_type]['p'])\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "threshold_evt = {\n",
    "    'pos': sorted_lr['pos'][min_kolmog_smirn['pos'][1]],\n",
    "    'neg': sorted_lr['neg'][min_kolmog_smirn['neg'][1]],\n",
    "    'abs': sorted_lr['abs'][min_kolmog_smirn['abs'][1]],\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "title_format = \"{:>15}\"*4\n",
    "row_format = \"{:>15}{:>15.4f}{:>15}{:>15.6f}\"\n",
    "print(title_format.format('Return type', 'Min d_KS', 'i', 'Return'))\n",
    "print(\"-\"*60)\n",
    "for ret_type in return_type:\n",
    "    print(row_format.format(\n",
    "        ret_type,\n",
    "        min_kolmog_smirn[ret_type][0],\n",
    "        min_kolmog_smirn[ret_type][1],\n",
    "        threshold_evt[ret_type]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo graficamente se le distribuzioni fittano bene i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparo i dati per plottare\n",
    "titles = {\n",
    "    'pos': 'Positive extreme returns',\n",
    "    'neg': 'Negative extreme returns',\n",
    "    'abs': 'Absolute extreme returns',\n",
    "}\n",
    "\n",
    "xlabels = {\n",
    "    'pos': r'Positive extreme returns $r$',\n",
    "    'neg': r'Negative extreme returns $-r$',\n",
    "    'abs': r'Absolute extreme returns  $|r|$',\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}\n",
    "\n",
    "# plot\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "for ret_type, a in zip(return_type, ax):\n",
    "    data = sorted_lr[ret_type].values[:min_kolmog_smirn[ret_type][1]]\n",
    "    sns.distplot(\n",
    "        data,\n",
    "        color=colors[ret_type],\n",
    "        label=legend_labels[ret_type],\n",
    "        kde=False,\n",
    "        norm_hist=True,\n",
    "        ax=a\n",
    "    )\n",
    "    \n",
    "    a.set_xlabel(xlabels[ret_type])\n",
    "\n",
    "for a, ret_type in zip(ax, return_type):\n",
    "    i_min = min_kolmog_smirn[ret_type][1]\n",
    "    best_fit = fits[ret_type][i_min]\n",
    "    _, b = a.xaxis.get_data_interval()\n",
    "    x = np.linspace(0, b, 1000)\n",
    "    pdf = gev.pdf(x, *best_fit)\n",
    "    a.plot(x, pdf, color=colors[ret_type], label='GEV pdf')\n",
    "    a.legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottimo, sembra che fittino piuttosto bene. A questo punto é finita la sezione 4.1 del paper.\n",
    "\n",
    "Finora abbiamo quindi ottenuto:\n",
    "\n",
    "- le distribuzioni di probabilità degli extreme returns (positivi, negativi, assoluti)\n",
    "- i threshold che massimizzano il fitting della distribuzione *GEV* sugli extreme returns. Tali threshold possono essere quindi usati per determinare quali movimenti siano estremi e quali no!\n",
    "\n",
    "Concludiamo quindi confrontando i threshold così ottenuti con i threshold del 95% percentile che abbiamo utilizzato finora per le azioni S&P500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = returns_before_after[split_key][0]['Adj Close'].quantile(q=[0.05, 0.95])\n",
    "\n",
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(18, 9))\n",
    "\n",
    "dates = log_returns.index\n",
    "# training set\n",
    "ax.plot(\n",
    "    returns_before_after[split_key][0]['Adj Close'],\n",
    "    label='Log Returns - train',\n",
    "    color='slategrey',\n",
    "    alpha=0.25,\n",
    "    linestyle='',\n",
    "    marker='.'\n",
    ")\n",
    "# testing set\n",
    "ax.plot(\n",
    "    returns_before_after[split_key][1]['Adj Close'],\n",
    "    label='Log Returns - test',\n",
    "    color='navy',\n",
    "    alpha=0.25,\n",
    "    linestyle='',\n",
    "    marker='.'\n",
    ")\n",
    "\n",
    "ax.plot(dates, perc[0.05] * np.ones(len(dates)), color=colors['neg'], linestyle=':', label='5% percentile')\n",
    "ax.plot(dates, perc[0.95] * np.ones(len(dates)), color=colors['pos'], linestyle=':', label='95% percentile')\n",
    "ax.plot(dates, -threshold_evt['neg'] * np.ones(len(dates)), color=colors['neg'], linestyle='--', label=r'$-x_t$ da EVT')\n",
    "ax.plot(dates, threshold_evt['pos'] * np.ones(len(dates)), color=colors['pos'], linestyle='--', label=r'$x_t$ da EVT')\n",
    "\n",
    "ax.set(xlabel='Date', ylabel='Log Returns', title='Dow Jones Log Returns and thresholds', ylim=[-0.12, 0.12])\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può vedere dal plot, i percentili sono molto più stringenti rispetto al valore che massimizza il fitting della *GEV*. Vediamo anche di confermarlo con un po' di numeri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent(data, thresh_low, thresh_up):\n",
    "    indexes = np.logical_or(data.values <= thresh_low, data.values >= thresh_up)\n",
    "    num = len(data[indexes])\n",
    "    denom = len(data)\n",
    "    \n",
    "    return num / denom\n",
    "\n",
    "extreme_percent_with_percentiles = get_percent(log_returns['Adj Close'], perc[0.05], perc[0.95])\n",
    "extreme_percent_with_gev = get_percent(log_returns['Adj Close'], -threshold_evt['neg'], threshold_evt['pos'])\n",
    "\n",
    "print(\"{:>20}{:>15}\".format('Threshold type', 'Extremes %'))\n",
    "print(\"-\"*35)\n",
    "print(\"{:>20}{:>15.3f}\".format('percentile 5-95 %', extreme_percent_with_percentiles))\n",
    "print(\"{:>20}{:>15.3f}\".format('EVT', extreme_percent_with_gev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pratica, vuol dire che se usassimo i valori di threshold ricavati dalla *EVT* avremmo un dataset sicuramente più bilanciato, ma c'è da chiedersi se si possano effettivamente allora considerare \"estremi\". Non è troppo \"inclusivo\" un tale threshold?\n",
    "\n",
    "### 3.5 Calcolo dei $\\tau_Q$ e delle $Q$\n",
    "\n",
    "Calcoliamoli per poi usarli nella maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo i tau e calcolo i Q\n",
    "tau_q_95 = 1.0 / (1.0 - 0.95)\n",
    "tau_q_975 = 1.0 / (1.0 - 0.975)\n",
    "tau_q_99 = 1.0 / (1.0 - 0.99)\n",
    "\n",
    "# calcolo i quantili equivalenti ai threshold della EVT\n",
    "Q = {\n",
    "    ret_type: 1.0 - (sum(returns[ret_type].abs() >= abs(threshold_evt[ret_type])) / len(returns[ret_type]))\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "tau_q = {\n",
    "    ret_type: {\n",
    "        '95': tau_q_95,\n",
    "        '97.5': tau_q_975,\n",
    "        '99': tau_q_99,\n",
    "        'evt': 1.0 / (1.0 - Q[ret_type])\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcolo degli intervalli di ricorrenza e plot della loro distribuzione\n",
    "\n",
    "Ora che abbiamo i threshold possiamo calcolare gli intervalli di ricorrenza e vederne la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrence_intervals(is_extreme: pd.DataFrame):\n",
    "    \"\"\"Get the recurrence intervals durations between extremes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_extreme: pd.DataFrame\n",
    "        a DataFrame with the date on the index and 1 if the return at time t is extreme,\n",
    "        0 otherwise. Must contain a single column named 'extreme'\n",
    "    \"\"\"\n",
    "    assert isinstance(is_extreme.index, pd.DatetimeIndex)\n",
    "    assert len(is_extreme.columns) == 1\n",
    "    \n",
    "    # convert to int\n",
    "    data = is_extreme.astype(np.int8)\n",
    "    data.loc[:, 'date'] = data.index\n",
    "    data.index = pd.RangeIndex(len(is_extreme))\n",
    "    \n",
    "    data_is_extreme = data[data[data.columns[0]] == 1]\n",
    "    \n",
    "    intervals = []\n",
    "    for i in range(1, len(data_is_extreme)):\n",
    "        last_time = data_is_extreme.date.iloc[i - 1]\n",
    "        current_time = data_is_extreme.date.iloc[i]\n",
    "        \n",
    "        n_days = data_is_extreme.index[i] - data_is_extreme.index[i - 1]\n",
    "        \n",
    "        intervals.append((last_time, current_time, n_days))\n",
    "        \n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quantili vanno presi al 95%, 97.5%, 99%. Creo quindi il `dict` che contiene gli intervalli di ricorrenza, organizzati secondo il tipo di return ed il tipo di threshold (quantile o evt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantili: attenzione che sono TUTTI POSITIVI\n",
    "thresholds = {\n",
    "    ret_type: {\n",
    "        q_type: returns[ret_type].abs().quantile(float(q_type) / 100)\n",
    "        for q_type in quantile_type[:-1]\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "for ret_type in return_type:\n",
    "    thresholds[ret_type]['evt'] = abs(threshold_evt[ret_type])\n",
    "\n",
    "extremes = {\n",
    "    ret_type: {\n",
    "        q_type: (returns[ret_type].abs() >= thresholds[ret_type][q_type]).astype(np.int8)\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "tmp_rec_int = {\n",
    "    ret_type: {\n",
    "        q_type: get_recurrence_intervals(pd.DataFrame(extremes[ret_type][q_type]))\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "recurrence_intervals = {\n",
    "    ret_type: {\n",
    "        q_type: pd.DataFrame(data={\n",
    "            'last_extreme': [x[0] for x in tmp_rec_int[ret_type][q_type]],\n",
    "            'current_extreme': [x[1] for x in tmp_rec_int[ret_type][q_type]],\n",
    "            'n_days': [x[2] for x in tmp_rec_int[ret_type][q_type]],\n",
    "        })\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plot istogrammi intervalli di ricorrenza\n",
    "\n",
    "Ora visualizzo graficamente la lunghezza degli intervalli di ricorrenza con degli istogrammi, rispettivamente per i returns positivi, negativi ed assoluti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_labels = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $r$',\n",
    "    'neg': r'Negative $-r$',\n",
    "    'abs': r'Absolute $|r|$',\n",
    "}\n",
    "\n",
    "y_lims = {\n",
    "    'pos': [0.0, 0.15],\n",
    "    'neg': [0.0, 0.15],\n",
    "    'abs': [0.0, 0.08],\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=1, ncols=3, figsize=(18, 7))\n",
    "\n",
    "# positive returns\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals[ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[i])\n",
    "\n",
    "    ax[i].legend()\n",
    "    ax[i].set(title=titles[ret_type], ylim=y_lims[ret_type])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creazione delle tabelle come nel paper\n",
    "\n",
    "Ora creo le tabelle riassuntive come a pagina 9 del paper di [Jiang et al](https://doi.org/10.1080/14697688.2017.1373843).\n",
    "\n",
    "Prima mi creo due funzioncine e poi le chiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_table(intervals: pd.DataFrame, returns: pd.DataFrame, ret_type: str, thresh: float, col_name='perc'):\n",
    "    \"\"\"Get a single panel sub-table.\"\"\"\n",
    "    obsv = int(intervals.shape[0])\n",
    "    mean = intervals['n_days'].mean()\n",
    "    median = intervals['n_days'].median()\n",
    "    std_dev = intervals['n_days'].std()\n",
    "    skewness = intervals['n_days'].skew()\n",
    "    kurtosis = intervals['n_days'].kurt()\n",
    "    \n",
    "    ret_mean = returns.mean()\n",
    "    ret_std_dev = returns.std()\n",
    "    \n",
    "    m = (thresh - ret_mean) / ret_std_dev\n",
    "#     print(f\"\\nRet_type: {ret_type}, q_type: {col_name}\")\n",
    "#     print(f\"Threshold: {thresh:5.4f}, Mean: {ret_mean:5.4f}, m: {m:5.4f}\")\n",
    "        \n",
    "    acf, qstat, pvals = stattools.acf(intervals['n_days'].values, qstat=True, nlags=30)\n",
    "    rho1 = acf[1]\n",
    "    _, p_rho1 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[1:],\n",
    "        intervals['n_days'].shift(periods=1).values[1:],\n",
    "    )\n",
    "    \n",
    "    rho5 = acf[5]\n",
    "    _, p_rho5 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[5:],\n",
    "        intervals['n_days'].shift(periods=5).values[5:],\n",
    "    )\n",
    "\n",
    "    Q30 = qstat[-1]\n",
    "    p_Q30 = pvals[-1]\n",
    "    \n",
    "    index = pd.Index(data=[\n",
    "        'm',\n",
    "        'obsv',\n",
    "        'mean',\n",
    "        'median',\n",
    "        'stdev',\n",
    "        'skew',\n",
    "        'kurt',\n",
    "        'rho(1)',\n",
    "        'p-value(rho1)',\n",
    "        'rho(5)',\n",
    "        'p-value(rho5)',\n",
    "        'Q(30)',\n",
    "        'p-value(Q30)',\n",
    "    ])\n",
    "    \n",
    "    result = pd.DataFrame(data=[\n",
    "        [m],\n",
    "        [obsv],\n",
    "        [mean],\n",
    "        [median],\n",
    "        [std_dev],\n",
    "        [skewness],\n",
    "        [kurtosis],\n",
    "        [rho1],\n",
    "        [p_rho1],\n",
    "        [rho5],\n",
    "        [p_rho5],\n",
    "        [Q30],\n",
    "        [p_Q30],\n",
    "    ],\n",
    "    index=index,\n",
    "    columns=[col_name])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il titolo è la sezione della tabella (Negative/Positive/Absolute), i quantili sono quelli che mi interessano e finiranno sulle colonne della tabella ed il risultato è un `dict` che ha come chiavi i titoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    ret_type: {\n",
    "        q_type: get_single_table(recurrence_intervals[ret_type][q_type],\n",
    "                                 returns[ret_type],\n",
    "                                 ret_type,\n",
    "                                 thresh=thresholds[ret_type][q_type],\n",
    "                                 col_name=q_type)\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "panels = {\n",
    "    ret_type: pd.concat([tables[ret_type][q_type] for q_type in quantile_type], axis='columns')\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo le tabelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['abs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot degli autocorrelogrammi\n",
    "\n",
    "Vediamo con gli [autocorrelogammi](https://en.wikipedia.org/wiki/Correlogram) se c'è autocorrelazione nelle serie dei *recurrence interval*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle righe il tipo di threshold, sulle colonne il tipo di return\n",
    "fig, ax = pl.subplots(nrows=3, ncols=3, figsize=(27, 24))\n",
    "fig.suptitle(\"Autocorrelation plots\", fontsize=16)\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['95']['n_days'].values, lags=30, ax=ax[0][0], title=r'$-r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['95']['n_days'].values, lags=30, ax=ax[0][1], title=r'$r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['95']['n_days'].values, lags=30, ax=ax[0][2], title=r'$|r|$, 95%')\n",
    "ax[0, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['97.5']['n_days'].values, lags=30, ax=ax[1][0], title=r'$-r$, 97.5%')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['97.5']['n_days'].values, lags=30, ax=ax[1][1], title=r'$r$, 97.5%')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['97.5']['n_days'].values, lags=30, ax=ax[1][2], title=r'$|r|$, 97.5%')\n",
    "ax[1, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "# WARNING: non ci sono abbastanza dati\n",
    "# tsaplots.plot_acf(recurrence_intervals['neg']['99']['n_days'].values, lags=30, ax=ax[2][0], title=r'$-r$, 99%')\n",
    "# tsaplots.plot_acf(recurrence_intervals['pos']['99']['n_days'].values, lags=30, ax=ax[2][1], title=r'$r$, 99%')\n",
    "# tsaplots.plot_acf(recurrence_intervals['abs']['99']['n_days'].values, lags=30, ax=ax[2][2], title=r'$|r|$, 99%')\n",
    "# ax[2, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['evt']['n_days'].values, lags=30, ax=ax[2][0], title=r'$-r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['evt']['n_days'].values, lags=30, ax=ax[2][1], title=r'$r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['evt']['n_days'].values, lags=30, ax=ax[2][2], title=r'$|r|$, EVT')\n",
    "ax[2, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "for a in ax[2]:\n",
    "    a.set_xlabel('lag in the recurrence interval array')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per interpretare i plot, bisogna ricordare che:\n",
    "\n",
    "- sulle $x$ c'è il lag della serie temporale relativa ai giorni tra i movimenti estremi, cioè quella degli intervalli di ricorrenza. Vuol dire che $x=22$ significa il 22° intervallo di ricorrenza visto nel passato, prima di quello attuale, non 22 giorni prima di oggi. La distanza in giorni potrebbe anche essere un anno o più.\n",
    "- sulle y c'è la correlazione di Pearson $R$\n",
    "\n",
    "Apparentemente c'è autocorrelazione nei recurrence intervals selezionati con il quantile $q_{0.95}$ fino a 6 per positivi e negativi, 2 per gli assoluti.\n",
    "\n",
    "Per i recurrence intervals con $q_{0.975}$ solo a 1 giorno per positivi e negativi, nessuna per gli assoluti.\n",
    "\n",
    "Per i recurrence intervals con $q_{0.99}$ non c'è autocorrelazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verifica relazione empirica\n",
    "\n",
    "Verifichiamo ora la relazione empirica $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_format = \"{:>15}\"*5\n",
    "row_format = \"{:>15.3f}{:>15}{:>15.3f}{:>15.3f}\" + \"{:>14.3f}%\"\n",
    "print(title_format.format('Quantile', 'Return type', 'tau_q', 'True mean', 'Error %'))\n",
    "print(title_format.format('-'*15, '-'*15, '-'*15, '-'*15, '-'*15))\n",
    "\n",
    "for q_type in quantile_type[:-1]:\n",
    "    for i, name in enumerate(return_type):\n",
    "        data_mean = recurrence_intervals[name][q_type]['n_days'].mean()\n",
    "        \n",
    "        q = float(q_type) / 100.0\n",
    "        tau = 1.0 / (1.0 - q)\n",
    "        \n",
    "        perc_diff = (tau - data_mean) / data_mean\n",
    "        \n",
    "        print(row_format.format(q, name, tau, data_mean, perc_diff * 100))\n",
    "        \n",
    "        if i == len(return_type) - 1:\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In effetti, la relazione è valida con un margine di errore massimo di circa il $5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Determinazione della probabilità di pericolo - TODO\n",
    "\n",
    "Gli autori definiscono la *hazard probability* come\n",
    "\n",
    "\\begin{equation}\n",
    "    W(\\Delta t | t) = \\frac{\\int_t^{t + \\Delta t} p(\\tau)d\\tau}{\\int_t^{\\infty}p(\\tau)d\\tau}\n",
    "\\end{equation}\n",
    "\n",
    "dove $p(\\tau)$ è la distribuzione di probabilità (`pdf` per scipy).\n",
    "\n",
    "La hazard probability definisce la probabilità che, dato che si è verificato un evento estremo $t$ giorni nel passato, ci sia un tempo di attesa $\\Delta t$ ulteriore prima di un altro evento estremo.\n",
    "Se consideriamo $W(1 | t)$ è simile al problema che abbiamo affrontato con la rete neurale.\n",
    "\n",
    "Ora, nota la ditribuzione $p(\\tau)$, si può derivare analiticamente l'integrale. Il problema è quindi: come trovare $p(\\tau)$, e che forma ha?\n",
    "\n",
    "Gli autori utilizzano una [stretched exponential distribution](https://en.wikipedia.org/wiki/Stretched_exponential_function), una [*q*-exponential distribution](https://en.wikipedia.org/wiki/Q-exponential_distribution) ed una [Weibull distribution](https://it.wikipedia.org/wiki/Distribuzione_di_Weibull). I parametri delle 3 distribuzioni vengono stimati tramite MLE.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "1. scegli una distribuzione (s-exp, q-exp, Weibull)\n",
    "2. riformula la parametrizzazione in funzione solo  dello *shape parameter*\n",
    "3. calcola la log-likelihood utilizzando una semplice ricerca a griglia sui parametri liberi\n",
    "4. i parametri che forniscono la massima log-likelihood sono quelli cercati, e trova la formula teorica della *hazard probability* con le equazioni del paper\n",
    "\n",
    "Cominciamo con la Weibull, ma prima creiamo una funzione che calcoli la *hazard probability* empirica, con la formula\n",
    "\n",
    "$$\n",
    "W_{emp}(\\Delta t | t) = \\frac{\\#(t < \\tau \\leq t + \\Delta t)}{\\#(\\tau > t)}\n",
    "$$\n",
    "\n",
    "dove al numeratore c'è il numero di recurrence intervals con valore compreso in $(t, t + \\Delta t]$, al denominatore il numero di recurrence intervals con valore maggiore di $t$, cioè nel range $(t, +\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empirical_hazard_prob(rec_ints: np.ndarray, t, delta_t):\n",
    "    \"\"\"Compute the empirical hazard probability.\"\"\"\n",
    "    assert isinstance(rec_ints, np.ndarray)\n",
    "    num = np.sum(np.logical_and(rec_ints > t, rec_ints <= t + delta_t))\n",
    "    denom = np.sum(rec_ints > t)\n",
    "    \n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Fitting della Weibull\n",
    "\n",
    "In `scipy.stats` è definita come\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&f(x, c) = c x^{c - 1} e^{-x^{c}} \\\\\n",
    "&f(x, c, loc, scale) = \\frac{1}{scale}f\\left(\\frac{x - loc}{scale}, c\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "dove $c$ è lo *shape parameter*. Nel paper invece è\n",
    "\n",
    "\\begin{equation}\n",
    "f(x, \\beta, \\alpha) = \\frac{\\alpha}{\\beta} \\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha - 1} e^{-\\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha}}\n",
    "\\end{equation}\n",
    "\n",
    "quindi la corrispondenza è\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&loc = 0 \\\\\n",
    "&\\beta = scale \\\\\n",
    "&shape = c = \\alpha\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ora dobbiamo stimare i parametri della Weibull con una maximum log-likelihood estimation (*MLE*). Riscrivendoli in funzione di $\\tau_Q$ e $\\beta = scale$ abbiamo\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)} \\\\\n",
    "cioè \\\\\n",
    "&\\beta = scale = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{c} \\right)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ricordiamo che $\\tau_Q = \\frac{1}{1 - Q}$ dove $Q$ è il quantile.\n",
    "\n",
    "A questo punto la MLE ha formula:\n",
    "\n",
    "$$ ln(L_w) = n \\cdot ln\\left( \\frac{c}{\\beta} \\right) + \\sum_{i=1}^{n} \\left[ (c - 1) ln\\left( \\frac{\\tau_i}{\\beta} \\right) - \\left( \\frac{\\tau_i}{\\beta} \\right)^c \\right] $$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $t_i$ il corrispondente valore dell'intervallo di ricorrenza (es: 14 giorni, 4 giorni...).\n",
    "\n",
    "Il flow è quindi, in questo caso:\n",
    "\n",
    "1. a seconda del percentile (95% o EVT) calcolare $Q$ e quindi $\\tau_Q$\n",
    "2. utilizzare una ricerca con step $1e-6$ sul parametro $c = \\alpha$, il quale risulta in un certo valore di $\\beta$\n",
    "3. utilizzare quei valori di $c$ e di $\\beta$ nella MLE\n",
    "4. trovare il massimo della MLE ed i corrispondenti valori di $c$ e $\\beta$\n",
    "5. urrà! Ora possiamo usarli nella *pdf* della distribuzione Weibull per ottenere l'hazard $W(\\Delta t | t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=True, parallel=True, nogil=True)\n",
    "def mle_weibull(rec_ints: np.ndarray, c: np.ndarray, beta: np.ndarray):\n",
    "    \"\"\"MLE estimation for weibull distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"       \n",
    "    m = beta.shape[0]\n",
    "    n = rec_ints.shape[0]\n",
    "\n",
    "    # log-likelihood    \n",
    "    log_likelihoods = np.zeros_like(beta)\n",
    "    \n",
    "    # precompute matrices for tau_beta and ln_tau_beta\n",
    "    tau_beta = np.zeros((n, m), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            tau_beta[i, j] = rec_ints[i] / beta[j]\n",
    "            \n",
    "    ln_tau_beta = np.log(tau_beta)\n",
    "    \n",
    "    c_beta = c / beta\n",
    "    n_ln_c_beta = n * np.log(c_beta)\n",
    "    c_1 = c - 1.0\n",
    "\n",
    "    for j in numba.prange(m):  # no progress indication, it's a parallel for loop\n",
    "        summ = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            summ += c_1[j] * ln_tau_beta[i, j] - tau_beta[i, j] ** c[j]\n",
    "            \n",
    "        log_likelihoods[j] = n_ln_c_beta[j] + summ\n",
    "        \n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della Weibull per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.arange(0.25, 2, 1e-3)\n",
    "sfg = sfun.gamma(1.0 + (1.0 / c))\n",
    "\n",
    "beta = {\n",
    "    ret_type: {\n",
    "        q_type: tau_q[ret_type][q_type] / sfg\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "i_ok = {\n",
    "    ret_type: {\n",
    "        q_type: np.argwhere(beta[ret_type][q_type] > 1e-6).flatten()\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "beta_ok = {\n",
    "    ret_type: {\n",
    "        q_type: beta[ret_type][q_type][i_ok[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "c_ok = {\n",
    "    ret_type: {\n",
    "        q_type: c[i_ok[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = True\n",
    "\n",
    "ll_weib_file = f\"./log-like-weib_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_weib_file, 'rb') as infile:\n",
    "        log_like_weib = pickle.load(infile)\n",
    "else:\n",
    "    log_like_weib = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_weib[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing Weibull MLE on quantile: {q_type}, c={c_ok[ret_type][q_type].shape}, beta={beta_ok[ret_type][q_type].shape}\")\n",
    "\n",
    "            ll = mle_weibull(x, c_ok[ret_type][q_type], beta_ok[ret_type][q_type])\n",
    "\n",
    "            log_like_weib[ret_type][q_type] = ll\n",
    "          \n",
    "    with open(ll_weib_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_weib, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            c_ok[ret_type][q_type],\n",
    "            log_like_weib[ret_type][q_type],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "\n",
    "        i_max = np.argmax(log_like_weib[ret_type][q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            c_ok[ret_type][q_type][i_max],\n",
    "            log_like_weib[ret_type][q_type][i_max],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set(xlabel=r'$c = \\alpha$', ylabel=r'$log(L_W)$')\n",
    "    a.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ora che abbiamo le MLE per i tre tipi di returns e i minimi, possiamo fittare la Weibull sui recurrence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_min = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_weib[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: c_ok[ret_type][q_type][i_min[ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_scale = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: tau_q[ret_type][q_type] / sfun.gamma(1.0 + 1.0 / best_shape['weibull'][ret_type][q_type])\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: {\n",
    "                'shape': best_shape['weibull'][ret_type][q_type],\n",
    "                'scale': best_scale['weibull'][ret_type][q_type],\n",
    "                'loc': 0.0,\n",
    "            }\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fitting della stretched-exponential (s-exp)\n",
    "\n",
    "Creiamo ora la classe per la s-exp, che ha *pdf*:\n",
    "\n",
    "$$\n",
    "p(x, c, a, b) = a e^{-\\left( bx \\right)^c}\n",
    "$$\n",
    "\n",
    "dove $c$, $a$ e $b$ sono *shape parameters*, con $0 < c < 1$, $b \\geq 0$ e $a > 0$.\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della s-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{s-exp}) = n \\cdot \\ln(a) - \\sum_{i=1}^{n} (b \\cdot x_i)^c\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $a = \\frac{c \\Gamma \\left( \\frac{2}{c} \\right)}{\\left[ \\Gamma \\left( \\frac{1}{c} \\right) \\right]^2 \\tau_Q}$ e $b = \\frac{ \\Gamma \\left( \\frac{2}{c} \\right)}{\\Gamma \\left( \\frac{1}{c} \\right) \\tau_Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_b_sexp(c, tau_q):\n",
    "    \"\"\"Get the a and b params.\"\"\"\n",
    "    gamma_2_c = sfun.gamma(2.0 / c)\n",
    "    gamma_1_c = sfun.gamma(1.0 / c)\n",
    "    \n",
    "    b_all = gamma_2_c / (gamma_1_c * tau_q)\n",
    "    a_all = b_all * c / gamma_1_c\n",
    "    \n",
    "    return a_all, b_all\n",
    "\n",
    "@timeit\n",
    "def mle_sexp(rec_ints: np.ndarray, c: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for s-exponential distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    n = rec_ints.shape[0]\n",
    "    \n",
    "    a_all, b_all = get_a_b_sexp(c, tau_q)\n",
    "    \n",
    "    ln_a_all = np.log(a_all)\n",
    "    \n",
    "    ll = np.zeros((c.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j, c in enumerate(c):\n",
    "        ssum = 0\n",
    "        a = a_all[j]\n",
    "        b = b_all[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.power((b * rec_ints[i]), c)\n",
    "            \n",
    "        ll[j] = n * ln_a_all[j] - ssum\n",
    "    \n",
    "    ll[np.isnan(ll)] = -np.inf\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sexp = np.arange(1e-3, 1.0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = True\n",
    "\n",
    "ll_sexp_file = f\"./log-like-sexp_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_sexp_file, 'rb') as infile:\n",
    "        log_like_sexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_sexp = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_sexp[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing s-exp MLE on quantile: {q_type}, c={c_sexp.shape}\")\n",
    "\n",
    "            ll = mle_sexp(x, c_sexp, tau_q[ret_type][q_type])\n",
    "\n",
    "            log_like_sexp[ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_sexp['c'] = c_sexp\n",
    "\n",
    "    with open(ll_sexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_sexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_sexp[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape['s-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: c_sexp[i_max_sexp[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_a_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: get_a_b_sexp(best_shape['s-exp'][ret_type][q_type], tau_q[ret_type][q_type])[0]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_b_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: get_a_b_sexp(best_shape['s-exp'][ret_type][q_type], tau_q[ret_type][q_type])[1]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_params['s-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: {\n",
    "            'shape': best_shape['s-exp'][ret_type][q_type],\n",
    "            'a': best_a_sexp[ret_type][q_type],\n",
    "            'b': best_b_sexp[ret_type][q_type],\n",
    "            'loc': 0.0,\n",
    "        }\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            c_sexp[truncation:],\n",
    "            log_like_sexp[ret_type][q_type][truncation:],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            c_sexp[i_max_sexp[ret_type][q_type]],\n",
    "            log_like_sexp[ret_type][q_type][i_max_sexp[ret_type][q_type]],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set(xlabel=r'$c = \\alpha$', ylabel=r'$log(L_{s-exp})$')\n",
    "    a.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Fitting della q-exponential\n",
    "\n",
    "La terza distribuzione è la [q-exponential](https://en.wikipedia.org/wiki/Q-exponential_distribution).\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della q-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{q-exp}) = n \\cdot \\ln[\\lambda (2 - q)] - \\frac{1}{q - 1} \\sum_{i=1}^{n} \\ln[1 + (q - 1) \\lambda \\tau_i]\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $\\tau_i$ il valore dell'i-esimo recurrence interval, e il parametro $\\lambda$ si stima così:\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{1}{\\tau_Q(3 - 2q)}\n",
    "$$\n",
    "\n",
    "il parametro libero $q$ ha il range $\\left( 0, \\frac{3}{2} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def mle_qexp(rec_ints: np.ndarray, q: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for q-exponential distribution, given an array of q shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    assert np.all(q < 1.5)\n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    n = rec_ints.shape[0]\n",
    "    m = q.shape[0]\n",
    "    \n",
    "    lam = 1.0 / (tau_q * (3 - 2 * q))\n",
    "    \n",
    "    ll = np.zeros((q.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j in range(m):\n",
    "        ssum = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.log(1 + (q[j] - 1) * lam[j] * rec_ints[i])\n",
    "            \n",
    "#         ipdb.set_trace()\n",
    "        ll[j] = n * np.log(lam[j] * (2 - q[j])) - (1 / (q[j] - 1)) * ssum\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_qexp = np.arange(1.0, 1.5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = False\n",
    "\n",
    "ll_qexp_file = f\"./log-like-qexp_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_qexp_file, 'rb') as infile:\n",
    "        log_like_qexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_qexp = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_qexp[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing q-exp MLE on quantile: {q_type}, c={q_qexp.shape}\")\n",
    "\n",
    "            ll = mle_qexp(x, q_qexp, tau_q[ret_type][q_type])\n",
    "            ll[np.isnan(ll)] = -np.inf\n",
    "            \n",
    "            log_like_qexp[ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_qexp['c'] = q_qexp\n",
    "\n",
    "    with open(ll_qexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_qexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_qexp = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_qexp[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape['q-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: q_qexp[i_max_qexp[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_lambda_qexp = {\n",
    "    ret_type: {\n",
    "        q_type: 1.0 / (tau_q[ret_type][q_type] * (3 - 2 * best_shape['q-exp'][ret_type][q_type]))\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_params['q-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: {\n",
    "            'shape': best_shape['q-exp'][ret_type][q_type],\n",
    "            'lambda': best_lambda_qexp[ret_type][q_type],\n",
    "            'loc': 0.0,\n",
    "        }\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            q_qexp,\n",
    "            log_like_qexp[ret_type][q_type],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            q_qexp[i_max_qexp[ret_type][q_type]],\n",
    "            log_like_qexp[ret_type][q_type][i_max_qexp[ret_type][q_type]],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set_ylim([-1500, 0.0])\n",
    "    a.set(xlabel=r'$c = q$', ylabel=r'$log(L_{q-exp})$')\n",
    "    a.legend(loc='lower left')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Calcolo Hazard Probability\n",
    "\n",
    "Perfetto, ora ho i parametri della Weibull, della s-exp e della q-exp per ogni tipo di return e di threshold. Posso quindi ottenere la curva teorica per il fitting dei recurrence intervals.\n",
    "\n",
    "Per la Weibull è\n",
    "\n",
    "$$\n",
    "W_W(\\Delta t | t) = 1 - e^{\\left[ \\left( \\frac{t}{\\beta} \\right)^\\alpha - \\left( \\frac{t + \\Delta t}{\\beta} \\right)^\\alpha \\right]}\n",
    "$$\n",
    "\n",
    "dove $\\alpha = c^*$ lo *shape* ottimale, e $\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)}$ lo *scale* ottimale.\n",
    "\n",
    "Per la s-exp è:\n",
    "\n",
    "$$\n",
    "W_{s-exp}(\\Delta t | t) = \\frac{\\frac{bc}{a} - \\Gamma_l \\left( \\frac{1}{c}, (bt)^c \\right) - \\Gamma_u \\left( \\frac{1}{c}, [b(t + \\Delta t)]^c \\right)}{\\Gamma_u \\left( \\frac{1}{c}, (bt)^c \\right)}\n",
    "$$\n",
    "\n",
    "Per la q-exp è:\n",
    "\n",
    "$$\n",
    "W_{q-exp}(\\Delta t | t) = 1 - \\left[ 1 + \\frac{(q - 1)\\lambda \\Delta t}{1 + (q - 1)\\lambda t} \\right]^{1 - \\frac{1}{q - 1}}\n",
    "$$\n",
    "\n",
    "Mi creo allora le funzioni che le calcolano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_hazard(t, shape, scale, delta_t=1):\n",
    "    part_1 = np.power((t / scale), shape)\n",
    "    part_2 = np.power(((t + delta_t) / scale), shape)\n",
    "    \n",
    "    hazard = 1 - np.exp(part_1 - part_2)\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def sexp_hazard(t, c, a, b, delta_t=1):\n",
    "    num1 = (b * c / a)\n",
    "    num2 = sfun.gammainc((1.0 / c), np.power((b * t), c)) * sfun.gamma(1.0 / c)\n",
    "    num3 = sfun.gammaincc((1.0 / c), np.power(b * (t + delta_t), c)) * sfun.gamma(1.0 / c)\n",
    "    \n",
    "    num = num1 - num2 - num3\n",
    "    \n",
    "    denom = sfun.gammaincc((1.0 / c), np.power(b * t, c))\n",
    "    \n",
    "    hazard = num / denom\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def qexp_hazard(t, q, lam, delta_t=1):\n",
    "    num = (q - 1) * lam * delta_t\n",
    "    denom = 1 + (q - 1) * lam * t\n",
    "    exponent = 1 - (1 / (q - 1))\n",
    "    \n",
    "    hazard = 1 - np.power((1 + num / denom), exponent)\n",
    "    \n",
    "    return hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delta_t = 60\n",
    "x = np.arange(max_delta_t + 1)\n",
    "\n",
    "ret_type = 'neg'\n",
    "q_type = '99'\n",
    "\n",
    "theoretical_hazard = {\n",
    "    'weibull': weibull_hazard(\n",
    "        x,\n",
    "        best_params['weibull'][ret_type][q_type]['shape'],\n",
    "        best_params['weibull'][ret_type][q_type]['scale']\n",
    "    ),\n",
    "    's-exp': sexp_hazard(\n",
    "        x,\n",
    "        best_params['s-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['s-exp'][ret_type][q_type]['a'],\n",
    "        best_params['s-exp'][ret_type][q_type]['b'],\n",
    "    ),\n",
    "    'q-exp': qexp_hazard(\n",
    "        x,\n",
    "        best_params['q-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['q-exp'][ret_type][q_type]['lambda'],\n",
    "    )\n",
    "}\n",
    "\n",
    "empirical_hazard = np.array([\n",
    "    get_empirical_hazard_prob(recurrence_intervals[ret_type][q_type]['n_days'].values, t, 1)\n",
    "    for t in x\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(16, 9))\n",
    "\n",
    "dist_colors = {\n",
    "    'weibull': 'orchid',\n",
    "    's-exp': 'orangered',\n",
    "    'q-exp': 'mediumblue'\n",
    "}\n",
    "\n",
    "dist_labels = {\n",
    "    'weibull': r'$W_W$',\n",
    "    's-exp': r'$W_{s-exp}$',\n",
    "    'q-exp': r'$W_{q-exp}$',\n",
    "}\n",
    "\n",
    "for dist_type in distribution_type:\n",
    "    ax.plot(\n",
    "        x,\n",
    "        theoretical_hazard[dist_type],\n",
    "        color=dist_colors[dist_type],\n",
    "        label=dist_labels[dist_type])\n",
    "    \n",
    "ax.plot(\n",
    "    x,\n",
    "    empirical_hazard_prob,\n",
    "    label=r'$W_{emp}$',\n",
    "    color='black',\n",
    "    linestyle='-',\n",
    "    marker='o',\n",
    "    markersize=1,\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel=r'$t$', ylabel=r'$W(1 | \\Delta t)$', title=r'Hazard probability for $q = 0.99$')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSA NON FUNZIONA\n",
    "\n",
    "- [ ] La EVT ha come shape parameter un numero maggiore di 1, non va bene perché la curva qui sopra viene al contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NON servivano...sprecata mezza giornata, si cercano improperi creativi\n",
    "\n",
    "# class QExponential(scipy.stats.rv_continuous):\n",
    "#     \"\"\"\n",
    "#     The q-exponential distribution centered in 0 with scale 1 and shape q.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def _get_support(self, *args):\n",
    "#         print(f\"args: {args}\")\n",
    "        \n",
    "    \n",
    "#     def _pdf(self, x, c, loc=0, scale=1, rate=1):\n",
    "#         \"\"\"Probability density function for the q-exponential distribution\"\"\"\n",
    "#         exponent = - 1.0 / (c - 1)\n",
    "#         if c == 1:\n",
    "#             e_q = np.exp(c * x)\n",
    "#         else:\n",
    "#             e_q = np.power((1 + (c - 1) * rate * x), exponent)\n",
    "\n",
    "#         pdf = (2 - c) * rate * e_q\n",
    "        \n",
    "#         return pdf\n",
    "    \n",
    "#     def _pdf(self, x, c):\n",
    "#         return self._pdf(x, c, loc=0, scale=1, rate=1)\n",
    "\n",
    "# class StretchedExponential(scipy.stats.rv_continuous):\n",
    "#     \"\"\"Stretched exponential distribution.\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         \"\"\"Create a new instance of the s-exp distribution.\"\"\"\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.a = 0.0\n",
    "#         self.b = np.inf\n",
    "    \n",
    "#     def pdf(self, x, shapes, loc=0, scale=1):\n",
    "#         c = shapes[0]\n",
    "#         a = shapes[1]\n",
    "#         b = shapes[2]\n",
    "        \n",
    "#         if loc != 0.0:\n",
    "#             y = (x - loc) / scale\n",
    "#             return self._pdf(y, c, a, b) / scale\n",
    "#         else:\n",
    "#             return self._pdf(x, c, a, b)\n",
    "    \n",
    "#     def _pdf(self, x, c, a, b):\n",
    "#         \"\"\"Probability density function.\"\"\"\n",
    "#         exponent = -np.power((b * x), c)\n",
    "#         return a * np.exp(exponent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
