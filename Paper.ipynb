{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi statistica intervalli di ricorrenza extreme returns\n",
    "\n",
    "Questo notebook contiene le analisi statistiche fatte per il paper.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *Dow Jones* con la massima ampiezza storica disponibile (1985 - 2019)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] plot degli intervalli di ricorrenza per il Dow Jones, con metodi diversi:\n",
    "    - [x] quantile threshold al\n",
    "        - [x] 95%\n",
    "        - [x] 97.5%\n",
    "        - [x] 99%\n",
    "        - [x] verifica della relazione $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio, e confronto con l'evidenza dei dati\n",
    "    - [x] peak-over-threshold definito come $pot = \\mu \\pm m \\cdot \\sigma$: ricavare analiticamente m come $m = \\frac{q_x - \\mu}{\\sigma}$ dove $q_x$ è il quantile di ordine $x$\n",
    "        \n",
    "Infine, tutto dovrà essere rifatto per una azione dell'*S&P500*, non per il Dow Jones come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import List\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import scipy.special as sfun\n",
    "from scipy.stats import genextreme as gev\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "from statsmodels.tsa import stattools\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "\n",
    "import numba\n",
    "\n",
    "# import dello stimatore di Hill e del @timeit\n",
    "import tail_estimation\n",
    "from my_timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_type = ['pos', 'neg', 'abs']\n",
    "quantile_type = ['95', '97.5', '99', 'evt']\n",
    "distribution_type = ['weibull', 's-exp', 'q-exp']\n",
    "\n",
    "colors = {\n",
    "    'pos': 'seagreen',\n",
    "    'neg': 'darkred',\n",
    "    'abs': 'royalblue',\n",
    "}\n",
    "\n",
    "legend_labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Caricamento dei dati e divisione train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "djia_path = os.path.join(data_path, 'DJIA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia = pd.read_csv(djia_path)\n",
    "djia.loc[:, 'Date'] = pd.to_datetime(djia['Date'], format=\"%Y-%m-%d\")\n",
    "djia.index = djia['Date']\n",
    "djia.drop(columns=['Date'], inplace=True)\n",
    "djia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora dichiariamo le date in cui bisogna dividere i dati, visto che ci sono state crisi nei mesi/anni successivi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dates = {\n",
    "    'insurance': datetime.datetime(1987, 1, 1), # insurance companies crisis\n",
    "    'dot-com': datetime.datetime(2000, 1, 1), # dot-com bubble explodes\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 1), # subprime crisis\n",
    "    'eu-debt': datetime.datetime(2011, 1, 1), # EU sovereign debt crisis\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcolo log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns = np.log(djia.loc[:, ['Adj Close']]).diff(periods=1).iloc[1:, :]\n",
    "log_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(18, 7))\n",
    "\n",
    "ax[0].plot(djia['Adj Close'])\n",
    "ax[0].set(xlabel='Date', ylabel='DJIA', title='Dow Jones index value')\n",
    "sns.despine()\n",
    "\n",
    "ax[1].plot(log_returns, label='Log Returns')\n",
    "ax[1].set(xlabel='Date', ylabel='Log Returns', title='Dow Jones Log Returns')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funzione per dividere il dataset prima e dopo gli eventi critici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(data: pd.DataFrame, before_date: datetime.datetime):\n",
    "    \"\"\"Split the data before and after the before_date.\"\"\"\n",
    "    before = data[data.index < before_date]\n",
    "    after = data[data.index >= before_date]\n",
    "    \n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_before_after = {\n",
    "    event: divide(log_returns, split_dates[event])\n",
    "    for event in split_dates.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora scelgo quale sia la data di splitting e faccio le analisi con quella:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_key = 'subprime-crisis'\n",
    "\n",
    "# divisione dataset in training (in-sample) e testing (out-of-sample), qui prima e dopo la crisi finanziaria 2007-2008\n",
    "lr_before, lr_after = returns_before_after[split_key]\n",
    "\n",
    "# returns contiene il training set\n",
    "returns = {\n",
    "    'pos': lr_before['Adj Close'][lr_before['Adj Close'] > 0.0],\n",
    "    'neg': lr_before['Adj Close'][lr_before['Adj Close'] < 0.0],\n",
    "    'abs': lr_before['Adj Close'].abs(),\n",
    "}\n",
    "\n",
    "returns_test = {\n",
    "    'pos': lr_after['Adj Close'][lr_after['Adj Close'] > 0.0],\n",
    "    'neg': lr_after['Adj Close'][lr_after['Adj Close'] < 0.0],\n",
    "    'abs': lr_after['Adj Close'].abs(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stima dei parametri della distribuzione GEV\n",
    "\n",
    "In questa sezione si replica la sezione 4.1 del paper.\n",
    "\n",
    "Secondo la [Extreme Value Theory](https://en.wikipedia.org/wiki/Extreme_value_theory), trovare gli estremi significa trovare un gruppo di dati $x \\geq x_t$ dove $x_t$ è l'*extreme value threshold*, e che soddisfi la GEV ([Generalized Extreme Values Distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution)) che ha distribuzione cumulativa:\n",
    "\n",
    "\\begin{align*}\n",
    "    G(x) &= exp\\left[- \\left(1 + \\xi \\frac{x - \\mu}{\\sigma}\\right)^{-1/\\xi}\\right] \\; for \\; \\xi \\neq 0\\\\ \n",
    "    G(x) &= exp\\left[-exp\\left(\\frac{x - \\mu}{\\sigma}\\right)\\right] \\; for \\; \\xi = 0\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "dove $\\xi$ è lo *shape parameter* che determina la forma della coda, $1/\\xi$ è il *tail exponent* della distribuzione.\n",
    "\n",
    "Per trovare il threshold $x_t$, usiamo questo metodo:\n",
    "\n",
    "1. sort dei dati (tutti i log-returns) in ordine discendente (o non-ascendente) per avere la sequenza $x_1 \\geq x_2 \\geq \\ldots \\geq x_n$\n",
    "2. applicare lo [stimatore di Hill](https://en.wikipedia.org/wiki/Heavy-tailed_distribution#Hill's_tail-index_estimator) dove $n$ è il numero di samples, $k$ l'indice del k-esimo dato più grande (posizione k nella sequenza ordinata) chiamato *k-th order statistic*\n",
    "\n",
    "$$\\hat{\\xi}_{k,n} = \\frac{1}{\\gamma - 1} = \\frac{1}{k}\\sum_{i=1}^{k}log\\left(\\frac{x_i}{x_{k+1}}\\right)$$\n",
    "\n",
    "3. calcolare la statistica di [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic) per quantificare il fitting tra la distribuzione GEV così ricavata (con $\\hat{\\xi}_{k,n}$ come esponente) e quella empirica della coda, dove la coda è rappresentata dai returns che sono $x \\geq x_t$, cioé quelli ordinati discendenti con indice $i < k$\n",
    "4. scegliere $k$ (e di conseguenza $x_t$ che non è altro che il k-esimo elemento dei return ordinati discendenti) come il valore associato alla *minima* statistica di Kolmogorov-Smirnov\n",
    "\n",
    "Questo flusso viene applicato a tutti e 3 i tipi di returns considerati: positivi, negativi e assoluti.\n",
    "\n",
    "Inoltre, usiamo una seconda versione di questo flusso, che è:\n",
    "\n",
    "1. sort dei dati (tutti i log-returns) in ordine discendente (o non-ascendente) per avere la sequenza $x_1 \\geq x_2 \\geq \\ldots \\geq x_n$\n",
    "2. scorrere nei returns ordinati con un indice $k$ che identifica il threshold scelto e fitting di una GEV sui *tail data*, dove per tail data si intendono tutti i returns con indice $i < k$\n",
    "3. calcolare la statistica di [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic) per quantificare il fitting tra la distribuzione GEV e i dati\n",
    "4. scegliere $k$ (e di conseguenza $x_t$ che non è altro che il k-esimo elemento dei return ordinati discendenti) come il valore associato alla *minima* statistica di Kolmogorov-Smirnov\n",
    "\n",
    "Ciò che cambia tra i due flussi è che nel primo, calcoliamo *separatamente* $\\gamma$ e gli altri parametri $\\mu$ e $\\sigma$, mentre nel secondo caso tutti insieme con una maximum-likelihood estimation.\n",
    "\n",
    "Calcolo i return ordinati dal più grande al più piccolo per i positivi, negativi e assoluti per prima cosa e poi procedo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sorting dei returns - va fatto sia per il positivo, che per il negativo, che per gli assoluti\n",
    "sorted_positive_lr = returns['pos'].sort_values(ascending=False)\n",
    "sorted_negative_lr = (-returns['neg']).sort_values(ascending=False)\n",
    "sorted_absolute_lr = returns['abs'].sort_values(ascending=False)\n",
    "\n",
    "eps = 5e-6\n",
    "\n",
    "sorted_lr = {\n",
    "    'pos': sorted_positive_lr[sorted_positive_lr >= eps],\n",
    "    'neg': sorted_negative_lr[sorted_negative_lr >= eps],\n",
    "    'abs': sorted_absolute_lr[sorted_absolute_lr >= eps],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Primo flusso\n",
    "\n",
    "Cominciamo stimando $\\gamma$.\n",
    "\n",
    "#### 3.1.1 Stima di $\\gamma$ separata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def estimate_shape_param(x: np.ndarray, get_optimum=False, n_times=1):\n",
    "    \"\"\"Estimate the shape parameter gamma for the GEV using Hill estimator.\n",
    "    Input MUST be sorted in descending order.\n",
    "    \"\"\"\n",
    "    # xi_estimation[0]: order statistics\n",
    "    # xi_estimation[1]: tail index estimates (xi su Wikipedia, gamma nel paper)\n",
    "    # xi_estimation[2]: optimal order statistics (k)\n",
    "    # xi_estimation[3]: tail index for the optimal order statistics\n",
    "\n",
    "    # xi_estimation[4]: array of fractions of order statistics used for the 1st bootstrap sample\n",
    "    # xi_estimation[5]: corresponding AMSE values\n",
    "    # xi_estimation[6]: fraction of order statistics corresponding to the minimum of AMSE for the 1st bootstrap\n",
    "    # xi_estimation[7]: index of the 1st bootstrap sample's order statistics array corresponding to the\n",
    "    #                   minimization boundary set by eps_stop parameter\n",
    "\n",
    "    # xi_estimation[8]: array of fractions of order statistics used for the 2nd bootstrap sample\n",
    "    # xi_estimation[9]: corresponding AMSE values\n",
    "    # xi_estimation[10]: fraction of order statistics corresponding to the minimum of AMSE for the 2nd bootstrap\n",
    "    # xi_estimation[11]: index of the 2nd bootstrap sample's order statistics array corresponding to the\n",
    "    #                   minimization boundary set by eps_stop parameter\n",
    "    assert isinstance(get_optimum, bool)\n",
    "    \n",
    "    # check descending\n",
    "    assert np.all(np.diff(x) <= 0.0)\n",
    "    \n",
    "    xis = np.zeros((n_times, x.shape[0] - 1))\n",
    "    optimal_kappas = np.zeros((n_times, ), dtype=np.int)\n",
    "    optimal_xis = np.zeros((n_times, ), dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_times):\n",
    "        xi_estimation = tail_estimation.hill_estimator(x, bootstrap=get_optimum)\n",
    "\n",
    "        kappas = xi_estimation[0]\n",
    "        xi_hill = xi_estimation[1]\n",
    "        \n",
    "        xis[i, :] = xi_hill[:]\n",
    "        \n",
    "        if get_optimum:\n",
    "            optimal_kappas[i] = xi_estimation[2]\n",
    "            optimal_xis[i] = xi_estimation[3]\n",
    "            \n",
    "    xis = np.mean(xis, axis=0)  # per ogni colonna (k) la media delle xi che ha trovato\n",
    "    \n",
    "    if get_optimum:\n",
    "        optimal_k = int(round(np.mean(optimal_kappas)))\n",
    "        optimal_xi = np.mean(optimal_xis)\n",
    "        optimal_gamma = 1.0 + (1.0 / optimal_xi)\n",
    "        \n",
    "        return {\n",
    "            'kappas': kappas,\n",
    "            'xis': xis,\n",
    "            'gammas': 1.0 + (1.0 / xis),\n",
    "            'k_opt': optimal_k,\n",
    "            'xi_opt': optimal_xi,\n",
    "            'gamma_opt': optimal_gamma,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'kappas': kappas,\n",
    "            'xis': xis,\n",
    "            'gammas': 1.0 + (1.0 / xis),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso la mia funzione per stimare $\\gamma$ esattamente come nel paper, cioè:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{k} \\sum_{i = 1}{k} \\left[ \\ln x_{n + 1 - k} - \\ln x_k \\right]\n",
    "$$\n",
    "\n",
    "che però richiede i returns ordinati **discendenti**.\n",
    "\n",
    "Invece, uso la formula nell'altro paper [Tail index estimation, concentration and adaptivity](http://arxiv.org/abs/1503.05077), cioé\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}(k) = \\frac{1}{k} \\sum_{i = 1}^{k} \\ln \\left( \\frac{x_i}{x_{k + 1}} \\right)\n",
    "$$\n",
    "\n",
    "che richiede i returns ordinati **discendenti**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hill_estimator(lr: np.ndarray):\n",
    "    \"\"\"Get the estimation of gamma.\"\"\"\n",
    "    assert np.all(np.diff(lr) <= 0.0)  # ordinati discendenti, lr[i] - lr[i - 1] <= 0\n",
    "    \n",
    "    k_max = lr.shape[0]\n",
    "    kappas = np.arange(1, k_max)\n",
    "    \n",
    "    gammas = np.zeros((kappas.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for index, k in enumerate(kappas):  # index = k - 1\n",
    "        ssum = np.sum(np.log(lr[:k - 1] / lr[k - 1 + 1]))        \n",
    "        gammas[index] = (1 / k) * ssum\n",
    "        \n",
    "    return {\n",
    "        'xis': gammas,\n",
    "        'kappas': kappas\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo la stima di $\\xi = \\frac{1}{\\gamma - 1}$ con lo stimatore di Hill per i tre returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_estimation = {\n",
    "    ret_type: estimate_shape_param(sorted_lr[ret_type].values, get_optimum=True, n_times=30)\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "print(\"\")\n",
    "title_format = \"{:>15}\"*5\n",
    "row_format = \"{:>15}\" + \"{:>15.3f}\" * 2 + \"{:>15}{:>15f}\"\n",
    "print(title_format.format('Return type', 'Xi', 'gamma = -c', 'k', 'r_opt'))\n",
    "print(\"-\" * 15 * 5)\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    print(row_format.format(ret_type, est['xi_opt'], est['gamma_opt'], est['k_opt'], sorted_lr[ret_type].values[est['k_opt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, calcoliamo $\\xi = \\gamma = -c$ con la mia funzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hill_estimation = {\n",
    "    ret_type: my_hill_estimator(sorted_lr[ret_type].values)\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Plot di $\\xi_{k, n}$ e $\\gamma$ al variare di k e dei returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot di $\\xi$ e $\\gamma$ al variare di $k$, cioè del threshold, e plot di $\\xi$ e $\\gamma$ al variare dei return, per vedere come cambia a seconda di quale return si prenda come threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot delle stime di xi e gamma al variare di k\n",
    "truncation = 100\n",
    "\n",
    "# sulle righe in funzione di k o dei returns\n",
    "# sulle colonne lo xi stimato con il codice trovato su GitHub, lo xi stimato da me, e la conversione gamma = 1 + 1/xi\n",
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(18, 14))\n",
    "\n",
    "# Xi - GitHub\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = est['kappas'][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[0][0].plot(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "    ax[0][0].plot(est['k_opt'], est['xi_opt'], marker='s', markersize=5, color=colors[ret_type])\n",
    "\n",
    "ax[0][0].set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\frac{1}{\\hat{\\gamma} - 1}$',\n",
    "    title=r'Hill estimation of $\\hat{\\xi}_{k,n}$ as function of $k$'\n",
    ")\n",
    "\n",
    "# Xi - mio codice\n",
    "for ret_type in return_type:\n",
    "    est = my_hill_estimation[ret_type]\n",
    "    x = est['kappas'][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[0][1].plot(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax[0][1].set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\gamma(k)$',\n",
    "    title=r'My Hill estimation of $\\hat{\\xi}_{k,n}$ as function of $k$'\n",
    ")\n",
    "\n",
    "###########################################\n",
    "# seconda riga, in funzione dei log-returns\n",
    "# Xi - GitHub\n",
    "for ret_type in return_type:\n",
    "    est = hill_estimation[ret_type]\n",
    "    x = sorted_lr[ret_type].values[1:][::-1][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[1][0].semilogx(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax[1][0].set(\n",
    "    xlabel=r'$r$, $-r$, $|r|$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\frac{1}{\\hat{\\gamma} - 1}$',\n",
    "    title=r'Hill estimation of $\\hat{\\xi}_{k,n}$ as function of the returns'\n",
    ")\n",
    "\n",
    "# Xi - mio codice\n",
    "for ret_type in return_type:\n",
    "    est = my_hill_estimation[ret_type]\n",
    "    x = sorted_lr[ret_type].values[1:][::-1][truncation:]\n",
    "    y = est['xis'][truncation:]\n",
    "\n",
    "    ax[1][1].semilogx(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax[1][1].set(\n",
    "    xlabel=r'$r$, $-r$, $|r|$',\n",
    "    ylabel=r'$\\hat{\\xi}_{k,n} = \\gamma(k)$',\n",
    "    title=r'My Hill estimation of $\\hat{\\xi}_{k,n}$ as function of the returns'\n",
    ")\n",
    "\n",
    "m, n = ax.shape\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        ax[i, j].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'è differenza tra la mia stima e quella del file che ho trovato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(14, 8))\n",
    "\n",
    "\n",
    "for ret_type in return_type:\n",
    "    est1 = hill_estimation[ret_type]\n",
    "    x1 = est1['kappas'][truncation:]\n",
    "    y1 = est1['xis'][truncation:]\n",
    "    \n",
    "    est2 = my_hill_estimation[ret_type]\n",
    "    x2 = est2['kappas'][truncation:]\n",
    "    y2 = est2['xis'][truncation:]\n",
    "    \n",
    "    assert np.all(x1 == x2)\n",
    "\n",
    "    ax.plot(x1, y1 - y2, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "\n",
    "ax.set(\n",
    "    xlabel=r'$k$',\n",
    "    ylabel=r'Difference between the $\\xi = \\gamma$ estimations',\n",
    ")\n",
    "ax.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, non c'è differenza tra la stima che ho trovato su GitHub e la mia, bon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Fitting della GEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora biogna passare a fittare la GEV e calcolare la statistica KS per ogni valore di $k$ (e quindi del return che funge da threshold, $x_t$).\n",
    "\n",
    "Andremo poi a scegliere il valore di $k$ che minimizza la KS.\n",
    "\n",
    "Creiamo allora una funzione apposita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=False, parallel=True, nogil=True)\n",
    "def fit_gev(lr, xi, k, size=None, n_tries=1):\n",
    "    \"\"\"Find the best fitting GEV to the tail distribution of data in lr.\n",
    "    lr MUST BE in descending order\n",
    "    \"\"\"\n",
    "#     assert isinstance(xi, np.ndarray)\n",
    "#     assert isinstance(k, np.ndarray)\n",
    "#     assert xi.shape == k.shape\n",
    "    \n",
    "    # check descending\n",
    "#     assert np.all(np.diff(lr) <= 0.0)\n",
    "    nk = k.shape[0]\n",
    "    n_xi = xi.shape[0]\n",
    "    \n",
    "    ks = np.zeros((n_xi, ))\n",
    "    pvals = np.zeros((n_xi, ))\n",
    "    fits = []\n",
    "    \n",
    "    print(\"Start fitting\")\n",
    "    for i in numba.prange(nk):\n",
    "        current_k = k[i]\n",
    "        current_xi = xi[i]\n",
    "        \n",
    "        threshold = lr[current_k]\n",
    "        tail_data = lr[:current_k]\n",
    "        \n",
    "        if size:\n",
    "            ss = size\n",
    "        else:\n",
    "            ss = current_k\n",
    "        \n",
    "        fit = gev.fit(tail_data, fix_c=-current_xi)  # convenzioni diverse in SciPy\n",
    "        fits.append(fit)\n",
    "        \n",
    "        k_stat_temp = np.zeros((n_tries, ))\n",
    "        pval_temp = np.zeros((n_tries, ))\n",
    "        \n",
    "        c = fit[0]\n",
    "        loc = fit[1]\n",
    "        scale = fit[2]\n",
    "        \n",
    "        for j in range(n_tries):\n",
    "            rvs = gev.rvs(c, loc, scale, ss)\n",
    "\n",
    "            kk, pv = scipy.stats.ks_2samp(tail_data, rvs)\n",
    "            k_stat_temp[j] = kk\n",
    "            pval_temp[j] = pv\n",
    "        \n",
    "        ks[i] = np.mean(k_stat_temp)\n",
    "        pvals[i] = np.mean(pval_temp)\n",
    "        \n",
    "    print(\"Fitting done\")\n",
    "    \n",
    "    return {\n",
    "        'ks': ks,\n",
    "        'p': pvals,\n",
    "        'fits': fits,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed applichiamola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "npz_filename = f\"ks_stat-pvals_{split_key}.npz\"\n",
    "pickle_filename = f\"./fits_{split_key}.pickle\"\n",
    "\n",
    "if load:  # just load the already computed KS and p-values\n",
    "    loaded = np.load(npz_filename)\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as infile:\n",
    "        fits = pickle.load(infile)\n",
    "    \n",
    "    kolmog_smirn = {\n",
    "        'pos': {\n",
    "            'ks': loaded['ks_pos'],\n",
    "            'p': loaded['pvals_pos'],\n",
    "            'fits': fits['pos'],\n",
    "        },\n",
    "        'neg': {\n",
    "            'ks': loaded['ks_neg'],\n",
    "            'p': loaded['pvals_neg'],\n",
    "            'fits': fits['neg'],\n",
    "        },\n",
    "        'abs': {\n",
    "            'ks': loaded['ks_abs'],\n",
    "            'p': loaded['pvals_abs'],\n",
    "            'fits': fits['abs'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "else:  # compute them and save them\n",
    "    size = 10000\n",
    "    n_tries = 10\n",
    "    print(\"\\nFitting returns\")\n",
    "\n",
    "    kolmog_smirn = {\n",
    "        ret_type: fit_gev(\n",
    "            sorted_lr[ret_type].values,\n",
    "            hill_estimation[ret_type]['xis'],\n",
    "            hill_estimation[ret_type]['kappas'],\n",
    "            size=size,\n",
    "            n_tries=n_tries,\n",
    "        )\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    np.savez_compressed(npz_filename, **{\n",
    "        'ks_pos': kolmog_smirn['pos']['ks'],\n",
    "        'pvals_pos': kolmog_smirn['pos']['p'],\n",
    "        'ks_neg': kolmog_smirn['neg']['ks'],\n",
    "        'pvals_neg': kolmog_smirn['neg']['p'],\n",
    "        'ks_abs': kolmog_smirn['abs']['ks'],\n",
    "        'pvals_abs': kolmog_smirn['abs']['p'],\n",
    "    })\n",
    "    \n",
    "    fits = {\n",
    "        ret_type: kolmog_smirn[ret_type]['fits']\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    with open(pickle_filename, 'wb') as outfile:\n",
    "        pickle.dump(fits, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Secondo flusso\n",
    "\n",
    "Proviamo invece a fittare tutti e 3 i parametri in un colpo solo\n",
    "\n",
    "#### 3.2.1 Fitting della GEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=False, parallel=True, nogil=True)\n",
    "def fit_gev_one_shot(lr, size=None, n_tries=1):\n",
    "    \"\"\"Find the best fitting GEV to the tail distribution of data in lr.\"\"\"\n",
    "#     assert isinstance(xi, np.ndarray)\n",
    "#     assert isinstance(k, np.ndarray)\n",
    "#     assert xi.shape == k.shape\n",
    "    \n",
    "    # check descending\n",
    "#     assert np.all(np.diff(lr) <= 0.0)\n",
    "    n = lr.shape[0]\n",
    "    \n",
    "    ks_dist = np.zeros((n - 1, ))\n",
    "    pvals = np.zeros((n - 1, ))\n",
    "    fits = []\n",
    "    \n",
    "    print(\"Start fitting\")\n",
    "    for k in numba.prange(1, n):\n",
    "        threshold = lr[k]\n",
    "        tail_data = lr[:k]\n",
    "        \n",
    "        if size:\n",
    "            ss = size\n",
    "        else:\n",
    "            ss = k\n",
    "        \n",
    "        fit = gev.fit(tail_data)\n",
    "        fits.append(fit)\n",
    "        \n",
    "        k_stat_temp = np.zeros((n_tries, ))\n",
    "        pval_temp = np.zeros((n_tries, ))\n",
    "        \n",
    "        c = fit[0]\n",
    "        loc = fit[1]\n",
    "        scale = fit[2]\n",
    "        \n",
    "        for j in range(n_tries):\n",
    "            rvs = gev.rvs(c, loc, scale, ss)\n",
    "\n",
    "            kk, pv = scipy.stats.ks_2samp(tail_data, rvs)\n",
    "            k_stat_temp[j] = kk\n",
    "            pval_temp[j] = pv\n",
    "        \n",
    "        ks_dist[k - 1] = np.mean(k_stat_temp)\n",
    "        pvals[k - 1] = np.mean(pval_temp)\n",
    "        \n",
    "    print(\"Fitting done\")\n",
    "    \n",
    "    return {\n",
    "        'ks': ks_dist,\n",
    "        'p': pvals,\n",
    "        'fits': fits,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "npz_filename = f\"ks_stat-pvals_{split_key}_one_step.npz\"\n",
    "pickle_filename = f\"./fits_{split_key}_one_step.pickle\"\n",
    "\n",
    "if load:  # just load the already computed KS and p-values\n",
    "    loaded_2 = np.load(npz_filename)\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as infile:\n",
    "        fits_2 = pickle.load(infile)\n",
    "    \n",
    "    kolmog_smirn_2 = {\n",
    "        'pos': {\n",
    "            'ks': loaded_2['ks_pos'],\n",
    "            'p': loaded_2['pvals_pos'],\n",
    "            'fits': fits_2['pos'],\n",
    "        },\n",
    "        'neg': {\n",
    "            'ks': loaded_2['ks_neg'],\n",
    "            'p': loaded_2['pvals_neg'],\n",
    "            'fits': fits_2['neg'],\n",
    "        },\n",
    "        'abs': {\n",
    "            'ks': loaded_2['ks_abs'],\n",
    "            'p': loaded_2['pvals_abs'],\n",
    "            'fits': fits_2['abs'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "else:  # compute them and save them\n",
    "    size = 10000\n",
    "    n_tries = 10\n",
    "    print(\"\\n\\nFitting returns\")\n",
    "\n",
    "    kolmog_smirn_2 = {\n",
    "        ret_type: fit_gev_one_shot(\n",
    "            sorted_lr[ret_type].values,\n",
    "            size=size,\n",
    "            n_tries=n_tries,\n",
    "        )\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    np.savez_compressed(npz_filename, **{\n",
    "        'ks_pos': kolmog_smirn_2['pos']['ks'],\n",
    "        'pvals_pos': kolmog_smirn_2['pos']['p'],\n",
    "        'ks_neg': kolmog_smirn_2['neg']['ks'],\n",
    "        'pvals_neg': kolmog_smirn_2['neg']['p'],\n",
    "        'ks_abs': kolmog_smirn_2['abs']['ks'],\n",
    "        'pvals_abs': kolmog_smirn_2['abs']['p'],\n",
    "    })\n",
    "    \n",
    "    fits_2 = {\n",
    "        ret_type: kolmog_smirn_2[ret_type]['fits']\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    \n",
    "    with open(pickle_filename, 'wb') as outfile:\n",
    "        pickle.dump(fits_2, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Confronto tra i due flussi\n",
    "\n",
    "Ora che ho la statistica KS per ogni valore di k e del return, plot delle statistiche rispetto ai returns e a $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pval = 0.05\n",
    "\n",
    "valid_pvals = {\n",
    "    ret_type: kolmog_smirn[ret_type]['p'] <= min_pval\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "valid_pvals_2 = {\n",
    "    ret_type: kolmog_smirn_2[ret_type]['p'] <= min_pval\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "# trovo gli indici minimi\n",
    "i_min_ks = dict()\n",
    "i_min_ks_2 = dict()\n",
    "\n",
    "for ret_type in return_type:\n",
    "    # indici booleani di validità del p-value\n",
    "    mask = valid_pvals[ret_type]\n",
    "    mask_2 = valid_pvals_2[ret_type]\n",
    "    \n",
    "    # copio la KS per quel return\n",
    "    y = copy.deepcopy(kolmog_smirn[ret_type]['ks'])\n",
    "    y_2 = copy.deepcopy(kolmog_smirn_2[ret_type]['ks'])\n",
    "    \n",
    "    # dove il p-value > 0.05, setto la KS al massimo così non viene considerata\n",
    "    y[np.logical_not(mask)] = np.max(y)\n",
    "    y_2[np.logical_not(mask_2)] = np.max(y_2)\n",
    "    \n",
    "    # trovo gli indici di minima distanza KS per questo return\n",
    "    i_min_ks[ret_type] = np.argmin(y)\n",
    "    i_min_ks_2[ret_type] = np.argmin(y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le figure seguenti mostrano l'andamento di $d_{KS}$ in funzione di k e dei returns ordinati, su scala $x$ semilogaritmica, per entrambi i modi di calcolare il fitting della GEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot della statistica KS al variare del sorted return e di k\n",
    "truncation = 100\n",
    "end = -170\n",
    "p_labels = {\n",
    "    'pos': r'valid $p$ for $r$',\n",
    "    'neg': r'valid $p$ for $-r$',\n",
    "    'abs': r'valid $p$ for $|r|$',\n",
    "}\n",
    "\n",
    "p_height = {\n",
    "    'pos': 0.0,\n",
    "    'neg': -0.01,\n",
    "    'abs': -0.02\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(22, 14))\n",
    "\n",
    "# prima colonna: d_KS in funzione dei returns\n",
    "for ret_type in return_type:\n",
    "    # prendo le x complete e le tronco per il plot\n",
    "    x = sorted_lr[ret_type].values[1:]\n",
    "    x_trunc = x#[truncation:end]\n",
    "    \n",
    "    # prendo le x con i p-value validi nei due casi\n",
    "    mask = valid_pvals[ret_type]#[truncation:end]\n",
    "    x_ok = x_trunc[mask]\n",
    "    \n",
    "    mask_2 = valid_pvals_2[ret_type]#[truncation:end]\n",
    "    x_ok_2 = x_trunc[mask_2]\n",
    "    \n",
    "    # prendo le y complete e le tronco per il plot nei due casi\n",
    "    y = kolmog_smirn[ret_type]['ks']\n",
    "    y_trunc = y#[truncation:end]\n",
    "    \n",
    "    y_2 = kolmog_smirn_2[ret_type]['ks']\n",
    "    y_trunc_2 = y_2#[truncation:end]\n",
    "\n",
    "    ### primo flusso: GEV in due steps\n",
    "    ax[0, 0].semilogx(\n",
    "        x_trunc,\n",
    "        y_trunc,\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.5,\n",
    "        label=legend_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[0, 0].semilogx(\n",
    "        x_ok,\n",
    "        p_height[ret_type] * np.ones((len(x_ok, ))),\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.7,\n",
    "#         label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[0, 0].axvline(\n",
    "        x[i_min_ks[ret_type]],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ### secondo flusso: GEV in un colpo solo\n",
    "    ax[1, 0].semilogx(\n",
    "        x_trunc,\n",
    "        y_trunc_2,\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.5,\n",
    "        label=legend_labels[ret_type])\n",
    "    \n",
    "    ax[1, 0].semilogx(\n",
    "        x_ok_2,\n",
    "        p_height[ret_type] * np.ones((len(x_ok_2, ))),\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.7,\n",
    "#         label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[1, 0].axvline(\n",
    "        x[i_min_ks_2[ret_type]],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i, 0].set(\n",
    "        xlabel=r'$r$, $-r$, $|r|$',\n",
    "        ylabel=r'$d_{KS}$',\n",
    "        title=r'$d_{KS} ( r )$, method ' + str(i)\n",
    "    )\n",
    "    ax[i, 0].legend()\n",
    "\n",
    "    \n",
    "########################################################################\n",
    "# seconda colonna: d_KS in funzione di k\n",
    "for ret_type in return_type:\n",
    "    # prendo le x complete e le tronco per il plot\n",
    "    x = hill_estimation[ret_type]['kappas']\n",
    "    x_trunc = x#[truncation:end]\n",
    "    \n",
    "   # prendo le x con i p-value validi nei due casi\n",
    "    mask = valid_pvals[ret_type]#[truncation:end]\n",
    "    x_ok = x_trunc[mask]\n",
    "    \n",
    "    mask_2 = valid_pvals_2[ret_type]#[truncation:end]\n",
    "    x_ok_2 = x_trunc[mask_2]\n",
    "    \n",
    "    # prendo le y complete e le tronco per il plot nei due casi\n",
    "    y = kolmog_smirn[ret_type]['ks']\n",
    "    y_trunc = y#[truncation:end]\n",
    "    \n",
    "    y_2 = kolmog_smirn_2[ret_type]['ks']\n",
    "    y_trunc_2 = y_2#[truncation:end]\n",
    "\n",
    "    ### primo flusso: GEV in due steps\n",
    "    ax[0, 1].semilogx(\n",
    "        x_trunc,\n",
    "        y_trunc,\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.5,\n",
    "        label=legend_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[0, 1].semilogx(\n",
    "        x_ok,\n",
    "        p_height[ret_type] * np.ones((len(x_ok, ))),\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.7,\n",
    "        label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[0, 1].axvline(\n",
    "        x[i_min_ks[ret_type]],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    ### secondo flusso: GEV in un colpo solo\n",
    "    ax[1, 1].semilogx(\n",
    "        x_trunc,\n",
    "        y_trunc_2,\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.5,\n",
    "        label=legend_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[1, 1].semilogx(\n",
    "        x_ok_2,\n",
    "        p_height[ret_type] * np.ones((len(x_ok_2, ))),\n",
    "        color=colors[ret_type],\n",
    "        linestyle='',\n",
    "        marker='.',\n",
    "        markersize=0.7,\n",
    "#         label=p_labels[ret_type]\n",
    "    )\n",
    "    \n",
    "    ax[1, 1].axvline(\n",
    "        x[i_min_ks_2[ret_type]],\n",
    "        linestyle='-.',\n",
    "        color=colors[ret_type],\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i, 1].set(\n",
    "        xlabel=r'$k$',\n",
    "        ylabel=r'$d_{KS}$',\n",
    "        title=r'$d_{KS} ( k )$, method ' + str(i)\n",
    "    )\n",
    "    ax[i, 1].legend()\n",
    "\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "# ax[0, 0].get_shared_x_axes().join(ax[0, 0], ax[1, 0])\n",
    "# ax[0, 1].get_shared_x_axes().join(ax[0, 1], ax[1, 1])\n",
    "# ax[0, 1].set_ylim([-0.03, 0.125])\n",
    "# ax[0, 0].set_ylim([-0.03, 0.125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le linee sottostanti le figure, composte in realtà da punti, identificano quei valori per cui il test di Kolmogorov-Smirnov ha dato un _p_-value $p \\leq 0.05$, ed è quindi ritenuto statisticamente valido. Si nota come agli estremi di $k$ e dei returns il *p*-value non sia significativo e ci siano grosse oscillazioni, probabilmente dovute a instabilità numeriche nel calcolo della maximum likelihood.\n",
    "\n",
    "### 3.4 Minima $d_{KS}$ per trovare il miglior fit della GEV\n",
    "\n",
    "Ottimo, ora bisogna selezionare il minimo valore della statistica KS $d_{KS}$ che abbia un p-value valido ($p < 0.05$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_dks(d_ks, pvals, min_pval=min_pval):\n",
    "    dks = d_ks.copy()\n",
    "    invalid = pvals > min_pval\n",
    "    \n",
    "    dks[invalid] = np.min(dks) + 1\n",
    "    i_min = np.argmin(dks)\n",
    "    \n",
    "    return dks[i_min], i_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_kolmog_smirn = {\n",
    "    ret_type: find_min_dks(kolmog_smirn[ret_type]['ks'], kolmog_smirn[ret_type]['p'])\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "threshold_evt = {\n",
    "    'pos': sorted_lr['pos'][min_kolmog_smirn['pos'][1]],\n",
    "    'neg': sorted_lr['neg'][min_kolmog_smirn['neg'][1]],\n",
    "    'abs': sorted_lr['abs'][min_kolmog_smirn['abs'][1]],\n",
    "}\n",
    "\n",
    "min_kolmog_smirn_2 = {\n",
    "    ret_type: find_min_dks(kolmog_smirn_2[ret_type]['ks'], kolmog_smirn_2[ret_type]['p'])\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "threshold_evt_2 = {\n",
    "    'pos': sorted_lr['pos'][min_kolmog_smirn_2['pos'][1]],\n",
    "    'neg': sorted_lr['neg'][min_kolmog_smirn_2['neg'][1]],\n",
    "    'abs': sorted_lr['abs'][min_kolmog_smirn_2['abs'][1]],\n",
    "}\n",
    "\n",
    "print(\"Primo flusso: stima di gamma separata\")\n",
    "title_format = \"{:>15}\"*4\n",
    "row_format = \"{:>15}{:>15.4f}{:>15}{:>15.6f}\"\n",
    "print(title_format.format('Return type', 'Min d_KS', 'i', 'Return'))\n",
    "print(\"-\"*60)\n",
    "for ret_type in return_type:\n",
    "    print(row_format.format(\n",
    "        ret_type,\n",
    "        min_kolmog_smirn[ret_type][0],\n",
    "        min_kolmog_smirn[ret_type][1],\n",
    "        threshold_evt[ret_type]\n",
    "    ))\n",
    "    \n",
    "print(\"\\n\\nSecondo flusso: stima con MLE di tutti i parametri\")\n",
    "title_format = \"{:>15}\"*4\n",
    "row_format = \"{:>15}{:>15.4f}{:>15}{:>15.6f}\"\n",
    "print(title_format.format('Return type', 'Min d_KS', 'i', 'Return'))\n",
    "print(\"-\"*60)\n",
    "for ret_type in return_type:\n",
    "    print(row_format.format(\n",
    "        ret_type,\n",
    "        min_kolmog_smirn_2[ret_type][0],\n",
    "        min_kolmog_smirn_2[ret_type][1],\n",
    "        threshold_evt_2[ret_type]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sembrerebbe che la stima con MLE in un colpo solo sia **molto peggiore** di quella in due steps, vediamo graficamente se le distribuzioni fittano bene i dati allora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparo i dati per plottare\n",
    "titles = {\n",
    "    'pos': 'Positive extreme returns',\n",
    "    'neg': 'Negative extreme returns',\n",
    "    'abs': 'Absolute extreme returns',\n",
    "}\n",
    "\n",
    "xlabels = {\n",
    "    'pos': r'Positive extreme returns $r$',\n",
    "    'neg': r'Negative extreme returns $-r$',\n",
    "    'abs': r'Absolute extreme returns  $|r|$',\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}\n",
    "\n",
    "# plot\n",
    "fig, ax = pl.subplots(nrows=2, ncols=len(return_type), figsize=(18, 10), sharex=False, sharey=True)\n",
    "\n",
    "# sulle righe i due flussi\n",
    "for i in range(ax.shape[0]):\n",
    "    \n",
    "    # sulle colonne i 3 tipi di returns e le loro GEV fittate\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        if i == 0: # primo flusso\n",
    "            i_min = min_kolmog_smirn[ret_type][1]\n",
    "            \n",
    "            data = sorted_lr[ret_type].values[:i_min]\n",
    "            best_fit = fits[ret_type][i_min]\n",
    "            \n",
    "            title = 'Separate GEV fitting'\n",
    "        elif i == 1: # secondo flusso\n",
    "            i_min = min_kolmog_smirn_2[ret_type][1]\n",
    "            \n",
    "            data = sorted_lr[ret_type].values[:i_min]\n",
    "            best_fit = fits_2[ret_type][i_min]\n",
    "            \n",
    "            title = 'One shot GEV fitting'\n",
    "        \n",
    "        print(f\"method: {i}, ret_type: {ret_type}, n_extremes: {data.shape[0]}\")\n",
    "        \n",
    "        sns.distplot(\n",
    "            data,\n",
    "            color=colors[ret_type],\n",
    "            label=legend_labels[ret_type],\n",
    "            kde=False,\n",
    "            norm_hist=True,\n",
    "            ax=ax[i, j]\n",
    "        )\n",
    "        \n",
    "        _, b = ax[i, j].xaxis.get_data_interval()\n",
    "        x = np.linspace(0, b, 1000)\n",
    "        pdf = gev.pdf(x, *best_fit)\n",
    "        ax[i, j].plot(x, pdf, color=colors[ret_type], label='GEV pdf')\n",
    "        \n",
    "        ax[i, j].set_title(title)\n",
    "        ax[i, j].set_xlabel(xlabels[ret_type])\n",
    "        ax[i, j].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_min = min_kolmog_smirn['neg'][1]\n",
    "i_min_2 = min_kolmog_smirn_2['neg'][1]\n",
    "\n",
    "data = sorted_lr['neg'].values[:i_min]\n",
    "data_2 = sorted_lr['neg'].values[:i_min_2]\n",
    "\n",
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(14, 7))\n",
    "\n",
    "ax.plot(data_2, color='steelblue', label='One-step GEV')\n",
    "ax.plot(data, color='indianred', label='Two-steps GEV')\n",
    "\n",
    "ax.fill_between(np.arange(len(data_2)), 0, data_2, color='steelblue', alpha=0.5)\n",
    "ax.fill_between(np.arange(len(data)), 0, data, color='indianred', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r\"$k$\", fontsize=16)\n",
    "ax.set_ylabel(r\"$-r$\", fontsize=16)\n",
    "ax.legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In effetti sembra proprio che separare in due steps la ricerca del tail exponent e poi degli altri parametri sia benefico, visto che riduce il numero di estremi.\n",
    "Se faccio il fit in un colpo solo infatti, viene un threshold ancora peggiore, quindi meglio non deviare dal seminato del paper.\n",
    "\n",
    "Finora abbiamo quindi ottenuto:\n",
    "\n",
    "- le distribuzioni di probabilità degli extreme returns (positivi, negativi, assoluti)\n",
    "- i threshold che massimizzano il fitting della distribuzione *GEV* sugli extreme returns. Tali threshold possono essere quindi usati per determinare quali movimenti siano estremi e quali no\n",
    "\n",
    "Concludiamo quindi confrontando i threshold così ottenuti con i threshold del 95% percentile che abbiamo utilizzato finora per le azioni S&P500, e con quello che si otterrebbe ad utilizzare il $k^*$ calcolato con lo stimatore di Hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo dei threshold\n",
    "# quantili: attenzione che sono TUTTI POSITIVI\n",
    "thresholds = {\n",
    "    ret_type: {\n",
    "        q_type: returns[ret_type].abs().quantile(float(q_type) / 100)\n",
    "        for q_type in quantile_type[:-1]\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "for ret_type in return_type:\n",
    "    thresholds[ret_type]['evt'] = abs(threshold_evt[ret_type])\n",
    "\n",
    "extremes = {\n",
    "    ret_type: {\n",
    "        q_type: (returns[ret_type].abs() >= thresholds[ret_type][q_type]).astype(np.int8)\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "extremes_test = {\n",
    "    ret_type: {\n",
    "        q_type: (returns_test[ret_type].abs() >= thresholds[ret_type][q_type]).astype(np.int8)\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo i threshold di Hill solamente\n",
    "threshold_hill = {\n",
    "    'neg': sorted_lr['neg'].values[hill_estimation['neg']['k_opt']],\n",
    "    'pos': sorted_lr['pos'].values[hill_estimation['pos']['k_opt']]   \n",
    "}\n",
    "\n",
    "# plot dei threshold\n",
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(18, 9))\n",
    "\n",
    "dates = log_returns.index\n",
    "# training set\n",
    "ax.plot(\n",
    "    returns_before_after[split_key][0]['Adj Close'],\n",
    "    label='Log Returns - train',\n",
    "    color='slategrey',\n",
    "    alpha=0.25,\n",
    "    linestyle='',\n",
    "    marker='.'\n",
    ")\n",
    "# testing set\n",
    "ax.plot(\n",
    "    returns_before_after[split_key][1]['Adj Close'],\n",
    "    label='Log Returns - test',\n",
    "    color='navy',\n",
    "    alpha=0.25,\n",
    "    linestyle='',\n",
    "    marker='.'\n",
    ")\n",
    "\n",
    "# percentili\n",
    "ax.plot(dates, -thresholds['neg']['95'] * np.ones(len(dates)), color=colors['neg'], linestyle=':', label=r'95% percentile, $-r$')\n",
    "ax.plot(dates, thresholds['pos']['95'] * np.ones(len(dates)), color=colors['pos'], linestyle=':', label=r'95% percentile, $r$')\n",
    "\n",
    "# EVT, flusso 1\n",
    "ax.plot(dates, -thresholds['neg']['evt'] * np.ones(len(dates)), color=colors['neg'], linestyle='--', label=r'$-x_t$, EVT')\n",
    "ax.plot(dates, thresholds['pos']['evt'] * np.ones(len(dates)), color=colors['pos'], linestyle='--', label=r'$x_t$, EVT')\n",
    "\n",
    "# Hill\n",
    "ax.plot(dates, -threshold_hill['neg'] * np.ones(len(dates)), color=colors['neg'], linestyle='-.', label=r'$-x_t$, Hill')\n",
    "ax.plot(dates, threshold_hill['pos'] * np.ones(len(dates)), color=colors['pos'], linestyle='-.', label=r'$x_t$, Hill')\n",
    "\n",
    "ax.set(xlabel='Date', ylabel='Log Returns', title='Dow Jones Log Returns and thresholds', ylim=[-0.12, 0.12])\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può vedere dal plot, i percentili sono molto più stringenti rispetto al valore che massimizza il fitting della *GEV*, mentre i threshold calcolati con il solo stimatore di Hill sono più stringenti dei percentili.\n",
    "\n",
    "Vediamo anche di confermarlo con un po' di numeri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent(data, thresh_low, thresh_up):\n",
    "    indexes = np.logical_or(data.values <= thresh_low, data.values >= thresh_up)\n",
    "    num = len(data[indexes])\n",
    "    denom = len(data)\n",
    "    \n",
    "    return num / denom\n",
    "\n",
    "perc = log_returns['Adj Close'].quantile(q=[0.05, 0.95])\n",
    "\n",
    "extreme_percent_with_percentiles = get_percent(log_returns['Adj Close'], perc[0.05], perc[0.95])\n",
    "extreme_percent_with_evt = get_percent(log_returns['Adj Close'], -thresholds['neg']['evt'], thresholds['pos']['evt'])\n",
    "extreme_percent_with_evt_2 = get_percent(log_returns['Adj Close'], -threshold_evt_2['neg'], threshold_evt_2['pos'])\n",
    "\n",
    "print(\"{:>20}{:>15}\".format('Threshold type', 'Extremes %'))\n",
    "print(\"-\"*35)\n",
    "print(\"{:>20}{:>15.3f}\".format('percentile 5-95 %', extreme_percent_with_percentiles))\n",
    "print(\"{:>20}{:>15.3f}\".format('EVT', extreme_percent_with_evt))\n",
    "print(\"{:>20}{:>15.3f}\".format('EVT2', extreme_percent_with_evt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pratica, vuol dire che se usassimo i valori di threshold ricavati dalla *EVT* avremmo un dataset sicuramente più bilanciato, ma c'è da chiedersi se si possano effettivamente allora considerare \"estremi\". Non è troppo \"inclusivo\" un tale threshold?\n",
    "\n",
    "### 3.5 Calcolo dei $\\tau_Q$ e delle $Q$\n",
    "\n",
    "Calcoliamoli per poi usarli nella maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo i tau e calcolo i Q\n",
    "tau_q_95 = 1.0 / (1.0 - 0.95)\n",
    "tau_q_975 = 1.0 / (1.0 - 0.975)\n",
    "tau_q_99 = 1.0 / (1.0 - 0.99)\n",
    "\n",
    "# calcolo i quantili equivalenti ai threshold della EVT\n",
    "Q = {\n",
    "    ret_type: 1.0 - (sum(returns[ret_type].abs() >= abs(threshold_evt[ret_type])) / len(returns[ret_type]))\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "tau_q = {\n",
    "    ret_type: {\n",
    "        '95': tau_q_95,\n",
    "        '97.5': tau_q_975,\n",
    "        '99': tau_q_99,\n",
    "        'evt': 1.0 / (1.0 - Q[ret_type])\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcolo degli intervalli di ricorrenza e plot della loro distribuzione\n",
    "\n",
    "Ora che abbiamo i threshold possiamo calcolare gli intervalli di ricorrenza e vederne la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrence_intervals(is_extreme: pd.DataFrame):\n",
    "    \"\"\"Get the recurrence intervals durations between extremes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_extreme: pd.DataFrame\n",
    "        a DataFrame with the date on the index and 1 if the return at time t is extreme,\n",
    "        0 otherwise. Must contain a single column named 'extreme'\n",
    "    \"\"\"\n",
    "    assert isinstance(is_extreme.index, pd.DatetimeIndex)\n",
    "    assert len(is_extreme.columns) == 1\n",
    "    \n",
    "    # convert to int\n",
    "    data = is_extreme.astype(np.int8)\n",
    "    data.loc[:, 'date'] = data.index\n",
    "    data.index = pd.RangeIndex(len(is_extreme))\n",
    "    \n",
    "    data_is_extreme = data[data[data.columns[0]] == 1]\n",
    "    \n",
    "    intervals = []\n",
    "    for i in range(1, len(data_is_extreme)):\n",
    "        last_time = data_is_extreme.date.iloc[i - 1]\n",
    "        current_time = data_is_extreme.date.iloc[i]\n",
    "        \n",
    "        n_days = data_is_extreme.index[i] - data_is_extreme.index[i - 1]\n",
    "        \n",
    "        intervals.append((last_time, current_time, n_days))\n",
    "        \n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quantili vanno presi al 95%, 97.5%, 99%. Creo quindi il `dict` che contiene gli intervalli di ricorrenza, organizzati secondo il tipo di return ed il tipo di threshold (quantile o evt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo intervalli di ricorrenza - training set\n",
    "tmp_rec_int = {\n",
    "    ret_type: {\n",
    "        q_type: get_recurrence_intervals(pd.DataFrame(extremes[ret_type][q_type]))\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "recurrence_intervals = {\n",
    "    ret_type: {\n",
    "        q_type: pd.DataFrame(data={\n",
    "            'last_extreme': [x[0] for x in tmp_rec_int[ret_type][q_type]],\n",
    "            'current_extreme': [x[1] for x in tmp_rec_int[ret_type][q_type]],\n",
    "            'n_days': [x[2] for x in tmp_rec_int[ret_type][q_type]],\n",
    "        })\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "# calcolo intervalli di ricorrenza - testing set\n",
    "tmp_rec_int_test = {\n",
    "    ret_type: {\n",
    "        q_type: get_recurrence_intervals(pd.DataFrame(extremes_test[ret_type][q_type]))\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "recurrence_intervals_test = {\n",
    "    ret_type: {\n",
    "        q_type: pd.DataFrame(data={\n",
    "            'last_extreme': [x[0] for x in tmp_rec_int_test[ret_type][q_type]],\n",
    "            'current_extreme': [x[1] for x in tmp_rec_int_test[ret_type][q_type]],\n",
    "            'n_days': [x[2] for x in tmp_rec_int_test[ret_type][q_type]],\n",
    "        })\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plot istogrammi intervalli di ricorrenza\n",
    "\n",
    "Ora visualizzo graficamente la lunghezza degli intervalli di ricorrenza con degli istogrammi, rispettivamente per i returns positivi, negativi ed assoluti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_labels = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $r$',\n",
    "    'neg': r'Negative $-r$',\n",
    "    'abs': r'Absolute $|r|$',\n",
    "}\n",
    "\n",
    "y_lims = {\n",
    "    'pos': [0.0, 0.15],\n",
    "    'neg': [0.0, 0.15],\n",
    "    'abs': [0.0, 0.08],\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "# riga 1: training set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals[ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[0, i])\n",
    "\n",
    "    ax[0, i].legend()\n",
    "    ax[0, i].set(title=titles[ret_type] + \" | training set\", ylim=y_lims[ret_type])\n",
    "\n",
    "# riga 2: testing set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals_test[ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[1, i])\n",
    "\n",
    "    ax[1, i].legend()\n",
    "    ax[1, i].set(title=titles[ret_type] + \" | testing set\", ylim=y_lims[ret_type])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creazione delle tabelle come nel paper\n",
    "\n",
    "Ora creo le tabelle riassuntive come a pagina 9 del paper di [Jiang et al](https://doi.org/10.1080/14697688.2017.1373843).\n",
    "\n",
    "Prima mi creo due funzioncine e poi le chiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_table(intervals: pd.DataFrame, returns: pd.DataFrame, ret_type: str, thresh: float, col_name='perc'):\n",
    "    \"\"\"Get a single panel sub-table.\"\"\"\n",
    "    obsv = int(intervals.shape[0])\n",
    "    mean = intervals['n_days'].mean()\n",
    "    median = intervals['n_days'].median()\n",
    "    std_dev = intervals['n_days'].std()\n",
    "    skewness = intervals['n_days'].skew()\n",
    "    kurtosis = intervals['n_days'].kurt()\n",
    "    \n",
    "    ret_mean = returns.mean()\n",
    "    ret_std_dev = returns.std()\n",
    "    \n",
    "    m = (thresh - ret_mean) / ret_std_dev\n",
    "#     print(f\"\\nRet_type: {ret_type}, q_type: {col_name}\")\n",
    "#     print(f\"Threshold: {thresh:5.4f}, Mean: {ret_mean:5.4f}, m: {m:5.4f}\")\n",
    "        \n",
    "    acf, qstat, pvals = stattools.acf(intervals['n_days'].values, qstat=True, nlags=30)\n",
    "    rho1 = acf[1]\n",
    "    _, p_rho1 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[1:],\n",
    "        intervals['n_days'].shift(periods=1).values[1:],\n",
    "    )\n",
    "    \n",
    "    rho5 = acf[5]\n",
    "    _, p_rho5 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[5:],\n",
    "        intervals['n_days'].shift(periods=5).values[5:],\n",
    "    )\n",
    "\n",
    "    Q30 = qstat[-1]\n",
    "    p_Q30 = pvals[-1]\n",
    "    \n",
    "    index = pd.Index(data=[\n",
    "        'm',\n",
    "        'obsv',\n",
    "        'mean',\n",
    "        'median',\n",
    "        'stdev',\n",
    "        'skew',\n",
    "        'kurt',\n",
    "        'rho(1)',\n",
    "        'p-value(rho1)',\n",
    "        'rho(5)',\n",
    "        'p-value(rho5)',\n",
    "        'Q(30)',\n",
    "        'p-value(Q30)',\n",
    "    ])\n",
    "    \n",
    "    result = pd.DataFrame(data=[\n",
    "        [m],\n",
    "        [obsv],\n",
    "        [mean],\n",
    "        [median],\n",
    "        [std_dev],\n",
    "        [skewness],\n",
    "        [kurtosis],\n",
    "        [rho1],\n",
    "        [p_rho1],\n",
    "        [rho5],\n",
    "        [p_rho5],\n",
    "        [Q30],\n",
    "        [p_Q30],\n",
    "    ],\n",
    "    index=index,\n",
    "    columns=[col_name])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il titolo è la sezione della tabella (Negative/Positive/Absolute), i quantili sono quelli che mi interessano e finiranno sulle colonne della tabella ed il risultato è un `dict` che ha come chiavi i titoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    ret_type: {\n",
    "        q_type: get_single_table(recurrence_intervals[ret_type][q_type],\n",
    "                                 returns[ret_type],\n",
    "                                 ret_type,\n",
    "                                 thresh=thresholds[ret_type][q_type],\n",
    "                                 col_name=q_type)\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "panels = {\n",
    "    ret_type: pd.concat([tables[ret_type][q_type] for q_type in quantile_type], axis='columns')\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo le tabelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['abs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot degli autocorrelogrammi\n",
    "\n",
    "Vediamo con gli [autocorrelogammi](https://en.wikipedia.org/wiki/Correlogram) se c'è autocorrelazione nelle serie dei *recurrence interval*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle righe il tipo di threshold, sulle colonne il tipo di return\n",
    "fig, ax = pl.subplots(nrows=3, ncols=3, figsize=(27, 18))\n",
    "fig.suptitle(\"Autocorrelation plots\", fontsize=16)\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['95']['n_days'].values, lags=30, ax=ax[0][0], title=r'$-r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['95']['n_days'].values, lags=30, ax=ax[0][1], title=r'$r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['95']['n_days'].values, lags=30, ax=ax[0][2], title=r'$|r|$, 95%')\n",
    "ax[0, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['97.5']['n_days'].values, lags=30, ax=ax[1][0], title=r'$-r$, 97.5%')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['97.5']['n_days'].values, lags=30, ax=ax[1][1], title=r'$r$, 97.5%')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['97.5']['n_days'].values, lags=30, ax=ax[1][2], title=r'$|r|$, 97.5%')\n",
    "ax[1, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "# WARNING: non ci sono abbastanza dati\n",
    "# tsaplots.plot_acf(recurrence_intervals['neg']['99']['n_days'].values, lags=30, ax=ax[2][0], title=r'$-r$, 99%')\n",
    "# tsaplots.plot_acf(recurrence_intervals['pos']['99']['n_days'].values, lags=30, ax=ax[2][1], title=r'$r$, 99%')\n",
    "# tsaplots.plot_acf(recurrence_intervals['abs']['99']['n_days'].values, lags=30, ax=ax[2][2], title=r'$|r|$, 99%')\n",
    "# ax[2, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals['neg']['evt']['n_days'].values, lags=30, ax=ax[2][0], title=r'$-r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals['pos']['evt']['n_days'].values, lags=30, ax=ax[2][1], title=r'$r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals['abs']['evt']['n_days'].values, lags=30, ax=ax[2][2], title=r'$|r|$, EVT')\n",
    "ax[2, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "for a in ax[2]:\n",
    "    a.set_xlabel('lag in the recurrence interval array')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per interpretare i plot, bisogna ricordare che:\n",
    "\n",
    "- sulle $x$ c'è il lag della serie temporale relativa ai giorni tra i movimenti estremi, cioè quella degli intervalli di ricorrenza. Vuol dire che $x=22$ significa il 22° intervallo di ricorrenza visto nel passato, prima di quello attuale, non 22 giorni prima di oggi. La distanza in giorni potrebbe anche essere un anno o più.\n",
    "- sulle y c'è la correlazione di Pearson $R$\n",
    "\n",
    "Apparentemente c'è autocorrelazione nei recurrence intervals selezionati con il quantile $q_{0.95}$ fino a 6 per positivi e negativi, 2 per gli assoluti.\n",
    "\n",
    "Per i recurrence intervals con $q_{0.975}$ solo a 1 giorno per positivi e negativi, nessuna per gli assoluti.\n",
    "\n",
    "Per i recurrence intervals con $q_{0.99}$ non c'è autocorrelazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verifica relazione empirica\n",
    "\n",
    "Verifichiamo ora la relazione empirica $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_format = \"{:>15}\"*5\n",
    "row_format = \"{:>15.3f}{:>15}{:>15.3f}{:>15.3f}\" + \"{:>14.3f}%\"\n",
    "print(title_format.format('Quantile', 'Return type', 'tau_q', 'True mean', 'Error %'))\n",
    "print(title_format.format('-'*15, '-'*15, '-'*15, '-'*15, '-'*15))\n",
    "\n",
    "for q_type in quantile_type[:-1]:\n",
    "    for i, name in enumerate(return_type):\n",
    "        data_mean = recurrence_intervals[name][q_type]['n_days'].mean()\n",
    "        \n",
    "        q = float(q_type) / 100.0\n",
    "        tau = 1.0 / (1.0 - q)\n",
    "        \n",
    "        perc_diff = (tau - data_mean) / data_mean\n",
    "        \n",
    "        print(row_format.format(q, name, tau, data_mean, perc_diff * 100))\n",
    "        \n",
    "        if i == len(return_type) - 1:\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In effetti, la relazione è valida con un margine di errore massimo di circa il $5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Determinazione della *Hazard Probability*\n",
    "\n",
    "Gli autori definiscono la *hazard probability* come\n",
    "\n",
    "\\begin{equation}\n",
    "    W(\\Delta t | t) = \\frac{\\int_t^{t + \\Delta t} p(\\tau)d\\tau}{\\int_t^{\\infty}p(\\tau)d\\tau}\n",
    "\\end{equation}\n",
    "\n",
    "dove $p(\\tau)$ è la distribuzione di probabilità (`pdf` per scipy).\n",
    "\n",
    "La hazard probability definisce la probabilità che, dato che si è verificato un evento estremo $t$ giorni nel passato, ci sia un tempo di attesa $\\Delta t$ ulteriore prima di un altro evento estremo.\n",
    "Se consideriamo $W(1 | t)$ è simile al problema che abbiamo affrontato con la rete neurale.\n",
    "\n",
    "Ora, nota la ditribuzione $p(\\tau)$, si può derivare analiticamente l'integrale. Il problema è quindi: come trovare $p(\\tau)$, e che forma ha?\n",
    "\n",
    "Gli autori utilizzano una [stretched exponential distribution](https://en.wikipedia.org/wiki/Stretched_exponential_function), una [*q*-exponential distribution](https://en.wikipedia.org/wiki/Q-exponential_distribution) ed una [Weibull distribution](https://it.wikipedia.org/wiki/Distribuzione_di_Weibull). I parametri delle 3 distribuzioni vengono stimati tramite MLE.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "1. scegli una distribuzione (s-exp, q-exp, Weibull)\n",
    "2. riformula la parametrizzazione in funzione solo  dello *shape parameter*\n",
    "3. calcola la log-likelihood utilizzando una semplice ricerca a griglia sui parametri liberi\n",
    "4. i parametri che forniscono la massima log-likelihood sono quelli cercati, e trova la formula teorica della *hazard probability* con le equazioni del paper\n",
    "\n",
    "Cominciamo con la Weibull, ma prima creiamo una funzione che calcoli la *hazard probability* empirica, con la formula\n",
    "\n",
    "$$\n",
    "W_{emp}(\\Delta t | t) = \\frac{\\#(t < \\tau \\leq t + \\Delta t)}{\\#(\\tau > t)}\n",
    "$$\n",
    "\n",
    "dove al numeratore c'è il numero di recurrence intervals con valore compreso in $(t, t + \\Delta t]$, al denominatore il numero di recurrence intervals con valore maggiore di $t$, cioè nel range $(t, +\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empirical_hazard_prob(rec_ints: np.ndarray, t, delta_t):\n",
    "    \"\"\"Compute the empirical hazard probability.\"\"\"\n",
    "    assert isinstance(rec_ints, np.ndarray)\n",
    "    num = np.sum(np.logical_and(rec_ints > t, rec_ints <= t + delta_t))\n",
    "    denom = np.sum(rec_ints > t)\n",
    "    \n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Fitting della Weibull\n",
    "\n",
    "In `scipy.stats` è definita come\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&f(x, c) = c x^{c - 1} e^{-x^{c}} \\\\\n",
    "&f(x, c, loc, scale) = \\frac{1}{scale}f\\left(\\frac{x - loc}{scale}, c\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "dove $c$ è lo *shape parameter*. Nel paper invece è\n",
    "\n",
    "\\begin{equation}\n",
    "f(x, \\beta, \\alpha) = \\frac{\\alpha}{\\beta} \\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha - 1} e^{-\\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha}}\n",
    "\\end{equation}\n",
    "\n",
    "quindi la corrispondenza è\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&loc = 0 \\\\\n",
    "&\\beta = scale \\\\\n",
    "&shape = c = \\alpha\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ora dobbiamo stimare i parametri della Weibull con una maximum log-likelihood estimation (*MLE*). Riscrivendoli in funzione di $\\tau_Q$ e $\\beta = scale$ abbiamo\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)} \\\\\n",
    "cioè \\\\\n",
    "&\\beta = scale = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{c} \\right)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ricordiamo che $\\tau_Q = \\frac{1}{1 - Q}$ dove $Q$ è il quantile.\n",
    "\n",
    "A questo punto la MLE ha formula:\n",
    "\n",
    "$$ ln(L_w) = n \\cdot ln\\left( \\frac{c}{\\beta} \\right) + \\sum_{i=1}^{n} \\left[ (c - 1) ln\\left( \\frac{\\tau_i}{\\beta} \\right) - \\left( \\frac{\\tau_i}{\\beta} \\right)^c \\right] $$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $t_i$ il corrispondente valore dell'intervallo di ricorrenza (es: 14 giorni, 4 giorni...).\n",
    "\n",
    "Il flow è quindi, in questo caso:\n",
    "\n",
    "1. a seconda del percentile (95% o EVT) calcolare $Q$ e quindi $\\tau_Q$\n",
    "2. utilizzare una ricerca con step $1e-6$ sul parametro $c = \\alpha$, il quale risulta in un certo valore di $\\beta$\n",
    "3. utilizzare quei valori di $c$ e di $\\beta$ nella MLE\n",
    "4. trovare il massimo della MLE ed i corrispondenti valori di $c$ e $\\beta$\n",
    "5. urrà! Ora possiamo usarli nella *pdf* della distribuzione Weibull per ottenere l'hazard $W(\\Delta t | t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=True, parallel=True, nogil=True)\n",
    "def mle_weibull(rec_ints: np.ndarray, c: np.ndarray, beta: np.ndarray):\n",
    "    \"\"\"MLE estimation for weibull distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"       \n",
    "    m = beta.shape[0]\n",
    "    n = rec_ints.shape[0]\n",
    "\n",
    "    # log-likelihood    \n",
    "    log_likelihoods = np.zeros_like(beta)\n",
    "    \n",
    "    # precompute matrices for tau_beta and ln_tau_beta\n",
    "    tau_beta = np.zeros((n, m), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            tau_beta[i, j] = rec_ints[i] / beta[j]\n",
    "            \n",
    "    ln_tau_beta = np.log(tau_beta)\n",
    "    \n",
    "    c_beta = c / beta\n",
    "    n_ln_c_beta = n * np.log(c_beta)\n",
    "    c_1 = c - 1.0\n",
    "\n",
    "    for j in numba.prange(m):  # no progress indication, it's a parallel for loop\n",
    "        summ = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            summ += c_1[j] * ln_tau_beta[i, j] - tau_beta[i, j] ** c[j]\n",
    "            \n",
    "        log_likelihoods[j] = n_ln_c_beta[j] + summ\n",
    "        \n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della Weibull per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.arange(0.25, 2, 1e-3)\n",
    "sfg = sfun.gamma(1.0 + (1.0 / c))\n",
    "\n",
    "beta = {\n",
    "    ret_type: {\n",
    "        q_type: tau_q[ret_type][q_type] / sfg\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "i_ok = {\n",
    "    ret_type: {\n",
    "        q_type: np.argwhere(beta[ret_type][q_type] > 1e-6).flatten()\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "beta_ok = {\n",
    "    ret_type: {\n",
    "        q_type: beta[ret_type][q_type][i_ok[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "c_ok = {\n",
    "    ret_type: {\n",
    "        q_type: c[i_ok[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = True\n",
    "\n",
    "ll_weib_file = f\"./log-like-weib_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_weib_file, 'rb') as infile:\n",
    "        log_like_weib = pickle.load(infile)\n",
    "else:\n",
    "    log_like_weib = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_weib[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing Weibull MLE on quantile: {q_type}, c={c_ok[ret_type][q_type].shape}, beta={beta_ok[ret_type][q_type].shape}\")\n",
    "\n",
    "            ll = mle_weibull(x, c_ok[ret_type][q_type], beta_ok[ret_type][q_type])\n",
    "\n",
    "            log_like_weib[ret_type][q_type] = ll\n",
    "          \n",
    "    with open(ll_weib_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_weib, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            c_ok[ret_type][q_type],\n",
    "            log_like_weib[ret_type][q_type],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "\n",
    "        i_max = np.argmax(log_like_weib[ret_type][q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            c_ok[ret_type][q_type][i_max],\n",
    "            log_like_weib[ret_type][q_type][i_max],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set(xlabel=r'$c = \\alpha$', ylabel=r'$log(L_W)$')\n",
    "    a.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ora che abbiamo le MLE per i tre tipi di returns e i minimi, possiamo fittare la Weibull sui recurrence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_min = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_weib[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: c_ok[ret_type][q_type][i_min[ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_scale = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: tau_q[ret_type][q_type] / sfun.gamma(1.0 + 1.0 / best_shape['weibull'][ret_type][q_type])\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'weibull': {\n",
    "        ret_type: {\n",
    "            q_type: {\n",
    "                'shape': best_shape['weibull'][ret_type][q_type],\n",
    "                'scale': best_scale['weibull'][ret_type][q_type],\n",
    "                'loc': 0.0,\n",
    "            }\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fitting della stretched-exponential (s-exp)\n",
    "\n",
    "Creiamo ora la classe per la s-exp, che ha *pdf*:\n",
    "\n",
    "$$\n",
    "p(x, c, a, b) = a e^{-\\left( bx \\right)^c}\n",
    "$$\n",
    "\n",
    "dove $c$, $a$ e $b$ sono *shape parameters*, con $0 < c < 1$, $b \\geq 0$ e $a > 0$.\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della s-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{s-exp}) = n \\cdot \\ln(a) - \\sum_{i=1}^{n} (b \\cdot x_i)^c\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $a = \\frac{c \\Gamma \\left( \\frac{2}{c} \\right)}{\\left[ \\Gamma \\left( \\frac{1}{c} \\right) \\right]^2 \\tau_Q}$ e $b = \\frac{ \\Gamma \\left( \\frac{2}{c} \\right)}{\\Gamma \\left( \\frac{1}{c} \\right) \\tau_Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_b_sexp(c, tau_q):\n",
    "    \"\"\"Get the a and b params.\"\"\"\n",
    "    gamma_2_c = sfun.gamma(2.0 / c)\n",
    "    gamma_1_c = sfun.gamma(1.0 / c)\n",
    "    \n",
    "    b_all = gamma_2_c / (gamma_1_c * tau_q)\n",
    "    a_all = b_all * c / gamma_1_c\n",
    "    \n",
    "    return a_all, b_all\n",
    "\n",
    "@timeit\n",
    "def mle_sexp(rec_ints: np.ndarray, c: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for s-exponential distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    n = rec_ints.shape[0]\n",
    "    \n",
    "    a_all, b_all = get_a_b_sexp(c, tau_q)\n",
    "    \n",
    "    ln_a_all = np.log(a_all)\n",
    "    \n",
    "    ll = np.zeros((c.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j, c in enumerate(c):\n",
    "        ssum = 0\n",
    "        a = a_all[j]\n",
    "        b = b_all[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.power((b * rec_ints[i]), c)\n",
    "            \n",
    "        ll[j] = n * ln_a_all[j] - ssum\n",
    "    \n",
    "    ll[np.isnan(ll)] = -np.inf\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sexp = np.arange(1e-3, 1.0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = True\n",
    "\n",
    "ll_sexp_file = f\"./log-like-sexp_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_sexp_file, 'rb') as infile:\n",
    "        log_like_sexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_sexp = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_sexp[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing s-exp MLE on quantile: {q_type}, c={c_sexp.shape}\")\n",
    "\n",
    "            ll = mle_sexp(x, c_sexp, tau_q[ret_type][q_type])\n",
    "\n",
    "            log_like_sexp[ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_sexp['c'] = c_sexp\n",
    "\n",
    "    with open(ll_sexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_sexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_sexp[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape['s-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: c_sexp[i_max_sexp[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_a_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: get_a_b_sexp(best_shape['s-exp'][ret_type][q_type], tau_q[ret_type][q_type])[0]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_b_sexp = {\n",
    "    ret_type: {\n",
    "        q_type: get_a_b_sexp(best_shape['s-exp'][ret_type][q_type], tau_q[ret_type][q_type])[1]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_params['s-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: {\n",
    "            'shape': best_shape['s-exp'][ret_type][q_type],\n",
    "            'a': best_a_sexp[ret_type][q_type],\n",
    "            'b': best_b_sexp[ret_type][q_type],\n",
    "            'loc': 0.0,\n",
    "        }\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            c_sexp[truncation:],\n",
    "            log_like_sexp[ret_type][q_type][truncation:],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            c_sexp[i_max_sexp[ret_type][q_type]],\n",
    "            log_like_sexp[ret_type][q_type][i_max_sexp[ret_type][q_type]],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set(xlabel=r'$c = \\alpha$', ylabel=r'$log(L_{s-exp})$')\n",
    "    a.legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Fitting della q-exponential\n",
    "\n",
    "La terza distribuzione è la [q-exponential](https://en.wikipedia.org/wiki/Q-exponential_distribution).\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della q-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{q-exp}) = n \\cdot \\ln[\\lambda (2 - q)] - \\frac{1}{q - 1} \\sum_{i=1}^{n} \\ln[1 + (q - 1) \\lambda \\tau_i]\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $\\tau_i$ il valore dell'i-esimo recurrence interval, e il parametro $\\lambda$ si stima così:\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{1}{\\tau_Q(3 - 2q)}\n",
    "$$\n",
    "\n",
    "il parametro libero $q$ ha il range $\\left( 0, \\frac{3}{2} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def mle_qexp(rec_ints: np.ndarray, q: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for q-exponential distribution, given an array of q shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    assert np.all(q < 1.5)\n",
    "    \n",
    "    n = rec_ints.shape[0]\n",
    "    m = q.shape[0]\n",
    "    \n",
    "    lam = 1.0 / (tau_q * (3 - 2 * q))\n",
    "    \n",
    "    ll = np.zeros((q.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j in range(m):\n",
    "        ssum = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.log(1 + (q[j] - 1) * lam[j] * rec_ints[i])\n",
    "            \n",
    "        ll[j] = n * np.log(lam[j] * (2 - q[j])) - (1 / (q[j] - 1)) * ssum\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_qexp = np.arange(1.0, 1.5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = True\n",
    "\n",
    "ll_qexp_file = f\"./log-like-qexp_{split_key}.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_qexp_file, 'rb') as infile:\n",
    "        log_like_qexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_qexp = dict()\n",
    "\n",
    "    for ret_type in return_type:\n",
    "        log_like_qexp[ret_type] = dict()\n",
    "        print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "        for q_type in quantile_type:\n",
    "            x = recurrence_intervals[ret_type][q_type]['n_days'].values\n",
    "            print(f\"Computing q-exp MLE on quantile: {q_type}, c={q_qexp.shape}\")\n",
    "\n",
    "            ll = mle_qexp(x, q_qexp, tau_q[ret_type][q_type])\n",
    "            ll[np.isnan(ll)] = -np.inf\n",
    "            \n",
    "            log_like_qexp[ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_qexp['c'] = q_qexp\n",
    "\n",
    "    with open(ll_qexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_qexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_qexp = {\n",
    "    ret_type: {\n",
    "        q_type: np.argmax(log_like_qexp[ret_type][q_type])\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_shape['q-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: q_qexp[i_max_qexp[ret_type][q_type]]\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_lambda_qexp = {\n",
    "    ret_type: {\n",
    "        q_type: 1.0 / (tau_q[ret_type][q_type] * (3 - 2 * best_shape['q-exp'][ret_type][q_type]))\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}\n",
    "\n",
    "best_params['q-exp'] = {\n",
    "    ret_type: {\n",
    "        q_type: {\n",
    "            'shape': best_shape['q-exp'][ret_type][q_type],\n",
    "            'lambda': best_lambda_qexp[ret_type][q_type],\n",
    "            'loc': 0.0,\n",
    "        }\n",
    "        for q_type in quantile_type\n",
    "    }\n",
    "    for ret_type in return_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=1, figsize=(14, 15))\n",
    "\n",
    "# positive log-returns\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        ax[i].plot(\n",
    "            q_qexp,\n",
    "            log_like_qexp[ret_type][q_type],\n",
    "            color=colors_mle[q_type],\n",
    "            label=legend_labels_mle[q_type])\n",
    "        \n",
    "        ax[i].plot(\n",
    "            q_qexp[i_max_qexp[ret_type][q_type]],\n",
    "            log_like_qexp[ret_type][q_type][i_max_qexp[ret_type][q_type]],\n",
    "            marker='o',\n",
    "            color=colors_mle[q_type]\n",
    "        )\n",
    "    \n",
    "    ax[i].set(title=titles[ret_type])\n",
    "    \n",
    "for a in ax:\n",
    "    a.set_ylim([-1500, 0.0])\n",
    "    a.set(xlabel=r'$c = q$', ylabel=r'$log(L_{q-exp})$')\n",
    "    a.legend(loc='lower left')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Calcolo Hazard Probability\n",
    "\n",
    "Perfetto, ora ho i parametri della Weibull, della s-exp e della q-exp per ogni tipo di return e di threshold. Posso quindi ottenere la curva teorica per il fitting dei recurrence intervals.\n",
    "\n",
    "Per la Weibull è\n",
    "\n",
    "$$\n",
    "W_W(\\Delta t | t) = 1 - e^{\\left[ \\left( \\frac{t}{\\beta} \\right)^\\alpha - \\left( \\frac{t + \\Delta t}{\\beta} \\right)^\\alpha \\right]}\n",
    "$$\n",
    "\n",
    "dove $\\alpha = c^*$ lo *shape* ottimale, e $\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)}$ lo *scale* ottimale.\n",
    "\n",
    "Per la s-exp è:\n",
    "\n",
    "$$\n",
    "W_{s-exp}(\\Delta t | t) = \\frac{\\frac{bc}{a} - \\Gamma_l \\left( \\frac{1}{c}, (bt)^c \\right) - \\Gamma_u \\left( \\frac{1}{c}, [b(t + \\Delta t)]^c \\right)}{\\Gamma_u \\left( \\frac{1}{c}, (bt)^c \\right)}\n",
    "$$\n",
    "\n",
    "dove\n",
    "\n",
    "$$\n",
    "\\Gamma_u (a, x) = \\Gamma (a, x, +\\infty) = \\int_{x}^{+\\infty} t^{a - 1} e^{-t} dt\n",
    "$$\n",
    "\n",
    "è la *upper incomplete Gamma function* e\n",
    "\n",
    "$$\n",
    "\\Gamma_l (a, x) = \\Gamma (a, 0, x) = \\int_{0}^{x} t^{a - 1} e^{-t} dt\n",
    "$$\n",
    "\n",
    "è la *lower incomplete Gamma function*. Nel nostro caso, abbiamo quindi che $\\Gamma_l \\left( \\frac{1}{c}, (bt)^c \\right)$ si traduce in $a = 1/c$ e $x = (bt)^c$, mentre $\\Gamma_u \\left( \\frac{1}{c}, [b(t + \\Delta t)]^c \\right)$ in $a = 1/c$ e $x = [b(t + \\Delta t)]^c$.\n",
    "\n",
    "Per la q-exp è:\n",
    "\n",
    "$$\n",
    "W_{q-exp}(\\Delta t | t) = 1 - \\left[ 1 + \\frac{(q - 1)\\lambda \\Delta t}{1 + (q - 1)\\lambda t} \\right]^{1 - \\frac{1}{q - 1}}\n",
    "$$\n",
    "\n",
    "Mi creo allora le funzioni che le calcolano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_hazard(t, shape, scale, delta_t=1):\n",
    "    part_1 = np.power((t / scale), shape)\n",
    "    part_2 = np.power(((t + delta_t) / scale), shape)\n",
    "    \n",
    "    hazard = 1 - np.exp(part_1 - part_2)\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def sexp_hazard(t, c, a, b, delta_t=1):\n",
    "    num1 = (b * c / a)\n",
    "    num2 = sfun.gammainc((1.0 / c), np.power((b * t), c)) * sfun.gamma(1.0 / c)\n",
    "    num3 = sfun.gammaincc((1.0 / c), np.power(b * (t + delta_t), c)) * sfun.gamma(1.0 / c)\n",
    "    \n",
    "    num = num1 - num2 - num3\n",
    "    \n",
    "    denom = sfun.gammaincc((1.0 / c), np.power(b * t, c))\n",
    "    \n",
    "    hazard = num / denom\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def qexp_hazard(t, q, lam, delta_t=1):\n",
    "    num = (q - 1) * lam * delta_t\n",
    "    denom = 1 + (q - 1) * lam * t\n",
    "    exponent = 1 - (1 / (q - 1))\n",
    "    \n",
    "    hazard = 1 - np.power((1 + num / denom), exponent)\n",
    "    \n",
    "    return hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 60\n",
    "x = np.arange(max_t + 1)\n",
    "\n",
    "ret_type = 'neg'\n",
    "q_type = '99'\n",
    "\n",
    "theoretical_hazard = {\n",
    "    'weibull': weibull_hazard(\n",
    "        x,\n",
    "        best_params['weibull'][ret_type][q_type]['shape'],\n",
    "        best_params['weibull'][ret_type][q_type]['scale']\n",
    "    ),\n",
    "    's-exp': sexp_hazard(\n",
    "        x,\n",
    "        best_params['s-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['s-exp'][ret_type][q_type]['a'],\n",
    "        best_params['s-exp'][ret_type][q_type]['b'],\n",
    "    ),\n",
    "    'q-exp': qexp_hazard(\n",
    "        x,\n",
    "        best_params['q-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['q-exp'][ret_type][q_type]['lambda'],\n",
    "    )\n",
    "}\n",
    "\n",
    "empirical_hazard = np.array([\n",
    "    get_empirical_hazard_prob(recurrence_intervals[ret_type][q_type]['n_days'].values, t, 1)\n",
    "    for t in x\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(16, 9))\n",
    "\n",
    "dist_colors = {\n",
    "    'weibull': 'orchid',\n",
    "    's-exp': 'orangered',\n",
    "    'q-exp': 'mediumblue'\n",
    "}\n",
    "\n",
    "dist_labels = {\n",
    "    'weibull': r'$W_W$',\n",
    "    's-exp': r'$W_{s-exp}$',\n",
    "    'q-exp': r'$W_{q-exp}$',\n",
    "}\n",
    "\n",
    "for dist_type in distribution_type:\n",
    "    ax.plot(\n",
    "        x,\n",
    "        theoretical_hazard[dist_type],\n",
    "        color=dist_colors[dist_type],\n",
    "        label=dist_labels[dist_type])\n",
    "    \n",
    "ax.plot(\n",
    "    x,\n",
    "    empirical_hazard,\n",
    "    label=r'$W_{emp}$',\n",
    "    color='black',\n",
    "    linestyle='-',\n",
    "    marker='o',\n",
    "    markersize=1,\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(r'$t$', fontsize=16)\n",
    "ax.set_ylabel(r'$W(1 | t)$', fontsize=16)\n",
    "ax.set_title(r'Hazard probability for $q = 0.99$', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calcolo del miglior threshold $w_t$ massimizzando la *utility* $U(\\theta)$\n",
    "\n",
    "Ora dobbiamo calcolare il miglior threshold $w_t$ oltre il quale si dà il warning, cioè:\n",
    "\n",
    "- se $W(1|t) \\geq w_t$ --> warning --> 1\n",
    "- se $W(1|t) < w_t$ --> no warning --> 0\n",
    "\n",
    "Bisogna definire un peso $\\theta$ che si attribuisce alla Recall o al FPR, dove un valore di $\\theta$ maggiore dà più peso alla Recall. Inoltre, si definiscono due funzioni:\n",
    "\n",
    "- la *loss function*, che utilizza il peso:\n",
    "$$\n",
    "L(\\theta) = \\theta (1 - Recall) + (1 - \\theta)FPR\n",
    "$$\n",
    "- la *utility function*, che dipende dalla *loss*:\n",
    "$$\n",
    "U(\\theta) = \\min(\\theta, 1 - \\theta) - L(\\theta)\n",
    "$$\n",
    "che deve essere $U > 0$ per essere utile, e va massimizzata sul *training set*\n",
    "\n",
    "I passi per farlo sono i seguenti, utilizzando le distribuzioni fittate con i parametri migliori `best_params`:\n",
    "\n",
    "1. [x] fissare un valore $\\hat{\\theta}$ per il parametro $\\theta$\n",
    "2. [x] visto che la $Recall = f(w_t)$ e $FPR = g(w_t)$, far variare $w_t \\in [0, 1]$ per ottenere tutti i possibili valori di $U(\\theta^*)$, cioé:\n",
    "    1. [x] scegliere un $w_t \\in [0, 1] = w_t^i$\n",
    "    2. [x] calcolare le hazard probability delle tre distribuzioni $W_W(\\Delta t|t)$, $W_{s-exp}(\\Delta t|t)$ e $W_{q-exp}(\\Delta t|t)$ e trasformarle nel target binario $[0, 1]$ a seconda che siano minori o maggiori di $w_t$. Il $\\Delta t$ è il periodo tra un intervallo di ricorrenza e l'altro a questo punto\n",
    "    3. [x] calcolare le metriche $Recall(w_t^i)$, $FPR(w_t^i)$, $KSS(w_t^i)$ dove\n",
    "    $$\n",
    "    KSS = Recall - FPR\n",
    "    $$\n",
    "    4. [x] calcolare quindi la loss $L(\\hat{\\theta}, w_t^i) = (1 - Recall(w_t^i))\\hat{\\theta} + (1 - \\hat{\\theta})FPR(w_t^i)$\n",
    "    5. [x] calcolare di conseguenza il valore della utility $U(\\hat{\\theta})|_{w_t^i}$\n",
    "3. [x] dopo aver svolto il punto 2 per ogni valore di $w_t \\in [0, 1]$, selezionare il massimo $w_t$ con $argmax_{w_t} U(\\hat{\\theta})$\n",
    "4. [ ] plot della ROC curve per il training ed il testing set\n",
    "    1. [x] training set\n",
    "    2. [ ] testing set\n",
    "\n",
    "Cominciamo col definire $\\theta$, il range di $w_t$ e una funzione per la recall, l'FPR e il KSS score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scelta di theta\n",
    "theta = 0.5\n",
    "\n",
    "# w_t\n",
    "n_points = 1000\n",
    "w_t = np.linspace(0, 1, n_points + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definisco la *loss* e la *utility*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(theta, recall, fpr):\n",
    "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
    "    assert theta >= 0.0 and theta <= 1.0\n",
    "    \n",
    "    return theta * (1 - recall) + (1 - theta) * fpr\n",
    "\n",
    "def utility_function(theta, loss):\n",
    "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
    "    return min(theta, 1 - theta) - loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e una funzione che, data la funzione di *hazard* teorica, calcola la probabilità in ogni giorno che ci sia un estremo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione \n",
    "def hazard_prob(hazard_fn, extremes: np.ndarray):\n",
    "    \"\"\"\n",
    "    For every time t, predict the hazard probability using the supplied hazard function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hazard_fn: Callable[[int], float]\n",
    "        function that returns the hazard probability W(1|t), where t is the time passed\n",
    "        since the last extreme event\n",
    "    \n",
    "    extremes: np.ndarray\n",
    "        binary array of extremes, where 1 means extreme and 0 means normal, tim-ordered\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob: np.ndarray\n",
    "        hazard probability, at every time t, that there will be an extreme event\n",
    "    \"\"\"\n",
    "    assert isinstance(extremes, np.ndarray)\n",
    "    \n",
    "    ext_ind = np.argwhere(extremes == 1).flatten()\n",
    "    assert ext_ind.shape[0] >= 2  # ci sono almeno 2 estremi, sennò tutto questo non ha senso\n",
    "    \n",
    "    n = extremes.shape[0]\n",
    "    probs = np.zeros((n, ), dtype=np.float64)\n",
    "    \n",
    "    # curr_extreme_ind è sempre sull'ultimo estremo visto, a partire dall'inizio, fino al penultimo\n",
    "    # next_extreme_ind è sempre sul prossimo estremo, fino all'ultimo\n",
    "    for curr_extreme_ind, next_extreme_ind in zip(ext_ind[:-1], ext_ind[1:]):\n",
    "        t = 1\n",
    "        \n",
    "        while curr_extreme_ind + t <= next_extreme_ind:\n",
    "            probs[curr_extreme_ind + t] = hazard_fn(t)\n",
    "            if probs[curr_extreme_ind + t] < 0.0:\n",
    "                ipdb.set_trace()\n",
    "            t = t + 1\n",
    "            \n",
    "    # ora gli ultimi, può capitare che l'ultimo estremo non sia l'ultimo elemento di extremes\n",
    "    curr_extreme_ind = ext_ind[-1]\n",
    "    if curr_extreme_ind < n - 1:\n",
    "        t = 1\n",
    "        \n",
    "        while curr_extreme_ind + t < n:\n",
    "            probs[curr_extreme_ind + t] = hazard_fn(t)\n",
    "            t = t + 1\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bene, ora calcolo le probabilità teoriche per ogni tipo di distribuzione, di return e di quantile.\n",
    "\n",
    "Ogni volta è la migliore versione per quel tipo di return, di quantile e distribuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hazard_probabilities(best_params, extremes, verbose=False):\n",
    "    \"\"\"Get all the hazard probabilities, for every distribution, return and threshold type.\"\"\"\n",
    "    # per ogni distribuzione, tipo di ritorno e quantile calcola l'hazard probability\n",
    "    hazard_probabilities = dict()\n",
    "\n",
    "    for dist_type in distribution_type:\n",
    "        hazard_probabilities[dist_type] = dict()\n",
    "\n",
    "        for ret_type in return_type:\n",
    "            hazard_probabilities[dist_type][ret_type] = dict()\n",
    "\n",
    "            for q_type in quantile_type:\n",
    "                if verbose:\n",
    "                    print(f\"\\nDist: {dist_type}\\t ret_type: {ret_type}\\t q_type: {q_type}\")\n",
    "                bp = best_params[dist_type][ret_type][q_type]\n",
    "                ext = extremes[ret_type][q_type].values\n",
    "\n",
    "                if dist_type == 'weibull':\n",
    "                    shape = bp['shape']\n",
    "                    scale = bp['scale']\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Using Weibull with params shape: {shape:4.3f}, scale: {scale:4.3f}\")\n",
    "\n",
    "                    def f(x):\n",
    "                        return weibull_hazard(x, shape, scale)\n",
    "\n",
    "\n",
    "                    hazard_probabilities[dist_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "\n",
    "                elif dist_type == 's-exp':\n",
    "                    a = bp['a']\n",
    "                    b = bp['b']\n",
    "                    c = bp['shape']\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"Using s-exp with params a: {a:4.3f}, b: {b:4.3f}, c: {c:4.3f}\")\n",
    "\n",
    "                    def f(x):\n",
    "                        return sexp_hazard(x, c, a, b)\n",
    "\n",
    "                    hazard_probabilities[dist_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "\n",
    "                elif dist_type == 'q-exp':\n",
    "                    q = bp['shape']\n",
    "                    lam = bp['lambda']\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Using q-exp with params q: {q:4.3f}, lambda: {lam:4.3f}\")\n",
    "\n",
    "                    def f(x):\n",
    "                        return qexp_hazard(x, q, lam)\n",
    "\n",
    "                    hazard_probabilities[dist_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "                else:\n",
    "                    raise ValueError(f\"unrecognized distribution name {dist_type}\")\n",
    "                    \n",
    "    return hazard_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_probabilities = get_all_hazard_probabilities(best_params, extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifichiamo che non ci siano elementi negativi (quindi calcoli spurii):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist_type in distribution_type:\n",
    "    for ret_type in return_type:\n",
    "        for q_type in quantile_type:\n",
    "            if np.any(hazard_probabilities[dist_type][ret_type][q_type] < 0.0):\n",
    "                print(f\"Errori in dist: {dist_type}, ret_type: {ret_type}, q_type: {q_type}\")\n",
    "\n",
    "            if np.any(hazard_probabilities[dist_type][ret_type][q_type] > 1.0):\n",
    "                print(f\"Probabilità > 1 in dist: {dist_type}, ret_type: {ret_type}, q_type: {q_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, prendiamoli come errori numerici.\n",
    "\n",
    "Ora, a seconda del threshold $w_t$ convertiamo le probabilità in un target binario e calcoliamo le performance, la loss e la utility per ogni valore di $w_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(prob: np.ndarray, thresh: float):\n",
    "    assert thresh <= 1.0 and thresh >= 0.0\n",
    "    \n",
    "    return (prob >= thresh).astype(np.int8)\n",
    "\n",
    "\n",
    "def recall_fpr_kss(y_true, y_pred):\n",
    "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
    "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "    tn = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    fp = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        y_pred\n",
    "    ))\n",
    "    fn = np.sum(np.logical_and(\n",
    "        y_true,\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    \n",
    "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
    "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
    "    \n",
    "    kss = recall - fpr\n",
    "    \n",
    "    return recall, fpr, kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per tutti i valori possibili del threshold\n",
    "def optimize_wt(w, haz_probs, extremes, theta):\n",
    "    \"\"\"Find the best threshold and the performance with the supplied w.\"\"\"\n",
    "    # devo salvarmi le recall e gli fpr per ogni valore di w_t, dist, return, quantile\n",
    "    recalls = {\n",
    "        dist_type: {\n",
    "            ret_type: {\n",
    "                q_type: np.zeros((len(w_t), ), dtype=np.float64)\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    # per tutti i w_t = thresh\n",
    "    for i, thresh in enumerate(w):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"iteration: {i}/{len(w)}\")\n",
    "        # per tutte le distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "\n",
    "            # per tutti i tipi di returns\n",
    "            for ret_type in return_type:\n",
    "\n",
    "                # per tutti i tipi di threshold/quantile\n",
    "                for q_type in quantile_type:\n",
    "                    y_pred = to_binary(haz_probs[dist_type][ret_type][q_type], thresh)\n",
    "                    y_true = extremes[ret_type][q_type]\n",
    "\n",
    "                    recall, fpr, kss = recall_fpr_kss(y_true, y_pred)\n",
    "                    loss = loss_function(theta, recall, fpr)\n",
    "                    utility = utility_function(theta, loss)\n",
    "\n",
    "                    recalls[dist_type][ret_type][q_type][i] = recall\n",
    "                    fprs[dist_type][ret_type][q_type][i] = fpr\n",
    "                    ksss[dist_type][ret_type][q_type][i] = kss\n",
    "                    losses[dist_type][ret_type][q_type][i] = loss\n",
    "                    utilities[dist_type][ret_type][q_type][i] = utility\n",
    "             \n",
    "    # gli indici in w_t dove c'è la combinazione migliore di distribuzione, return e quantili\n",
    "    best_indexes = {\n",
    "        dist_type: {\n",
    "            ret_type: {\n",
    "                q_type: np.argmax(utilities[dist_type][ret_type][q_type])\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "    \n",
    "    return recalls, fprs, ksss, losses, utilities, best_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le curve ROC per ogni return, threshold e distribuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls, fprs, ksss, losses, utilities, best_indexes = optimize_wt(w_t, hazard_probabilities, extremes, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(\"ROC curves for the training set\", fontsize=16)\n",
    "\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            i_sorted = np.argsort(fprs[dist_type][ret_type][q_type])\n",
    "\n",
    "            x = fprs[dist_type][ret_type][q_type][i_sorted]\n",
    "            y = recalls[dist_type][ret_type][q_type][i_sorted]\n",
    "\n",
    "            ax[i, j].plot(x, y, color=dist_colors[dist_type], alpha=0.75, label=str.title(dist_type))\n",
    "\n",
    "        ax[i, j].plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "\n",
    "        ax[i, j].legend(loc='lower right', fontsize=11)\n",
    "        \n",
    "        ax[i, j].set_xlim([0, 1])\n",
    "        ax[i, j].set_ylim([0, 1])\n",
    "        \n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predizione nel periodo out-of-sample (test set)\n",
    "\n",
    "Ora bisogna utilizzare le 3 distribuzioni, già fittate sul training set, per predire sul test set.\n",
    "\n",
    "Riutilizzo il codice appena usato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_probabilities_test = get_all_hazard_probabilities(best_params, extremes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_test, fprs_test, ksss_test, losses_test, utilities_test, best_indexes_test = optimize_wt(w_t, hazard_probabilities_test, extremes_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(\"ROC curves for the test set\", fontsize=16)\n",
    "\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            i_sorted = np.argsort(fprs_test[dist_type][ret_type][q_type])\n",
    "\n",
    "            x = fprs_test[dist_type][ret_type][q_type][i_sorted]\n",
    "            y = recalls_test[dist_type][ret_type][q_type][i_sorted]\n",
    "\n",
    "            ax[i, j].plot(x, y, color=dist_colors[dist_type], alpha=0.75, label=str.title(dist_type))\n",
    "\n",
    "        ax[i, j].plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "\n",
    "        ax[i, j].legend(loc='lower right', fontsize=11)\n",
    "        \n",
    "        ax[i, j].set_xlim([0, 1])\n",
    "        ax[i, j].set_ylim([0, 1])\n",
    "        \n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confronto in-sample vs out-of-sample\n",
    "\n",
    "Confrontiamo allora le performance su training e test set.\n",
    "\n",
    "Bisogna tenere a mente, però, che il training set si ferma **prima** della crisi finanziaria del 2007-2008, quindi è possibile che il modello così sviluppato abbia performance migliori sul testing set piuttosto che sul training set, poiché in quel periodo ci sono molti estremi.\n",
    "\n",
    "### 8.1 Plot di $\\tau$ vs $p(\\tau)$\n",
    "\n",
    "Confrontiamo il fitting delle tre distribuzioni sui ritorni negativi con threshold al 99%. Nel plot ci sarà:\n",
    "\n",
    "- la *PDF* empirica\n",
    "- la *PDF* della Weibull\n",
    "- la *PDF* della s-exponential\n",
    "- la *PDF* della q-exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - forse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Plot di $W(1|t)$ e $W_{emp}(1|t)$\n",
    "\n",
    "Vediamo le hazard probability empiriche e fittate, sia sul training che sul testing set, coi returns negativi e threshold 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 60\n",
    "x = np.arange(max_t + 1)\n",
    "\n",
    "ret_type = 'neg'\n",
    "q_type = '99'\n",
    "\n",
    "theoretical_hazard_test = {\n",
    "    'weibull': weibull_hazard(\n",
    "        x,\n",
    "        best_params['weibull'][ret_type][q_type]['shape'],\n",
    "        best_params['weibull'][ret_type][q_type]['scale']\n",
    "    ),\n",
    "    's-exp': sexp_hazard(\n",
    "        x,\n",
    "        best_params['s-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['s-exp'][ret_type][q_type]['a'],\n",
    "        best_params['s-exp'][ret_type][q_type]['b'],\n",
    "    ),\n",
    "    'q-exp': qexp_hazard(\n",
    "        x,\n",
    "        best_params['q-exp'][ret_type][q_type]['shape'],\n",
    "        best_params['q-exp'][ret_type][q_type]['lambda'],\n",
    "    )\n",
    "}\n",
    "\n",
    "empirical_hazard_test = np.array([\n",
    "    get_empirical_hazard_prob(recurrence_intervals_test[ret_type][q_type]['n_days'].values, t, 1)\n",
    "    for t in x\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
    "\n",
    "dist_colors = {\n",
    "    'weibull': 'orchid',\n",
    "    's-exp': 'orangered',\n",
    "    'q-exp': 'mediumblue'\n",
    "}\n",
    "\n",
    "dist_labels = {\n",
    "    'weibull': r'$W_W$',\n",
    "    's-exp': r'$W_{s-exp}$',\n",
    "    'q-exp': r'$W_{q-exp}$',\n",
    "}\n",
    "\n",
    "for dist_type in distribution_type:\n",
    "    ax[0].plot(\n",
    "        x,\n",
    "        theoretical_hazard[dist_type],\n",
    "        color=dist_colors[dist_type],\n",
    "        label=dist_labels[dist_type]\n",
    "    )\n",
    "    \n",
    "    ax[1].plot(\n",
    "        x,\n",
    "        theoretical_hazard_test[dist_type],\n",
    "        color=dist_colors[dist_type],\n",
    "        label=dist_labels[dist_type]\n",
    "    )\n",
    "    \n",
    "ax[0].plot(\n",
    "    x,\n",
    "    empirical_hazard,\n",
    "    label=r'$W_{emp}$',\n",
    "    color='black',\n",
    "    linestyle='-',\n",
    "    marker='o',\n",
    "    markersize=1,\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax[1].plot(\n",
    "    x,\n",
    "    empirical_hazard_test,\n",
    "    label=r'$W_{emp}$',\n",
    "    color='black',\n",
    "    linestyle='-',\n",
    "    marker='o',\n",
    "    markersize=1,\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "for a in ax:\n",
    "    a.legend(fontsize=14)\n",
    "    a.set_xlabel(r'$t$', fontsize=16)\n",
    "    a.set_ylabel(r'$W(1 | t)$', fontsize=16)\n",
    "\n",
    "ax[0].set_title(r'Hazard probability for $q = 0.99$ | training set', fontsize=16)\n",
    "ax[1].set_title(r'Hazard probability for $q = 0.99$ | test set', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Recall, FPR, KSS score al variare di $w_t$\n",
    "\n",
    "Vediamo ora come i valori di performance sul training e testing set, per il threshold ottimale $w_t^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i migliori threshold per w_t, calcolati sul trainin set\n",
    "best_thresholds = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: w_t[best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# recall per w_t ottimale\n",
    "best_recalls = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: recalls[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "best_recalls_test = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: recalls_test[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# fpr per w_t ottimale\n",
    "best_fprs = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: fprs[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "best_fprs_test = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: fprs_test[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# kss per w_t ottimale\n",
    "best_ksss = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: ksss[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "best_ksss_test = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: ksss_test[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# utility per w_t ottimale\n",
    "best_utilities = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: utilities[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "best_utilities_test = {\n",
    "    dist_type: {\n",
    "        ret_type: {\n",
    "            q_type: utilities_test[dist_type][ret_type][q_type][best_indexes[dist_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSA NON FUNZIONA\n",
    "\n",
    "- [ ] La EVT ha come shape parameter un numero maggiore di 1, non va bene perché la curva qui sopra viene al contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
