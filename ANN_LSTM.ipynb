{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rete neurale per extreme returns su 2 azioni\n",
    "\n",
    "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [ ] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
    "- [ ] calcolo dei log returns\n",
    "- [ ] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
    "- [ ] creazione estremi al 95%\n",
    "- [ ] oversampling aggiungendo del rumore gaussiano\n",
    "- [ ] addestramento rete con hyperparameter optimization\n",
    "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
    "- [ ] confronto con i risultati del modello probabilistico\n",
    "- [ ] conclusioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import List, Union\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.testing as pt\n",
    "import scipy.stats\n",
    "import scipy.special as sfun\n",
    "from scipy.stats import genextreme as gev\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "from statsmodels.tsa import stattools\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "\n",
    "import numba\n",
    "\n",
    "# import del @timeit\n",
    "from my_timeit import timeit\n",
    "\n",
    "%pdb on\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un po' di dichiarazioni utili per il seguito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_type = ['min_vol', 'max_vol']\n",
    "return_type = ['pos', 'neg', 'abs']\n",
    "q_type = '95'\n",
    "\n",
    "stock_codes = {\n",
    "    'min_vol': '9CE4C7',\n",
    "    'max_vol': 'E28F22'\n",
    "}  # già trovate in Paper-azioni.ipynb\n",
    "\n",
    "stock_colors = {\n",
    "    'min_vol': 'palegoldenrod',\n",
    "    'max_vol': 'coral',\n",
    "}\n",
    "\n",
    "# i giorni sono i primi disponibili in quel mese nei dati\n",
    "split_dates = {\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
    "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
    "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
    "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
    "    'last_train': datetime.datetime(2017, 1, 3), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importazione dei dati \n",
    "\n",
    "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(prices_path)\n",
    "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
    "prices.index = prices['date']\n",
    "prices.drop(columns=['date'], inplace=True)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
    "\n",
    "I prezzi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
    "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
    "prices_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I volumi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
    "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
    "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
    "volume_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora calcoliamo i log-returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
    "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
    "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
    "\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_temp = {\n",
    "    s_type: log_returns.loc[:, stock_codes[s_type]]\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "lr_train_for_percentiles = dict()\n",
    "lr_test_for_percentiles = dict()\n",
    "returns_train_for_percentiles = dict()\n",
    "\n",
    "# aggiungiamo i dati in modalità taglia-e-cuci\n",
    "for s_type in stock_type:\n",
    "    # training set\n",
    "    lr_train_1 = lr_temp[s_type] \\\n",
    "        [lr_temp[s_type].index < split_dates['subprime-crisis-halfway']]\n",
    "    lr_train_2 = lr_temp[s_type] \\\n",
    "        [(lr_temp[s_type].index >= split_dates['eu-debt-halfway']) & \\\n",
    "         (lr_temp[s_type].index < split_dates['last_train'])]\n",
    "    \n",
    "    lr_train_for_percentiles[s_type] = pd.concat([lr_train_1, lr_train_2], axis='index')\n",
    "    \n",
    "    # test set\n",
    "    lr_test_1 = lr_temp[s_type] \\\n",
    "        [lr_temp[s_type].index >= split_dates['last_train']]\n",
    "    lr_test_2 = lr_temp[s_type] \\\n",
    "        [(lr_temp[s_type].index < split_dates['eu-debt-halfway']) & \\\n",
    "         (lr_temp[s_type].index >= split_dates['subprime-crisis-halfway'])]\n",
    "    \n",
    "    lr_test_for_percentiles[s_type] = pd.concat([lr_test_1, lr_test_2], axis='index')\n",
    "    \n",
    "    # returns train, tutti POSITIVI\n",
    "    returns_train_for_percentiles[s_type] = {\n",
    "        'pos': lr_train_for_percentiles[s_type][lr_train_for_percentiles[s_type] > 0.0],\n",
    "        'neg': -(lr_train_for_percentiles[s_type][lr_train_for_percentiles[s_type] < 0.0]),\n",
    "        'abs': lr_train_for_percentiles[s_type].abs()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bene, ora che ho i dataset così creati, uso il training set per calcolare i thresholds con i percentili al 95%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: returns_train_for_percentiles[s_type][ret_type].quantile(0.95)\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creazione dataset train-test\n",
    "\n",
    "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
    "\n",
    "- X: cubo dati\n",
    "- y: estremo si/no\n",
    "\n",
    "Per prima cosa, creo le due serie di estremi/non estremi, che poi potrò spezzettare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_all = dict()\n",
    "returns_all = dict()\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    returns_all[s_type] = log_returns.loc[:, s_code]\n",
    "    extremes_all[s_type] = dict()\n",
    "    \n",
    "    ext = np.logical_or(\n",
    "        log_returns.loc[:, s_code] >= thresholds[s_type]['pos'][q_type],\n",
    "        log_returns.loc[:, s_code] <= -thresholds[s_type]['neg'][q_type],\n",
    "    )\n",
    "    \n",
    "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Funzioni per la creazione dati\n",
    "\n",
    "Ora, con i returns e gli estremi già calcolati, possiamo creare i dataset di training e testing.\n",
    "\n",
    "L'idea è di dare i dati completi, l'indice di inizio, l'indice di fine e la bptt a una funzione che crea il cubo `X` e il vettore `y`, e restituisce anche le date `dates`.\n",
    "\n",
    "Cominciamo con quella che ritorna il cubo `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testata, funziona con array, Series e DataFrame\n",
    "def rolling_window(data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "                   start: int,\n",
    "                   end: int,\n",
    "                   lookback: int):\n",
    "    \"\"\"\n",
    "    Create a rolling window view of data, starting at index start, finishing\n",
    "    at index end, with loockback days of bptt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: series, dataframe or array\n",
    "        the data, containing one row for each time point and one column for each feature\n",
    "        \n",
    "    start: int\n",
    "        starting index in the data\n",
    "        \n",
    "    end: int\n",
    "        index where the whole thing ends, data[end] is **excluded**\n",
    "        \n",
    "    lookback: int\n",
    "        length of the lookback period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray\n",
    "        array of shape(n_points, lookback, n_features)\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        if data.ndim == 1:\n",
    "            my_data = data.reshape(data.shape[0], 1)\n",
    "        elif data.ndim == 2:\n",
    "            my_data = data\n",
    "    elif isinstance(data, pd.Series):\n",
    "        my_data = data.values.reshape(data.shape[0], 1)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        my_data = data.values\n",
    "    else:\n",
    "        raise TypeError(\"data should be pd.Series, pd.DataFrame or np.ndarray\")\n",
    "\n",
    "    assert lookback < my_data.shape[0]  # lookback sano\n",
    "    assert start - lookback + 1 >= 0  # lookback sano\n",
    "    \n",
    "    n_features = my_data.shape[1]\n",
    "    n_points = end - start\n",
    "    \n",
    "    X = np.zeros((n_points, lookback, n_features), dtype = my_data.dtype)\n",
    "    \n",
    "    # range strano per l'indicizzazione numpy\n",
    "    for i, t in enumerate(range(start + 1, end + 1)):\n",
    "        X[i, :, :] = my_data[t - lookback:t, :]\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# testata, funziona hehehe\n",
    "def rolling_window_xy(data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "                     target: Union[np.ndarray, pd.Series],\n",
    "                     start: int,\n",
    "                     end: int,\n",
    "                     lookback: int):\n",
    "    \"\"\"\n",
    "    Create X and y in a single shot.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = rolling_window(data, start, end, lookback)\n",
    "    \n",
    "    if isinstance(target, pd.Series):\n",
    "        y = target.values[start + 1:end + 1]\n",
    "    elif isinstance(target, np.ndarray):\n",
    "        y = target[start + 1:end + 1]\n",
    "    else:\n",
    "        raise TypeError(\"target should be a pandas Series or numpy array\")\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Divisione dataset\n",
    "\n",
    "Ora che abbiamo le due funzioni, dividiamo il dataset in training e testing, utilizzando le date di prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_1 = returns_all['min_vol'].index.get_loc(split_dates['subprime-crisis-halfway'])\n",
    "test_end_1 = returns_all['min_vol'].index.get_loc(split_dates['eu-debt-halfway'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
