{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "-VbbtgVOqGWb",
    "outputId": "d4993974-ebc5-43a4-823a-1ee3c7091221"
   },
   "outputs": [],
   "source": [
    "ENV = 'local'  # 'colab'\n",
    "if ENV == 'colab':\n",
    "    !pip install -q PyDrive imbalanced-learn ipdb hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qdJhI2pqGWf"
   },
   "source": [
    "# Rete neurale per extreme returns su 2 azioni\n",
    "\n",
    "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
    "- [x] creazione estremi al 95%\n",
    "- [ ] oversampling con due possibili strategie: replicare le istanze positive, o replicarle con aggiunta di rumore gaussiano\n",
    "- [ ] addestramento rete con hyperparameter optimization\n",
    "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
    "- [ ] confronto con i risultati del modello probabilistico\n",
    "- [ ] conclusioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yIWuJMSBqGWg",
    "outputId": "7d8c1db3-fd8b-4726-e66d-80501317e9c2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.testing as pt\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.preprocessing as skpp\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import hyperopt as hy\n",
    "from hyperopt import hp, Trials, fmin\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "import ipdb\n",
    "\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ImRJwq-Fq4aj",
    "outputId": "5cc513a6-af79-4009-982d-b31c1ae16799"
   },
   "outputs": [],
   "source": [
    "if ENV == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joUuYwuYqGWj"
   },
   "source": [
    "Un po' di dichiarazioni utili per il seguito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRPdSilJqGWk"
   },
   "outputs": [],
   "source": [
    "stock_type = ['min_vol', 'max_vol']\n",
    "return_type = ['pos', 'neg', 'abs']\n",
    "q_type = '95'\n",
    "rs = 42  # random state\n",
    "\n",
    "stock_codes = {\n",
    "    'min_vol': '9CE4C7',\n",
    "    'max_vol': 'E28F22'\n",
    "}  # già trovate in Paper-azioni.ipynb\n",
    "\n",
    "stock_colors = {\n",
    "    'min_vol': 'palegoldenrod',\n",
    "    'max_vol': 'coral',\n",
    "}\n",
    "\n",
    "# i giorni sono i primi disponibili in quel mese nei dati\n",
    "split_dates = {\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
    "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
    "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
    "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
    "    'last_train': datetime.datetime(2017, 1, 3), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIgaOP67qGWm"
   },
   "source": [
    "## 1. Importazione dei dati \n",
    "\n",
    "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "R3FzgUr5qGWn",
    "outputId": "423a6706-366c-42f6-91c9-980d77e939ad"
   },
   "outputs": [],
   "source": [
    "if ENV == 'colab':\n",
    "    data_path = '/gdrive/My Drive/OptiRisk Thesis/data'\n",
    "else:\n",
    "    data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "\n",
    "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
    "ta_dir = os.path.join(data_path, 'technical_features', 'features_all_years')\n",
    "print(ta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsmulRB3qGWp"
   },
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "aX80lWk8qGWq",
    "outputId": "39954491-1764-4c7e-ba4a-2429a4883008"
   },
   "outputs": [],
   "source": [
    "prices = pd.read_csv(prices_path)\n",
    "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
    "prices.index = prices['date']\n",
    "prices.drop(columns=['date'], inplace=True)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBiVnJIqGWs"
   },
   "source": [
    "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
    "\n",
    "I prezzi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "HyNlWyFnqGWt",
    "outputId": "21b453f9-7506-4d0f-f10e-c0066e45a787"
   },
   "outputs": [],
   "source": [
    "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
    "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
    "prices_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-TCT_OnqGWv"
   },
   "source": [
    "I volumi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "2EegGob7qGWv",
    "outputId": "5be180a6-42d2-416e-d939-1229a0c26b73"
   },
   "outputs": [],
   "source": [
    "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
    "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
    "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
    "volume_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D11Ov9SBqGWy"
   },
   "source": [
    "Ora calcoliamo i log-returns e le direzioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-hSvXqxqGW0"
   },
   "outputs": [],
   "source": [
    "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
    "directions_ts_no_nan = prices_ts_no_nan.diff(periods=1).iloc[1:, :]\n",
    "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
    "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
    "\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, directions.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3N3bQbJkr3c"
   },
   "source": [
    "Mi conviene creare una funzione che standardizzi le features, visto che poi ne avrò più di una (es: returns + volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqZ2KaSJk2Kt"
   },
   "outputs": [],
   "source": [
    "def only_train_notime(feature: pd.Series) -> pd.Series:\n",
    "    \"\"\"Just return the training part of a Series.\"\"\"\n",
    "    f = feature[np.logical_or(\n",
    "        feature.index < split_dates['subprime-crisis-halfway'],\n",
    "        np.logical_and(\n",
    "            feature.index >= split_dates['eu-debt-halfway'],\n",
    "            feature.index < split_dates['last_train']\n",
    "        )\n",
    "    )]\n",
    "\n",
    "    return f\n",
    "\n",
    "def standardize(feature: pd.Series) -> pd.Series:\n",
    "    \"\"\"Standardize a feature by computing the statistics on the training set.\"\"\"\n",
    "    # prendo solo la parte di training, perdendo ogni riferimento alla\n",
    "    # sequenza temporale\n",
    "    tmp_feature_train = only_train_notime(feature)\n",
    "\n",
    "    scaler = skpp.RobustScaler()\n",
    "    scaler.fit(tmp_feature_train.values.reshape(-1, 1))\n",
    "\n",
    "    result = pd.Series(\n",
    "        data=scaler.transform(feature.values.reshape(-1, 1)).flatten(),\n",
    "        index=feature.index\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVoCHZZurG4j"
   },
   "source": [
    "Ora creo i thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soghP5HhqGW3"
   },
   "outputs": [],
   "source": [
    "# ora creo i dati per i returns (non standardizzati), i thresholds e i volumi (standardizzati)\n",
    "lr_train_notime = dict()\n",
    "lr_test_notime = dict()\n",
    "returns_train_notime = dict()\n",
    "\n",
    "# aggiungiamo i dati in modalità taglia-e-cuci\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # training set\n",
    "    lr_current = log_returns.loc[:, s_code]\n",
    "    lr_train_notime[s_type] = only_train_notime(lr_current)\n",
    "    \n",
    "    # returns train, tutti POSITIVI\n",
    "    returns_train_notime[s_type] = {\n",
    "        'pos': lr_train_notime[s_type][lr_train_notime[s_type] > 0.0],\n",
    "        'neg': -(lr_train_notime[s_type][lr_train_notime[s_type] < 0.0]),\n",
    "        'abs': lr_train_notime[s_type].abs()\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# ora creo i threshold\n",
    "thresholds = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: returns_train_notime[s_type][ret_type].quantile(0.95)\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lr51ts-sjf6b"
   },
   "source": [
    "ed infine creo i DataFrame e gli arrays che contengono tutti gli estremi e tutti i dati.\n",
    "\n",
    "Le features che qui utilizziamo sono:\n",
    "\n",
    "- log-returns standardizzati\n",
    "- volume scambiato standardizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "FzG8axsrrSSa",
    "outputId": "4a1ed268-9de3-4334-8f15-7bd2b1f99bd2"
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
    "]\n",
    "\n",
    "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
    "\n",
    "features = dict()\n",
    "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
    "\n",
    "to_standardize = {\n",
    "    'sma5', 'sma10', 'sma15',\n",
    "    'ema5', 'ema10', 'ema15',\n",
    "    'macd',\n",
    "    'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'roc', 'atr', 'cci', 'adx',\n",
    "    }\n",
    "\n",
    "to_divide = {\n",
    "    'rsi': 100.0,\n",
    "    'aroon_down': 100.0,\n",
    "    'aroon_up': 100.0,\n",
    "    'cmo': 100.0,\n",
    "}\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    print(f\"Stock type: {s_type}\")\n",
    "    print(\"-\"*30)\n",
    "    features[s_type] = dict()\n",
    "\n",
    "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
    "        feature = pd.read_hdf(feature_path)\n",
    "\n",
    "        if feature_name in to_standardize:\n",
    "            print(f\"standardizing {feature_name}\")\n",
    "            feature_transformed = standardize(feature.loc[:, s_code])\n",
    "            features[s_type][feature_name] = feature_transformed\n",
    "        elif feature_name in to_divide.keys():\n",
    "            print(f\"dividing {feature_name}\")\n",
    "            features[s_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
    "        else:\n",
    "            raise ValueError(f\"unknown feature {feature_name}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90lESpICqGW9"
   },
   "outputs": [],
   "source": [
    "extremes_all = dict()  # keys: s_type, q_type\n",
    "data_all = dict()  # keys: s_type\n",
    "volumes = dict()  # keys: s_type\n",
    "directions_all = dict()  # keys: s_type\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # i returns\n",
    "    lr = log_returns.loc[:, s_code]\n",
    "    lr_transformed = standardize(lr)\n",
    "\n",
    "    # i volumi\n",
    "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
    "    volume_transformed = standardize(stock_volume)\n",
    "    volumes[s_type] = volume_transformed\n",
    "\n",
    "    # le features tecniche\n",
    "    all_features = [lr_transformed, volume_transformed] + \\\n",
    "              [features[s_type][name] for name in feature_names]\n",
    "\n",
    "    # tutte le features in un unico DataFrame\n",
    "    tmp_df = pd.concat(\n",
    "        all_features,\n",
    "        axis=1,\n",
    "        keys=['log_return', 'volume'] + feature_names\n",
    "    )\n",
    "\n",
    "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
    "    \n",
    "    data_all[s_type] = tmp_df\n",
    "    extremes_all[s_type] = dict()\n",
    "    \n",
    "    ext = np.logical_or(\n",
    "        lr >= thresholds[s_type]['pos'][q_type],\n",
    "        lr <= -thresholds[s_type]['neg'][q_type],\n",
    "    )\n",
    "    \n",
    "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
    "    \n",
    "    # le direzioni\n",
    "    direction = directions_ts_no_nan.loc[:, s_code] > 0.0\n",
    "    directions_all[s_type] = direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbKxpt4zqGW8"
   },
   "source": [
    "## 2. Creazione dataset train-test per TensorFlow\n",
    "\n",
    "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
    "\n",
    "- X: cubo dati\n",
    "- y: estremo si/no\n",
    "\n",
    "Per prima cosa, creo delle funzioni che mi creano i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sg7SbAHBqGXA"
   },
   "outputs": [],
   "source": [
    "# testata, funziona con array, Series e DataFrame\n",
    "def rolling_window(data: np.ndarray,\n",
    "                   start: int,\n",
    "                   end: int,\n",
    "                   lookback: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a rolling window view of data, starting at index start, finishing\n",
    "    at index end, with loockback days of bptt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: series, dataframe or array\n",
    "        the data, containing one row for each time point and one column for each feature\n",
    "        \n",
    "    start: int\n",
    "        starting index in the data\n",
    "        \n",
    "    end: int\n",
    "        index where the whole thing ends, data[end] is **excluded**\n",
    "        \n",
    "    lookback: int\n",
    "        length of the lookback period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray\n",
    "        array of shape(n_points, lookback, n_features)\n",
    "    \"\"\"\n",
    "    assert lookback < data.shape[0]  # lookback sano\n",
    "    assert start - lookback + 1 >= 0  # lookback sano\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    n_points = end - start\n",
    "    \n",
    "    X = np.zeros((n_points, lookback, n_features), dtype = data.dtype)\n",
    "    \n",
    "    # range strano per l'indicizzazione numpy\n",
    "    for i, t in enumerate(range(start + 1, end + 1)):\n",
    "        X[i, :, :] = data[t - lookback:t, :]\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# testata, funziona hehehe\n",
    "def rolling_window_xyd(data: Union[pd.Series, pd.DataFrame],\n",
    "                      target: List[pd.Series],\n",
    "                      start: int,\n",
    "                      end: int,\n",
    "                      lookback: int) -> Tuple[np.ndarray, List[np.ndarray], pd.Series]:\n",
    "    \"\"\"\n",
    "    Create X, y and dates in a single shot.\n",
    "    The returned dates are relative to the y array(s).\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        my_data = data.values.reshape(data.shape[0], 1)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        my_data = data.values\n",
    "    else:\n",
    "        raise TypeError(\"data should be a pandas Series or Dataframe\")\n",
    "\n",
    "    X = rolling_window(my_data, start, end, lookback)\n",
    "    \n",
    "    if not isinstance(target, list):\n",
    "        raise TypeError(\"target must be a list of pandas Series\")\n",
    "    if not all(isinstance(t, pd.Series) for t in target):\n",
    "        raise TypeError(\"all targets should be pandas Series\")\n",
    "    if not all(isinstance(t.index, pd.DatetimeIndex) for t in target):\n",
    "        raise TypeError(\"index of target should be a pandas DatetimeIndex\")\n",
    "        \n",
    "    y = [t.values[start + 1:end + 1] for t in target]\n",
    "    dates = pd.Series(data=target[0].index[start + 1: end + 1])\n",
    "        \n",
    "    return X, y, dates\n",
    "\n",
    "\n",
    "# TESTATO: funziona\n",
    "def create_Xyd(returns, extremes, directions, lookback) -> Tuple[\n",
    "    np.ndarray, np.ndarray, List[np.ndarray], List[np.ndarray], pd.Series, pd.Series\n",
    "]:\n",
    "    \"\"\"\n",
    "    Create the X, y and dates arrays for the ANN.\n",
    "    \"\"\"\n",
    "    test_start_1 = returns.index.get_loc(split_dates['subprime-crisis-halfway'])\n",
    "    test_end_1 = returns.index.get_loc(split_dates['eu-debt-halfway'])\n",
    "    test_start_2 = returns.index.get_loc(split_dates['last_train'])\n",
    "\n",
    "    # TRAIN\n",
    "    tmp_X_train_1, tmp_y_train_1, tmp_dates_train_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=lookback - 1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "\n",
    "    tmp_X_train_2, tmp_y_train_2, tmp_dates_train_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_end_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_2,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    assert len(tmp_y_train_1) == len(tmp_y_train_2)\n",
    "    \n",
    "    X_train = np.concatenate([tmp_X_train_1, tmp_X_train_2])\n",
    "    y_train = [np.concatenate([tmp_y_train_1[i], tmp_y_train_2[i]]) for i in range(len(tmp_y_train_1))]\n",
    "    dates_train = pd.concat([tmp_dates_train_1, tmp_dates_train_2], axis=0, ignore_index=True).values\n",
    "    assert X_train.shape[0] == dates_train.shape[0]\n",
    "    assert all(y_train[i].shape[0] == X_train.shape[0] for i in range(len(y_train)))\n",
    "\n",
    "    # TEST\n",
    "    tmp_X_test_1, tmp_y_test_1, tmp_dates_test_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_start_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_end_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    tmp_X_test_2, tmp_y_test_2, tmp_dates_test_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_start_2,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=returns.shape[0] - 1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "  \n",
    "\n",
    "    X_test = np.concatenate([tmp_X_test_1, tmp_X_test_2])\n",
    "    y_test = [np.concatenate([tmp_y_test_1[i], tmp_y_test_2[i]]) for i in range(len(tmp_y_test_1))]\n",
    "    dates_test = pd.concat([tmp_dates_test_1, tmp_dates_test_2], axis=0, ignore_index=True).values\n",
    "    assert X_test.shape[0] == dates_test.shape[0]\n",
    "    assert all(y_test[i].shape[0] == X_test.shape[0] for i in range(len(y_test)))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
    "\n",
    "\n",
    "def split_stratified(X: np.ndarray,\n",
    "                     y: List[np.ndarray],\n",
    "                     dates: np.ndarray,\n",
    "                     test_size=0.2,\n",
    "                     random_state=rs,\n",
    "                     verbose=False):\n",
    "    \"\"\"\n",
    "    Split a dataset in a stratified fashion on the target variable y[0].\n",
    "    \"\"\"\n",
    "    assert X.ndim == 3\n",
    "    # divido in train-validation, lo faccio prendendo gli indici dagli estremi y/n con un\n",
    "    # ShuffleSplit che divide a caso\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[2]\n",
    "    \n",
    "    XX = np.zeros(n_samples, dtype=np.int8)\n",
    "    \n",
    "    if verbose:\n",
    "        for i in range(len(y)):\n",
    "            vals, counts = np.unique(y[i], return_counts=True)\n",
    "            for v, c in zip(vals, counts):\n",
    "                print(f\"y[{i}] has {c} elements of class {v}\")\n",
    "    \n",
    "    train_index, test_index = next(splitter.split(XX, y[0]))\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_validation = X[test_index]\n",
    "    \n",
    "    y_train = [yy[train_index] for yy in y]\n",
    "    y_validation = [yy[test_index] for yy in y]\n",
    "    \n",
    "    dates_train = dates[train_index]\n",
    "    dates_validation = dates[test_index]\n",
    "\n",
    "    return X_train, X_validation, y_train, y_validation, dates_train, dates_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UVy3gkvqGXG"
   },
   "source": [
    "## 3. Addestramento rete per l'azione con minima volatilità\n",
    "\n",
    "Cominciamo con l'azione meno volatile. Gli step sono:\n",
    "\n",
    "- [x] dividere in train-test set, aggiungere il validation\n",
    "- [x] oversampling sul train set (**non** sul validation)\n",
    "- [ ] creazione rete\n",
    "- [ ] addestramento sul training set\n",
    "- [ ] valutazione performance in base al threshold del softmax:\n",
    "    - [ ] ROC, KSS, Utilità sul training set + Precision, Recall\n",
    "    - [ ] KSS, Utilità sul test set + Precision, Recall\n",
    "\n",
    "Cominciamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxKDWjIAqGXG"
   },
   "outputs": [],
   "source": [
    "s_type = 'min_vol'\n",
    "\n",
    "if data_all[s_type].ndim == 1:\n",
    "    n_features = 1\n",
    "else:\n",
    "    n_features = data_all[s_type].shape[1]\n",
    "\n",
    "lookback = 30\n",
    "\n",
    "# creo i primi (test va già bene)\n",
    "X_trv, X_test, y_trv, y_test, dates_trv, dates_test = create_Xyd(\n",
    "    data_all[s_type].astype(np.float32),\n",
    "    extremes_all[s_type]['95'].astype(np.float32),\n",
    "    directions_all[s_type].astype(np.float32),\n",
    "    lookback=lookback\n",
    ")\n",
    "\n",
    "# divido in train-validation\n",
    "X_train, X_validation, y_train, y_validation, dates_train, dates_validation = split_stratified(\n",
    "    X_trv,\n",
    "    y_trv,\n",
    "    dates_trv,\n",
    "    test_size=0.2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIxvkZvCqGXI"
   },
   "outputs": [],
   "source": [
    "def oversample_1(X: np.ndarray, y: np.ndarray, random_state=42, dt=np.float32):\n",
    "    \"\"\"Oversample a dataset on the 1 class.\"\"\"\n",
    "    assert X.dtype == y.dtype == dt\n",
    "    assert X.ndim == 3\n",
    "    assert y.ndim == 1\n",
    "    \n",
    "    # oversample\n",
    "    ro = RandomOverSampler(random_state=random_state)\n",
    "    nx = X.shape[0]\n",
    "    indexes = np.arange(nx).reshape(nx, 1)\n",
    "    \n",
    "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y)\n",
    "    \n",
    "    X_resampled = X[indexes_resampled[:, 0]]\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def oversample_mtl(X: np.ndarray, y: List[np.ndarray], random_state=rs, dt=np.float32):\n",
    "    \"\"\"Oversample a dataset on the 1 class.\"\"\"\n",
    "    assert X.dtype == dt\n",
    "    assert X.ndim == 3\n",
    "    assert isinstance(y, list) and y[0].ndim == 1 and all(yy.dtype == dt for yy in y)\n",
    "    \n",
    "    # oversample\n",
    "    ro = RandomOverSampler(random_state=random_state)\n",
    "    nx = X.shape[0]\n",
    "    indexes = np.arange(nx).reshape(nx, 1)\n",
    "    \n",
    "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y[0])\n",
    "    ir = indexes_resampled.flatten()\n",
    "    \n",
    "    X_resampled = X[ir]\n",
    "    y_resampled = [yy[ir] for yy in y]\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF5oQ7CyqGXK"
   },
   "outputs": [],
   "source": [
    "X_train_bal, y_train_bal = oversample_mtl(X_train, y_train)  # bal = balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okVkBuNFqGXN"
   },
   "source": [
    "### 3.1 Creazione del modello\n",
    "\n",
    "Ora che abbiamo i dati, dobbiamo creare il modello. Riutilizziamo una funzione che avevo già scritto due mesi fa :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7DM3GQrqGXO"
   },
   "outputs": [],
   "source": [
    "def create_model_mtl(\n",
    "    space: Dict[str, Any],\n",
    "    bptt: int,\n",
    "    n_features: int) -> keras.models.Model:\n",
    "    \"\"\"Create a model using the parameters in the search space.\"\"\"\n",
    "    l = space['layers']\n",
    "\n",
    "    input_dropout = float(l['input_dropout'])\n",
    "    assert input_dropout >= 0.0 and input_dropout <= 1.0\n",
    "\n",
    "    n_layers = int(l['num_layers']['how_many'])\n",
    "    assert n_layers <= 2 and n_layers > 0\n",
    "\n",
    "    n_cells_1 = int(l['num_layers']['n_cells_1'])\n",
    "    assert n_cells_1 >= 1\n",
    "\n",
    "    # creo il modello\n",
    "    model_input = keras.Input(shape=(bptt, n_features), name='model_input')\n",
    "\n",
    "    if n_layers == 1:\n",
    "        if input_dropout > 0.0:\n",
    "            x = keras.layers.LSTM(n_cells_1, dropout=input_dropout)(model_input)\n",
    "        else:\n",
    "            x = keras.layers.LSTM(n_cells_1)(model_input)\n",
    "    elif n_layers == 2:\n",
    "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
    "        x = keras.layers.LSTM(n_cells_1, return_sequences=True)(model_input)\n",
    "        x = keras.layers.LSTM(n_cells_2)(x)\n",
    "    elif n_layers == 3:\n",
    "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
    "        n_cells_3 = int(l['num_layers']['n_cells_3'])\n",
    "        x = keras.layers.LSTM(n_cells_1, return_sequences=True)(model_input)\n",
    "        x = keras.layers.LSTM(n_cells_2, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(n_cells_3)(x)\n",
    "\n",
    "    output_is_extreme = keras.layers.Dense(\n",
    "        2, activation='softmax', name='extreme')(x)\n",
    "    output_is_up_down = keras.layers.Dense(\n",
    "        2, activation='softmax', name='up_down')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=model_input,\n",
    "        outputs=[output_is_extreme, output_is_up_down],\n",
    "        name='MTL_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyxjqnBnqGXT"
   },
   "outputs": [],
   "source": [
    "optimizer_space = hp.choice('opt_name', [\n",
    "    {\n",
    "        'name': 'adam',\n",
    "        'lr': 1e-4, #hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
    "    },\n",
    "    #    {\n",
    "    #        'name': 'adadelta',\n",
    "    #    },\n",
    "])\n",
    "\n",
    "layer_space = {\n",
    "    'num_layers': hp.choice('number_of_layers', [\n",
    "        {\n",
    "            'how_many': 1,\n",
    "            'n_cells_1': 1, #hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "            'n_cells_2': 0,\n",
    "        },\n",
    "#        {\n",
    "#            'how_many': 2,\n",
    "#            'n_cells_1': hp.quniform('number_of_cells_1', low=10, high=100, q=2),\n",
    "#            'n_cells_2': hp.quniform('number_of_cells_2', low=10, high=50, q=2),\n",
    "#        }\n",
    "    ]),\n",
    "}\n",
    "\n",
    "early_stop_space = {\n",
    "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
    "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
    "}\n",
    "\n",
    "opt_space = {\n",
    "    'optimizer': optimizer_space,\n",
    "    'layers': layer_space,\n",
    "    'bptt': 30, #hp.quniform('bptt_len', low=10, high=150, q=1),\n",
    "    #'early_stop': early_stop_space,\n",
    "    #'use_class_weight': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0Gd1dXGiZV8"
   },
   "source": [
    "### 3.2 Ottimizzazione degli iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "vwdJJ5BnqGXX",
    "outputId": "58aa585c-bc91-43fa-b825-9075558c2f5e"
   },
   "outputs": [],
   "source": [
    "model_input = keras.Input(shape=(lookback, n_features), name='model_input')\n",
    "x = keras.layers.LSTM(n_features)(model_input)\n",
    "\n",
    "output_is_extreme = keras.layers.Dense(\n",
    "        2, activation='softmax', name='extreme')(x)\n",
    "output_is_up_down = keras.layers.Dense(\n",
    "    2, activation='softmax', name='up_down')(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=model_input,\n",
    "    outputs=[output_is_extreme, output_is_up_down],\n",
    "    name='MTL_model')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=5e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjL0Q5tfrbjN"
   },
   "outputs": [],
   "source": [
    "y_train_bal_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_train_bal]\n",
    "y_validation_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IpYAG05UTWDx",
    "outputId": "4ac19649-9d71-42c9-d5be-2ffd9252eb5d"
   },
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "batch_size = 1645\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-2,\n",
    "    patience=30,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train_bal.astype(np.float32),\n",
    "    y=y_train_bal_cat,\n",
    "    batch_size=batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=(X_validation, y_validation_cat),\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5gX8eBRZeU4"
   },
   "source": [
    "Ora vedo come variare il threshold per ottenere le curve ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5nXyquVVsRW"
   },
   "outputs": [],
   "source": [
    "def loss_function(theta, recall, fpr):\n",
    "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
    "    assert theta >= 0.0 and theta <= 1.0\n",
    "    \n",
    "    return theta * (1 - recall) + (1 - theta) * fpr\n",
    "\n",
    "\n",
    "def utility_function(theta, loss):\n",
    "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
    "    return min(theta, 1 - theta) - loss\n",
    "\n",
    "\n",
    "def to_binary(prob: np.ndarray, thresh: float):\n",
    "    assert thresh <= 1.0 and thresh >= 0.0\n",
    "    \n",
    "    return (prob >= thresh).astype(np.int8)\n",
    "\n",
    "\n",
    "def recall_fpr_kss_precision(y_true, y_pred):\n",
    "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
    "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "    tn = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    fp = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        y_pred\n",
    "    ))\n",
    "    fn = np.sum(np.logical_and(\n",
    "        y_true,\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    \n",
    "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
    "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    kss = recall - fpr\n",
    "    \n",
    "    return recall, fpr, kss, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0ylM6MWZjHg"
   },
   "outputs": [],
   "source": [
    "w_t = np.arange(0, 1, 1e-3)\n",
    "theta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBtZN0LrZ4_R"
   },
   "outputs": [],
   "source": [
    "def optimize_wt(w, theta, probabilities, y_true, verbose=False):\n",
    "    \"\"\"Get the best threshold for the class 1 probability.\"\"\"\n",
    "    recalls = np.zeros((w.shape[0], ), dtype=np.float64)\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    precisions = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    for i, thresh in enumerate(w):\n",
    "        if i % 200 == 0 and verbose:\n",
    "            print(f\"iteration {i} / {len(w_t)}\")\n",
    "\n",
    "        y_pred = to_binary(probabilities, thresh).astype(np.int8)\n",
    "        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
    "        loss = loss_function(theta, recall, fpr)\n",
    "        utility = utility_function(theta, loss)\n",
    "\n",
    "        recalls[i] = recall\n",
    "        precisions[i] = precision\n",
    "        ksss[i] = kss\n",
    "        fprs[i] = fpr\n",
    "        losses[i] = loss\n",
    "        utilities[i] = utility\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished!\")\n",
    "\n",
    "    return recalls, fprs, ksss, precisions, losses, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_i6fKRXteWS0",
    "outputId": "9d073a6f-427e-47b6-bdb1-0dae3b4809d2"
   },
   "outputs": [],
   "source": [
    "probabilities_extreme_train = model.predict(X_train, batch_size=batch_size)[0][:, 1]\n",
    "probabilities_up_down_train = model.predict(X_train, batch_size=batch_size)[1][:, 1]\n",
    "\n",
    "probabilities_extreme_validation = model.predict(X_validation, batch_size=batch_size)[0][:, 1]\n",
    "probabilities_up_down_validation = model.predict(X_validation, batch_size=batch_size)[1][:, 1]\n",
    "\n",
    "# per le ROC sugli estremi\n",
    "recalls_train, fprs_train, ksss_train, precisions_train, losses_train, utilities_train = \\\n",
    "optimize_wt(w_t, theta, probabilities_extreme_train, y_train[0].astype(np.int8))\n",
    "\n",
    "recalls_validation, fprs_validation, ksss_validation, \\\n",
    "precisions_validation, losses_validation, utilities_validation = \\\n",
    "optimize_wt(w_t, theta, probabilities_extreme_validation, y_validation[0].astype(np.int8))\n",
    "\n",
    "# per le ROC sul su-giù\n",
    "recalls_train_ud, fprs_train_ud, ksss_train_ud, precisions_train_ud, losses_train_ud, utilities_train_ud = \\\n",
    "optimize_wt(w_t, theta, probabilities_up_down_train, y_train[1].astype(np.int8))\n",
    "\n",
    "recalls_validation_ud, fprs_validation_ud, ksss_validation_ud, \\\n",
    "precisions_validation_ud, losses_validation_ud, utilities_validation_ud = \\\n",
    "optimize_wt(w_t, theta, probabilities_up_down_validation, y_validation[1].astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "TcQpqDonZ9BB",
    "outputId": "d7324311-3b9b-4993-a1c1-2312876a1c7a"
   },
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "fig.suptitle(f\"{s_type} stock\", fontsize=16)\n",
    "\n",
    "# primo plot: EXTREMES\n",
    "# train set\n",
    "i_sorted = np.argsort(fprs_train)\n",
    "\n",
    "x_extreme = fprs_train[i_sorted]\n",
    "y_extreme = recalls_train[i_sorted]\n",
    "ax[0].plot(\n",
    "    x_extreme,\n",
    "    y_extreme,\n",
    "    color='navy',\n",
    "    label='train'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_train)\n",
    "best_x = fprs_train[i_sweet]\n",
    "best_y = recalls_train[i_sweet]\n",
    "\n",
    "ax[0].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='navy',\n",
    "    label='train - best'\n",
    ")\n",
    "\n",
    "# validation set\n",
    "i_sorted = np.argsort(fprs_validation)\n",
    "\n",
    "x = fprs_validation[i_sorted]\n",
    "y = recalls_validation[i_sorted]\n",
    "ax[0].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='forestgreen',\n",
    "    label='validation'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_validation)\n",
    "best_x = fprs_validation[i_sweet]\n",
    "best_y = recalls_validation[i_sweet]\n",
    "\n",
    "ax[0].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='forestgreen',\n",
    "    label='validation - best'\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Extreme prediction\")\n",
    "\n",
    "# ------------------------------------------------------- #\n",
    "# secondo plot: UP-DOWN\n",
    "# train set\n",
    "i_sorted = np.argsort(fprs_train_ud)\n",
    "\n",
    "x = fprs_train_ud[i_sorted]\n",
    "y = recalls_train_ud[i_sorted]\n",
    "ax[1].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='navy',\n",
    "    label='train'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_train)\n",
    "best_x = fprs_train_ud[i_sweet]\n",
    "best_y = recalls_train_ud[i_sweet]\n",
    "\n",
    "ax[1].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='navy',\n",
    "    label='train - best'\n",
    ")\n",
    "\n",
    "# validation set\n",
    "i_sorted = np.argsort(fprs_validation_ud)\n",
    "\n",
    "x = fprs_validation_ud[i_sorted]\n",
    "y = recalls_validation_ud[i_sorted]\n",
    "ax[1].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='forestgreen',\n",
    "    label='validation'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_validation)\n",
    "best_x = fprs_validation_ud[i_sweet]\n",
    "best_y = recalls_validation_ud[i_sweet]\n",
    "\n",
    "ax[1].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='forestgreen',\n",
    "    label='validation - best'\n",
    ")\n",
    "\n",
    "ax[1].set_title(\"Up-Down prediction\")\n",
    "\n",
    "# la linea del random classifier\n",
    "for a in ax:\n",
    "    a.plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "    a.legend(loc='lower right', fontsize=14)\n",
    "    a.set_xlim([0, 1.1])\n",
    "    a.set_ylim([0, 1.1])\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ANN_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
