{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "-VbbtgVOqGWb",
    "outputId": "d4993974-ebc5-43a4-823a-1ee3c7091221"
   },
   "outputs": [],
   "source": [
    "ENV = 'colab'  # 'colab'\n",
    "if ENV == 'colab':\n",
    "    !pip install -q PyDrive imbalanced-learn ipdb hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qdJhI2pqGWf"
   },
   "source": [
    "# Rete neurale per extreme returns su 2 azioni\n",
    "\n",
    "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
    "- [x] creazione estremi al 95%\n",
    "- [ ] oversampling con due possibili strategie: replicare le istanze positive, o replicarle con aggiunta di rumore gaussiano\n",
    "- [ ] addestramento rete con hyperparameter optimization\n",
    "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
    "- [ ] confronto con i risultati del modello probabilistico\n",
    "- [ ] conclusioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yIWuJMSBqGWg",
    "outputId": "7d8c1db3-fd8b-4726-e66d-80501317e9c2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Dict, List, Union\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.testing as pt\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.preprocessing as skpp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import hyperopt as hy\n",
    "from hyperopt import hp, Trials, fmin\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "import ipdb\n",
    "\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ImRJwq-Fq4aj",
    "outputId": "5cc513a6-af79-4009-982d-b31c1ae16799"
   },
   "outputs": [],
   "source": [
    "if ENV == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joUuYwuYqGWj"
   },
   "source": [
    "Un po' di dichiarazioni utili per il seguito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRPdSilJqGWk"
   },
   "outputs": [],
   "source": [
    "stock_type = ['min_vol', 'max_vol']\n",
    "return_type = ['pos', 'neg', 'abs']\n",
    "q_type = '95'\n",
    "\n",
    "stock_codes = {\n",
    "    'min_vol': '9CE4C7',\n",
    "    'max_vol': 'E28F22'\n",
    "}  # già trovate in Paper-azioni.ipynb\n",
    "\n",
    "stock_colors = {\n",
    "    'min_vol': 'palegoldenrod',\n",
    "    'max_vol': 'coral',\n",
    "}\n",
    "\n",
    "# i giorni sono i primi disponibili in quel mese nei dati\n",
    "split_dates = {\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
    "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
    "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
    "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
    "    'last_train': datetime.datetime(2017, 1, 3), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIgaOP67qGWm"
   },
   "source": [
    "## 1. Importazione dei dati \n",
    "\n",
    "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "R3FzgUr5qGWn",
    "outputId": "423a6706-366c-42f6-91c9-980d77e939ad"
   },
   "outputs": [],
   "source": [
    "if ENV == 'colab':\n",
    "    data_path = '/gdrive/My Drive/OptiRisk Thesis/data'\n",
    "else:\n",
    "    data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "\n",
    "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
    "ta_dir = os.path.join(data_path, 'technical_features', 'features_all_years')\n",
    "print(ta_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsmulRB3qGWp"
   },
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "aX80lWk8qGWq",
    "outputId": "39954491-1764-4c7e-ba4a-2429a4883008"
   },
   "outputs": [],
   "source": [
    "prices = pd.read_csv(prices_path)\n",
    "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
    "prices.index = prices['date']\n",
    "prices.drop(columns=['date'], inplace=True)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBiVnJIqGWs"
   },
   "source": [
    "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
    "\n",
    "I prezzi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "HyNlWyFnqGWt",
    "outputId": "21b453f9-7506-4d0f-f10e-c0066e45a787"
   },
   "outputs": [],
   "source": [
    "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
    "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
    "prices_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-TCT_OnqGWv"
   },
   "source": [
    "I volumi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "2EegGob7qGWv",
    "outputId": "5be180a6-42d2-416e-d939-1229a0c26b73"
   },
   "outputs": [],
   "source": [
    "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
    "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
    "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
    "volume_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D11Ov9SBqGWy"
   },
   "source": [
    "Ora calcoliamo i log-returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-hSvXqxqGW0"
   },
   "outputs": [],
   "source": [
    "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
    "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
    "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
    "\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3N3bQbJkr3c"
   },
   "source": [
    "Mi conviene creare una funzione che standardizzi le features, visto che poi ne avrò più di una (es: returns + volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqZ2KaSJk2Kt"
   },
   "outputs": [],
   "source": [
    "def only_train_notime(feature: pd.Series):\n",
    "    \"\"\"Just return the training part of a Series.\"\"\"\n",
    "    f = feature[np.logical_or(\n",
    "        feature.index < split_dates['subprime-crisis-halfway'],\n",
    "        np.logical_and(\n",
    "            feature.index >= split_dates['eu-debt-halfway'],\n",
    "            feature.index < split_dates['last_train']\n",
    "        )\n",
    "    )]\n",
    "\n",
    "    return f\n",
    "\n",
    "def standardize(feature: pd.Series):\n",
    "    \"\"\"Standardize a feature by computing the statistics on the training set.\"\"\"\n",
    "    # prendo solo la parte di training, perdendo ogni riferimento alla\n",
    "    # sequenza temporale\n",
    "    tmp_feature_train = only_train_notime(feature)\n",
    "\n",
    "    scaler = skpp.RobustScaler()\n",
    "    scaler.fit(tmp_feature_train.values.reshape(-1, 1))\n",
    "\n",
    "    result = pd.Series(\n",
    "        data=scaler.transform(feature.values.reshape(-1, 1)).flatten(),\n",
    "        index=feature.index\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVoCHZZurG4j"
   },
   "source": [
    "Ora creo i thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soghP5HhqGW3"
   },
   "outputs": [],
   "source": [
    "# ora creo i dati per i returns (non standardizzati), i thresholds e i volumi (standardizzati)\n",
    "lr_train_notime = dict()\n",
    "lr_test_notime = dict()\n",
    "returns_train_notime = dict()\n",
    "\n",
    "# aggiungiamo i dati in modalità taglia-e-cuci\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # training set\n",
    "    lr_current = log_returns.loc[:, s_code]\n",
    "    lr_train_notime[s_type] = only_train_notime(lr_current)\n",
    "    \n",
    "    # returns train, tutti POSITIVI\n",
    "    returns_train_notime[s_type] = {\n",
    "        'pos': lr_train_notime[s_type][lr_train_notime[s_type] > 0.0],\n",
    "        'neg': -(lr_train_notime[s_type][lr_train_notime[s_type] < 0.0]),\n",
    "        'abs': lr_train_notime[s_type].abs()\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# ora creo i threshold\n",
    "thresholds = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: returns_train_notime[s_type][ret_type].quantile(0.95)\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lr51ts-sjf6b"
   },
   "source": [
    "ed infine creo i DataFrame e gli arrays che contengono tutti gli estremi e tutti i dati.\n",
    "\n",
    "Le features che qui utilizziamo sono:\n",
    "\n",
    "- log-returns standardizzati\n",
    "- volume scambiato standardizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "FzG8axsrrSSa",
    "outputId": "4a1ed268-9de3-4334-8f15-7bd2b1f99bd2"
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
    "]\n",
    "\n",
    "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
    "\n",
    "features = dict()\n",
    "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
    "\n",
    "to_standardize = {\n",
    "    'sma5', 'sma10', 'sma15',\n",
    "    'ema5', 'ema10', 'ema15',\n",
    "    'macd',\n",
    "    'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'roc', 'atr', 'cci', 'adx',\n",
    "    }\n",
    "\n",
    "to_divide = {\n",
    "    'rsi': 100.0,\n",
    "    'aroon_down': 100.0,\n",
    "    'aroon_up': 100.0,\n",
    "    'cmo': 100.0,\n",
    "}\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    print(f\"Stock type: {s_type}\")\n",
    "    print(\"-\"*30)\n",
    "    features[s_type] = dict()\n",
    "\n",
    "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
    "        feature = pd.read_hdf(feature_path)\n",
    "\n",
    "        if feature_name in to_standardize:\n",
    "            print(f\"standardizing {feature_name}\")\n",
    "            feature_transformed = standardize(feature.loc[:, s_code])\n",
    "            features[s_type][feature_name] = feature_transformed\n",
    "        elif feature_name in to_divide.keys():\n",
    "            print(f\"dividing {feature_name}\")\n",
    "            features[s_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
    "        else:\n",
    "            raise ValueError(f\"unknown feature {feature_name}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90lESpICqGW9"
   },
   "outputs": [],
   "source": [
    "extremes_all = dict()  # keys: s_type, q_type\n",
    "data_all = dict()  # keys: s_type\n",
    "volumes = dict()  # keys: s_type\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # i returns\n",
    "    lr = log_returns.loc[:, s_code]\n",
    "    lr_transformed = standardize(lr)\n",
    "\n",
    "    # i volumi\n",
    "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
    "    volume_transformed = standardize(stock_volume)\n",
    "    volumes[s_type] = volume_transformed\n",
    "\n",
    "    # le features tecniche\n",
    "    all_features = [lr_transformed, volume_transformed] + \\\n",
    "              [features[s_type][name] for name in feature_names]\n",
    "\n",
    "    # qui puoi aggiungere altre features\n",
    "    tmp_df = pd.concat(\n",
    "        all_features,\n",
    "        axis=1,\n",
    "        keys=['log_returns', 'volume'] + feature_names\n",
    "    )\n",
    "\n",
    "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
    "    \n",
    "    data_all[s_type] = tmp_df\n",
    "    extremes_all[s_type] = dict()\n",
    "    \n",
    "    ext = np.logical_or(\n",
    "        lr >= thresholds[s_type]['pos'][q_type],\n",
    "        lr <= -thresholds[s_type]['neg'][q_type],\n",
    "    )\n",
    "    \n",
    "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbKxpt4zqGW8"
   },
   "source": [
    "## 2. Creazione dataset train-test per TensorFlow\n",
    "\n",
    "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
    "\n",
    "- X: cubo dati\n",
    "- y: estremo si/no\n",
    "\n",
    "Per prima cosa, creo delle funzioni che mi creano i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sg7SbAHBqGXA"
   },
   "outputs": [],
   "source": [
    "# testata, funziona con array, Series e DataFrame\n",
    "def rolling_window(data: np.ndarray,\n",
    "                   start: int,\n",
    "                   end: int,\n",
    "                   lookback: int):\n",
    "    \"\"\"\n",
    "    Create a rolling window view of data, starting at index start, finishing\n",
    "    at index end, with loockback days of bptt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: series, dataframe or array\n",
    "        the data, containing one row for each time point and one column for each feature\n",
    "        \n",
    "    start: int\n",
    "        starting index in the data\n",
    "        \n",
    "    end: int\n",
    "        index where the whole thing ends, data[end] is **excluded**\n",
    "        \n",
    "    lookback: int\n",
    "        length of the lookback period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray\n",
    "        array of shape(n_points, lookback, n_features)\n",
    "    \"\"\"\n",
    "    assert lookback < data.shape[0]  # lookback sano\n",
    "    assert start - lookback + 1 >= 0  # lookback sano\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    n_points = end - start\n",
    "    \n",
    "    X = np.zeros((n_points, lookback, n_features), dtype = data.dtype)\n",
    "    \n",
    "    # range strano per l'indicizzazione numpy\n",
    "    for i, t in enumerate(range(start + 1, end + 1)):\n",
    "        X[i, :, :] = data[t - lookback:t, :]\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# testata, funziona hehehe\n",
    "def rolling_window_xyd(data: Union[pd.Series, pd.DataFrame],\n",
    "                      target: pd.Series,\n",
    "                      start: int,\n",
    "                      end: int,\n",
    "                      lookback: int):\n",
    "    \"\"\"\n",
    "    Create X, y and dates in a single shot.\n",
    "    The returned dates are relative to the y array.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        my_data = data.values.reshape(data.shape[0], 1)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        my_data = data.values\n",
    "    else:\n",
    "        raise TypeError(\"data should be a pandas Series or Dataframe\")\n",
    "\n",
    "    X = rolling_window(my_data, start, end, lookback)\n",
    "    \n",
    "    if not isinstance(target, pd.Series):\n",
    "        raise TypeError(\"target should be a pandas Series\")\n",
    "    if not isinstance(target.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"index of target should be a pandas DatetimeIndex\")\n",
    "    \n",
    "    y = target.values[start + 1:end + 1]\n",
    "    dates = pd.Series(data=target.index[start + 1: end + 1])\n",
    "        \n",
    "    return X, y, dates\n",
    "\n",
    "\n",
    "# TESTATO: funziona\n",
    "def create_Xyd(returns, extremes, lookback):\n",
    "    \"\"\"\n",
    "    Create the X, y and dates arrays for the ANN.\n",
    "    \"\"\"\n",
    "    test_start_1 = returns.index.get_loc(split_dates['subprime-crisis-halfway'])\n",
    "    test_end_1 = returns.index.get_loc(split_dates['eu-debt-halfway'])\n",
    "    test_start_2 = returns.index.get_loc(split_dates['last_train'])\n",
    "\n",
    "    # TRAIN\n",
    "    tmp_X_train_1, tmp_y_train_1, tmp_dates_train_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        extremes,\n",
    "        start=lookback - 1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "\n",
    "    tmp_X_train_2, tmp_y_train_2, tmp_dates_train_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        extremes,\n",
    "        start=test_end_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_2,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    X_train = np.concatenate([tmp_X_train_1, tmp_X_train_2])\n",
    "    y_train = np.concatenate([tmp_y_train_1, tmp_y_train_2])\n",
    "    dates_train = pd.concat([tmp_dates_train_1, tmp_dates_train_2], axis=0, ignore_index=True).values\n",
    "    assert X_train.shape[0] == y_train.shape[0] == dates_train.shape[0]\n",
    "\n",
    "    # TEST\n",
    "    tmp_X_test_1, tmp_y_test_1, tmp_dates_test_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        extremes,\n",
    "        start=test_start_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_end_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    tmp_X_test_2, tmp_y_test_2, tmp_dates_test_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        extremes,\n",
    "        start=test_start_2,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=returns.shape[0] - 1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "  \n",
    "\n",
    "    X_test = np.concatenate([tmp_X_test_1, tmp_X_test_2])\n",
    "    y_test = np.concatenate([tmp_y_test_1, tmp_y_test_2])\n",
    "    dates_test = pd.concat([tmp_dates_test_1, tmp_dates_test_2], axis=0, ignore_index=True).values\n",
    "    assert X_test.shape[0] == y_test.shape[0] == dates_test.shape[0]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, dates_train, dates_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UVy3gkvqGXG"
   },
   "source": [
    "## 3. Addestramento rete per l'azione con minima volatilità\n",
    "\n",
    "Cominciamo con l'azione meno volatile. Gli step sono:\n",
    "\n",
    "- [x] dividere in train-test set, aggiungere il validation\n",
    "- [x] oversampling sul train set (**non** sul validation)\n",
    "- [ ] creazione rete\n",
    "- [ ] addestramento sul training set\n",
    "- [ ] valutazione performance in base al threshold del softmax:\n",
    "    - [ ] ROC, KSS, Utilità sul training set + Precision, Recall\n",
    "    - [ ] KSS, Utilità sul test set + Precision, Recall\n",
    "\n",
    "Cominciamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxKDWjIAqGXG"
   },
   "outputs": [],
   "source": [
    "s_type = 'min_vol'\n",
    "\n",
    "if data_all[s_type].ndim == 1:\n",
    "    n_features = 1\n",
    "else:\n",
    "    n_features = data_all[s_type].shape[1]\n",
    "\n",
    "lookback = 30\n",
    "\n",
    "# creo i primi (test va già bene)\n",
    "X_trv, X_test, y_trv, y_test, dates_trv, dates_test = create_Xyd(\n",
    "    data_all[s_type].astype(np.float32),\n",
    "    extremes_all[s_type]['95'].astype(np.float32),\n",
    "    lookback=lookback\n",
    ")\n",
    "\n",
    "# divido in train-validation\n",
    "X_train, X_validation, y_train, y_validation, dates_train, dates_validation = train_test_split(\n",
    "    X_trv,\n",
    "    y_trv,\n",
    "    dates_trv,\n",
    "    test_size=0.2,\n",
    "    stratify=y_trv,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIxvkZvCqGXI"
   },
   "outputs": [],
   "source": [
    "def oversample_1(X: np.ndarray, y: np.ndarray, random_state=42, dt=np.float32):\n",
    "    \"\"\"Oversample a dataset on the 1 class.\"\"\"\n",
    "    assert X.dtype == y.dtype == dt\n",
    "    assert X.ndim == 3\n",
    "    assert y.ndim == 1\n",
    "    \n",
    "    # oversample\n",
    "    ro = RandomOverSampler(random_state=random_state)\n",
    "    nx = X.shape[0]\n",
    "    indexes = np.arange(nx).reshape(nx, 1)\n",
    "    \n",
    "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y)\n",
    "    \n",
    "    X_resampled = X[indexes_resampled[:, 0]]\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF5oQ7CyqGXK"
   },
   "outputs": [],
   "source": [
    "X_train_bal, y_train_bal = oversample_1(X_train, y_train)  # bal = balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okVkBuNFqGXN"
   },
   "source": [
    "### 3.1 Creazione del modello\n",
    "\n",
    "Ora che abbiamo i dati, dobbiamo creare il modello. Riutilizziamo una funzione che avevo già scritto due mesi fa :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7DM3GQrqGXO"
   },
   "outputs": [],
   "source": [
    "# TODO: sistemare qui\n",
    "def create_model(lstm_layers: List[Dict[str, Any]],\n",
    "                 fc_layers: List[Dict[str, Any]],\n",
    "                 lookback: int,\n",
    "                 n_features: int) -> keras.models.Model:\n",
    "    \"\"\"Create a model using the parameters in the search space.\"\"\"\n",
    "    n_lstm_layers = len(lstm_layers)\n",
    "    n_fc_layers = len(fc_layers)\n",
    "\n",
    "    \n",
    "    \n",
    "    # creo il modello\n",
    "    model_input = keras.Input(shape=(lookback, n_features), name='model_input')\n",
    "\n",
    "    if n_layers == 1:\n",
    "        x = keras.layers.LSTM(n_cells_1)(model_input)\n",
    "    elif n_layers == 2:\n",
    "        n_cells_2 = int(layers['n_cells_2'])\n",
    "        x = keras.layers.LSTM(\n",
    "            n_cells_1,\n",
    "            return_sequences=True\n",
    "        )(model_input)\n",
    "        x = keras.layers.LSTM(n_cells_2)(x)\n",
    "    elif n_layers == 3:\n",
    "        n_cells_2 = int(layers['n_cells_2'])\n",
    "        n_cells_3 = int(layers['n_cells_3'])\n",
    "        x = keras.layers.LSTM(\n",
    "            n_cells_1,\n",
    "            return_sequences=True\n",
    "        )(model_input)\n",
    "        x = keras.layers.LSTM(\n",
    "            n_cells_2, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(n_cells_3)(x)\n",
    "\n",
    "    output_is_extreme = keras.layers.Dense(\n",
    "        2, activation='softmax', name='is_extreme')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=model_input,\n",
    "        outputs=output_is_extreme,\n",
    "        name='MTL_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyxjqnBnqGXT"
   },
   "outputs": [],
   "source": [
    "optimizer_space = hp.choice('opt_name', [\n",
    "    {\n",
    "        'name': 'adam',\n",
    "        'lr': 1e-4, #hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
    "    },\n",
    "    #    {\n",
    "    #        'name': 'adadelta',\n",
    "    #    },\n",
    "])\n",
    "\n",
    "layer_space = {\n",
    "    'num_layers': hp.choice('number_of_layers', [\n",
    "        {\n",
    "            'how_many': 1,\n",
    "            'n_cells_1': 1, #hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "            'n_cells_2': 0,\n",
    "        },\n",
    "#        {\n",
    "#            'how_many': 2,\n",
    "#            'n_cells_1': hp.quniform('number_of_cells_1', low=10, high=100, q=2),\n",
    "#            'n_cells_2': hp.quniform('number_of_cells_2', low=10, high=50, q=2),\n",
    "#        }\n",
    "    ]),\n",
    "}\n",
    "\n",
    "early_stop_space = {\n",
    "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
    "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
    "}\n",
    "\n",
    "opt_space = {\n",
    "    'optimizer': optimizer_space,\n",
    "    'layers': layer_space,\n",
    "    'bptt': 30, #hp.quniform('bptt_len', low=10, high=150, q=1),\n",
    "    #'early_stop': early_stop_space,\n",
    "    #'use_class_weight': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0Gd1dXGiZV8"
   },
   "source": [
    "### 3.2 Ottimizzazione degli iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "vwdJJ5BnqGXX",
    "outputId": "58aa585c-bc91-43fa-b825-9075558c2f5e"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(n_features, input_shape=(lookback, n_features)))\n",
    "model.add(keras.layers.Dense(2, activation='softmax', name='is_extreme'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=5e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjL0Q5tfrbjN"
   },
   "outputs": [],
   "source": [
    "y_train_bal_cat = keras.utils.to_categorical(y_train_bal, num_classes=2)\n",
    "y_validation_cat = keras.utils.to_categorical(y_validation, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IpYAG05UTWDx",
    "outputId": "4ac19649-9d71-42c9-d5be-2ffd9252eb5d"
   },
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "batch_size = 1645\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train_bal.astype(np.float32),\n",
    "    y=y_train_bal_cat,\n",
    "    batch_size=batch_size,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=(X_validation, y_validation_cat),\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5gX8eBRZeU4"
   },
   "source": [
    "Ora vedo come variare il threshold per ottenere le curve ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5nXyquVVsRW"
   },
   "outputs": [],
   "source": [
    "def loss_function(theta, recall, fpr):\n",
    "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
    "    assert theta >= 0.0 and theta <= 1.0\n",
    "    \n",
    "    return theta * (1 - recall) + (1 - theta) * fpr\n",
    "\n",
    "\n",
    "def utility_function(theta, loss):\n",
    "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
    "    return min(theta, 1 - theta) - loss\n",
    "\n",
    "\n",
    "def to_binary(prob: np.ndarray, thresh: float):\n",
    "    assert thresh <= 1.0 and thresh >= 0.0\n",
    "    \n",
    "    return (prob >= thresh).astype(np.int8)\n",
    "\n",
    "\n",
    "def recall_fpr_kss_precision(y_true, y_pred):\n",
    "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
    "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "    tn = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    fp = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        y_pred\n",
    "    ))\n",
    "    fn = np.sum(np.logical_and(\n",
    "        y_true,\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    \n",
    "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
    "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    kss = recall - fpr\n",
    "    \n",
    "    return recall, fpr, kss, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0ylM6MWZjHg"
   },
   "outputs": [],
   "source": [
    "w_t = np.arange(0, 1, 1e-3)\n",
    "theta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBtZN0LrZ4_R"
   },
   "outputs": [],
   "source": [
    "def optimize_wt(w, theta, model, X, y_true, verbose=False):\n",
    "    \"\"\"Get the best threshold for the class 1 probability.\"\"\"\n",
    "    probabilities = model.predict(X, batch_size=batch_size)[:, 1]\n",
    "\n",
    "    recalls = np.zeros((w.shape[0], ), dtype=np.float64)\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    precisions = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    for i, thresh in enumerate(w):\n",
    "        if i % 200 == 0 and verbose:\n",
    "            print(f\"iteration {i} / {len(w_t)}\")\n",
    "\n",
    "        y_pred = to_binary(probabilities, thresh).astype(np.int8)\n",
    "        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
    "        loss = loss_function(theta, recall, fpr)\n",
    "        utility = utility_function(theta, loss)\n",
    "\n",
    "        recalls[i] = recall\n",
    "        precisions[i] = precision\n",
    "        ksss[i] = kss\n",
    "        fprs[i] = fpr\n",
    "        losses[i] = loss\n",
    "        utilities[i] = utility\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished!\")\n",
    "\n",
    "    return recalls, fprs, ksss, precisions, losses, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_i6fKRXteWS0",
    "outputId": "9d073a6f-427e-47b6-bdb1-0dae3b4809d2"
   },
   "outputs": [],
   "source": [
    "recalls_train, fprs_train, ksss_train, precisions_train, losses_train, utilities_train = \\\n",
    "optimize_wt(w_t, theta, model, X_train, y_train.astype(np.int8))\n",
    "\n",
    "r = optimize_wt(w_t, theta, model, X_train, y_train.astype(np.int8))\n",
    "recalls_validation, fprs_validation, ksss_validation, \\\n",
    "precisions_validation, losses_validation, utilities_validation = \\\n",
    "optimize_wt(w_t, theta, model, X_validation, y_validation.astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "TcQpqDonZ9BB",
    "outputId": "d7324311-3b9b-4993-a1c1-2312876a1c7a"
   },
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "fig.suptitle(f\"{s_type} | ROC curves for train and validation set\", fontsize=16)\n",
    "\n",
    "# train set\n",
    "i_sorted = np.argsort(fprs_train)\n",
    "\n",
    "x = fprs_train[i_sorted]\n",
    "y = recalls_train[i_sorted]\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='navy',\n",
    "    label='train'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_train)\n",
    "best_x = fprs_train[i_sweet]\n",
    "best_y = recalls_train[i_sweet]\n",
    "\n",
    "ax.plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='navy',\n",
    "    label='train - best'\n",
    ")\n",
    "\n",
    "# validation set\n",
    "i_sorted = np.argsort(fprs_train)\n",
    "\n",
    "x = fprs_validation[i_sorted]\n",
    "y = recalls_validation[i_sorted]\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='forestgreen',\n",
    "    label='validation'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_validation)\n",
    "best_x = fprs_validation[i_sweet]\n",
    "best_y = recalls_validation[i_sweet]\n",
    "\n",
    "ax.plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='forestgreen',\n",
    "    label='validation - best'\n",
    ")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "\n",
    "ax.legend(loc='lower right', fontsize=14)\n",
    "ax.set_xlim([0, 1.1])\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.set_xlabel('FPR', fontsize=16)\n",
    "ax.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BckFj1vxdb1B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ANN_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
