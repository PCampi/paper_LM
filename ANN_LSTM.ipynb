{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "-VbbtgVOqGWb",
    "outputId": "3886f162-4c6e-4e2b-fc24-2f34e837da03"
   },
   "outputs": [],
   "source": [
    "ENV = 'local'  # 'colab'\n",
    "if ENV == 'colab':\n",
    "    !pip install -q PyDrive imbalanced-learn ipdb hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qdJhI2pqGWf"
   },
   "source": [
    "# Rete neurale per extreme returns su 2 azioni\n",
    "\n",
    "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica. Qui faremo *solo* l'ottimizzazione degli iperparametri, l'addestramento finale con i parametri ottimali trovati sarà fatta in un altro notebook.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
    "- [x] creazione estremi al 95%\n",
    "- [x] oversampling con due possibili strategie: replicare le istanze positive, o replicarle con aggiunta di rumore gaussiano\n",
    "- [x] addestramento rete con hyperparameter optimization\n",
    "- [ ] ripetizione di ottimizzazione iperparametri per tutte e due le azioni con aggiunta di sentiment\n",
    "\n",
    "Nell'altro notebook dovrò fare:\n",
    "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
    "- [ ] confronto con i risultati del modello probabilistico\n",
    "- [ ] conclusioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "yIWuJMSBqGWg",
    "outputId": "cf7a23a4-0040-4491-d0d4-152e639fc784"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import datetime\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import pickle\n",
    "import copy\n",
    "import pprint\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.testing as pt\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.preprocessing as skpp\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import hyperopt as hy\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "import ipdb\n",
    "\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ImRJwq-Fq4aj",
    "outputId": "b5b7afba-c9a1-4dfd-827c-464cf7e33573"
   },
   "outputs": [],
   "source": [
    "if ENV == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joUuYwuYqGWj"
   },
   "source": [
    "Un po' di dichiarazioni utili per il seguito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRPdSilJqGWk"
   },
   "outputs": [],
   "source": [
    "stock_type = ['min_vol', 'max_vol']\n",
    "return_type = ['pos', 'neg', 'abs']\n",
    "q_type = '95'\n",
    "rs = 42  # random state\n",
    "MAX_EPOCHS = 1000\n",
    "\n",
    "stock_codes = {\n",
    "    'min_vol': '9CE4C7',\n",
    "    'max_vol': 'E28F22'\n",
    "}  # già trovate in Paper-azioni.ipynb\n",
    "\n",
    "stock_colors = {\n",
    "    'min_vol': 'palegoldenrod',\n",
    "    'max_vol': 'coral',\n",
    "}\n",
    "\n",
    "# i giorni sono i primi disponibili in quel mese nei dati\n",
    "split_dates = {\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
    "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
    "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
    "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
    "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
    "    'last_train': datetime.datetime(2017, 1, 3), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIgaOP67qGWm"
   },
   "source": [
    "## 1. Importazione dei dati \n",
    "\n",
    "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "R3FzgUr5qGWn",
    "outputId": "64fbc073-ca55-4683-ac55-bfffa7e45b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE path: /Users/pietro/Google Drive/OptiRisk Thesis/experiments/11_final_experiment\n",
      "TA dir: /Users/pietro/Google Drive/OptiRisk Thesis/data/technical_features/features_all_years\n"
     ]
    }
   ],
   "source": [
    "if ENV == 'colab':\n",
    "    data_path = '/gdrive/My Drive/OptiRisk Thesis/data'\n",
    "    base_path = '/gdrive/My Drive/OptiRisk Thesis/experiments/11_final_experiment'\n",
    "else:\n",
    "    data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "    base_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/experiments/11_final_experiment\"\n",
    "\n",
    "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
    "ta_dir = os.path.join(data_path, 'technical_features', 'features_all_years')\n",
    "print(f\"BASE path: {base_path}\")\n",
    "print(f\"TA dir: {ta_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsmulRB3qGWp"
   },
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "aX80lWk8qGWq",
    "outputId": "7fb5caa1-9c6b-4e46-8e25-14fba800cc7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ravenpackId</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>00067A</td>\n",
       "      <td>29.86</td>\n",
       "      <td>28.97</td>\n",
       "      <td>29.86</td>\n",
       "      <td>29.16</td>\n",
       "      <td>784200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>00067A</td>\n",
       "      <td>29.36</td>\n",
       "      <td>29.03</td>\n",
       "      <td>29.59</td>\n",
       "      <td>29.10</td>\n",
       "      <td>1452100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>00067A</td>\n",
       "      <td>29.15</td>\n",
       "      <td>28.92</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.70</td>\n",
       "      <td>1564400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>00067A</td>\n",
       "      <td>29.75</td>\n",
       "      <td>29.61</td>\n",
       "      <td>30.30</td>\n",
       "      <td>29.64</td>\n",
       "      <td>1321600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>00067A</td>\n",
       "      <td>29.39</td>\n",
       "      <td>29.26</td>\n",
       "      <td>29.56</td>\n",
       "      <td>29.42</td>\n",
       "      <td>808400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ravenpackId   open    low   high  close     volume\n",
       "date                                                         \n",
       "2005-01-03      00067A  29.86  28.97  29.86  29.16   784200.0\n",
       "2005-01-04      00067A  29.36  29.03  29.59  29.10  1452100.0\n",
       "2005-01-05      00067A  29.15  28.92  29.85  29.70  1564400.0\n",
       "2005-01-06      00067A  29.75  29.61  30.30  29.64  1321600.0\n",
       "2005-01-07      00067A  29.39  29.26  29.56  29.42   808400.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = pd.read_csv(prices_path)\n",
    "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
    "prices.index = prices['date']\n",
    "prices.drop(columns=['date'], inplace=True)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VlBiVnJIqGWs"
   },
   "source": [
    "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
    "\n",
    "I prezzi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "HyNlWyFnqGWt",
    "outputId": "a85ef326-d8de-49fe-f3f4-0c2bbe2c0bdc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ravenpackId</th>\n",
       "      <th>00067A</th>\n",
       "      <th>003B70</th>\n",
       "      <th>013528</th>\n",
       "      <th>0157B1</th>\n",
       "      <th>034B61</th>\n",
       "      <th>03B8CF</th>\n",
       "      <th>048590</th>\n",
       "      <th>055018</th>\n",
       "      <th>0624BE</th>\n",
       "      <th>067779</th>\n",
       "      <th>06EF42</th>\n",
       "      <th>071860</th>\n",
       "      <th>07CA6A</th>\n",
       "      <th>07EC43</th>\n",
       "      <th>095294</th>\n",
       "      <th>09F623</th>\n",
       "      <th>0A9D0A</th>\n",
       "      <th>0B57D7</th>\n",
       "      <th>0BC29E</th>\n",
       "      <th>0BE0AE</th>\n",
       "      <th>0BF4BA</th>\n",
       "      <th>0CE7F6</th>\n",
       "      <th>0E431C</th>\n",
       "      <th>0F0440</th>\n",
       "      <th>1151F4</th>\n",
       "      <th>12DE76</th>\n",
       "      <th>131443</th>\n",
       "      <th>143C52</th>\n",
       "      <th>147C38</th>\n",
       "      <th>1490F3</th>\n",
       "      <th>14A113</th>\n",
       "      <th>14BA06</th>\n",
       "      <th>14ED2B</th>\n",
       "      <th>159AE4</th>\n",
       "      <th>15ABD0</th>\n",
       "      <th>164D72</th>\n",
       "      <th>168A5D</th>\n",
       "      <th>16B183</th>\n",
       "      <th>1782D5</th>\n",
       "      <th>1791E7</th>\n",
       "      <th>...</th>\n",
       "      <th>EB6965</th>\n",
       "      <th>EC70EC</th>\n",
       "      <th>EC99D0</th>\n",
       "      <th>ECD263</th>\n",
       "      <th>ECF709</th>\n",
       "      <th>ED79D9</th>\n",
       "      <th>EE6F1C</th>\n",
       "      <th>EEA6B3</th>\n",
       "      <th>EF1AD9</th>\n",
       "      <th>EF5BED</th>\n",
       "      <th>EFD406</th>\n",
       "      <th>F11638</th>\n",
       "      <th>F1FA25</th>\n",
       "      <th>F30508</th>\n",
       "      <th>F39E1E</th>\n",
       "      <th>F40EE2</th>\n",
       "      <th>F4E882</th>\n",
       "      <th>F50BAB</th>\n",
       "      <th>F57F6F</th>\n",
       "      <th>F5C8AB</th>\n",
       "      <th>F5D410</th>\n",
       "      <th>F67165</th>\n",
       "      <th>F6DCE4</th>\n",
       "      <th>F6E248</th>\n",
       "      <th>F82D37</th>\n",
       "      <th>F93C8A</th>\n",
       "      <th>F97C81</th>\n",
       "      <th>FA40E2</th>\n",
       "      <th>FACF19</th>\n",
       "      <th>FAE021</th>\n",
       "      <th>FC1B7B</th>\n",
       "      <th>FC1F9D</th>\n",
       "      <th>FD39EB</th>\n",
       "      <th>FD5C77</th>\n",
       "      <th>FE7A63</th>\n",
       "      <th>FE89E0</th>\n",
       "      <th>FEC475</th>\n",
       "      <th>FEE4B0</th>\n",
       "      <th>FF4BA4</th>\n",
       "      <th>FF6644</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>29.16</td>\n",
       "      <td>25.02</td>\n",
       "      <td>51.94</td>\n",
       "      <td>44.52</td>\n",
       "      <td>23.73</td>\n",
       "      <td>82.40</td>\n",
       "      <td>5.520937</td>\n",
       "      <td>83.80</td>\n",
       "      <td>46.31</td>\n",
       "      <td>57.31</td>\n",
       "      <td>29.5125</td>\n",
       "      <td>19.190565</td>\n",
       "      <td>48.57</td>\n",
       "      <td>10.4575</td>\n",
       "      <td>20.253666</td>\n",
       "      <td>12.199479</td>\n",
       "      <td>16.988913</td>\n",
       "      <td>27.79</td>\n",
       "      <td>1106.385474</td>\n",
       "      <td>56.469823</td>\n",
       "      <td>12.45</td>\n",
       "      <td>26.53</td>\n",
       "      <td>23.24</td>\n",
       "      <td>49.27</td>\n",
       "      <td>18.02</td>\n",
       "      <td>19.32</td>\n",
       "      <td>20.5750</td>\n",
       "      <td>9.513338</td>\n",
       "      <td>33.06</td>\n",
       "      <td>13.59</td>\n",
       "      <td>26.17</td>\n",
       "      <td>37.20</td>\n",
       "      <td>22.795</td>\n",
       "      <td>28.750</td>\n",
       "      <td>23.190</td>\n",
       "      <td>32.125</td>\n",
       "      <td>57.980</td>\n",
       "      <td>48.28</td>\n",
       "      <td>12.733340</td>\n",
       "      <td>13.2700</td>\n",
       "      <td>...</td>\n",
       "      <td>25.010</td>\n",
       "      <td>16.623234</td>\n",
       "      <td>26.79</td>\n",
       "      <td>1.702855</td>\n",
       "      <td>90.75</td>\n",
       "      <td>16.089644</td>\n",
       "      <td>28.404721</td>\n",
       "      <td>20.770</td>\n",
       "      <td>46.66</td>\n",
       "      <td>35.096468</td>\n",
       "      <td>19.680</td>\n",
       "      <td>21.450</td>\n",
       "      <td>88.53</td>\n",
       "      <td>56.25</td>\n",
       "      <td>29.033333</td>\n",
       "      <td>11.71</td>\n",
       "      <td>19.17</td>\n",
       "      <td>32.275493</td>\n",
       "      <td>12.35</td>\n",
       "      <td>27.420</td>\n",
       "      <td>14.335</td>\n",
       "      <td>82.77</td>\n",
       "      <td>51.80</td>\n",
       "      <td>8.6825</td>\n",
       "      <td>17.25</td>\n",
       "      <td>14.049817</td>\n",
       "      <td>29.507073</td>\n",
       "      <td>22.745997</td>\n",
       "      <td>40.38</td>\n",
       "      <td>85.00</td>\n",
       "      <td>48.670100</td>\n",
       "      <td>9.2600</td>\n",
       "      <td>17.705</td>\n",
       "      <td>31.60</td>\n",
       "      <td>87.272161</td>\n",
       "      <td>32.059368</td>\n",
       "      <td>53.86</td>\n",
       "      <td>10.8450</td>\n",
       "      <td>24.777032</td>\n",
       "      <td>33.605482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>29.10</td>\n",
       "      <td>24.79</td>\n",
       "      <td>51.57</td>\n",
       "      <td>42.14</td>\n",
       "      <td>23.88</td>\n",
       "      <td>81.73</td>\n",
       "      <td>5.377500</td>\n",
       "      <td>82.25</td>\n",
       "      <td>47.28</td>\n",
       "      <td>57.05</td>\n",
       "      <td>28.2075</td>\n",
       "      <td>18.919827</td>\n",
       "      <td>48.19</td>\n",
       "      <td>10.3400</td>\n",
       "      <td>19.881972</td>\n",
       "      <td>11.993747</td>\n",
       "      <td>16.548941</td>\n",
       "      <td>26.79</td>\n",
       "      <td>1110.071750</td>\n",
       "      <td>55.707593</td>\n",
       "      <td>12.15</td>\n",
       "      <td>26.01</td>\n",
       "      <td>21.42</td>\n",
       "      <td>47.70</td>\n",
       "      <td>17.75</td>\n",
       "      <td>18.56</td>\n",
       "      <td>20.0925</td>\n",
       "      <td>9.210005</td>\n",
       "      <td>33.04</td>\n",
       "      <td>13.65</td>\n",
       "      <td>25.77</td>\n",
       "      <td>37.11</td>\n",
       "      <td>22.335</td>\n",
       "      <td>28.410</td>\n",
       "      <td>23.195</td>\n",
       "      <td>31.880</td>\n",
       "      <td>57.100</td>\n",
       "      <td>47.89</td>\n",
       "      <td>12.820006</td>\n",
       "      <td>13.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>24.835</td>\n",
       "      <td>16.118615</td>\n",
       "      <td>25.80</td>\n",
       "      <td>1.665713</td>\n",
       "      <td>89.70</td>\n",
       "      <td>15.665169</td>\n",
       "      <td>27.148167</td>\n",
       "      <td>20.480</td>\n",
       "      <td>46.38</td>\n",
       "      <td>34.555870</td>\n",
       "      <td>19.755</td>\n",
       "      <td>20.745</td>\n",
       "      <td>87.70</td>\n",
       "      <td>55.50</td>\n",
       "      <td>28.840000</td>\n",
       "      <td>11.53</td>\n",
       "      <td>19.19</td>\n",
       "      <td>31.872050</td>\n",
       "      <td>12.15</td>\n",
       "      <td>26.965</td>\n",
       "      <td>14.150</td>\n",
       "      <td>81.02</td>\n",
       "      <td>51.24</td>\n",
       "      <td>8.4700</td>\n",
       "      <td>16.56</td>\n",
       "      <td>13.989818</td>\n",
       "      <td>29.470589</td>\n",
       "      <td>22.668179</td>\n",
       "      <td>40.09</td>\n",
       "      <td>83.92</td>\n",
       "      <td>47.626845</td>\n",
       "      <td>9.0575</td>\n",
       "      <td>17.355</td>\n",
       "      <td>31.23</td>\n",
       "      <td>86.506884</td>\n",
       "      <td>31.849730</td>\n",
       "      <td>53.47</td>\n",
       "      <td>10.8450</td>\n",
       "      <td>24.396131</td>\n",
       "      <td>33.005724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>29.70</td>\n",
       "      <td>23.53</td>\n",
       "      <td>51.63</td>\n",
       "      <td>41.77</td>\n",
       "      <td>23.59</td>\n",
       "      <td>80.73</td>\n",
       "      <td>5.456250</td>\n",
       "      <td>81.43</td>\n",
       "      <td>47.47</td>\n",
       "      <td>56.95</td>\n",
       "      <td>27.6000</td>\n",
       "      <td>18.282795</td>\n",
       "      <td>47.90</td>\n",
       "      <td>10.2450</td>\n",
       "      <td>19.794515</td>\n",
       "      <td>11.910579</td>\n",
       "      <td>16.418949</td>\n",
       "      <td>25.88</td>\n",
       "      <td>1128.503130</td>\n",
       "      <td>54.977800</td>\n",
       "      <td>12.25</td>\n",
       "      <td>25.99</td>\n",
       "      <td>21.21</td>\n",
       "      <td>48.08</td>\n",
       "      <td>17.55</td>\n",
       "      <td>18.57</td>\n",
       "      <td>19.2175</td>\n",
       "      <td>9.433338</td>\n",
       "      <td>32.84</td>\n",
       "      <td>13.20</td>\n",
       "      <td>25.07</td>\n",
       "      <td>36.60</td>\n",
       "      <td>21.910</td>\n",
       "      <td>27.200</td>\n",
       "      <td>22.790</td>\n",
       "      <td>31.980</td>\n",
       "      <td>57.200</td>\n",
       "      <td>47.73</td>\n",
       "      <td>12.600006</td>\n",
       "      <td>12.8000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.275</td>\n",
       "      <td>15.805835</td>\n",
       "      <td>25.64</td>\n",
       "      <td>1.599998</td>\n",
       "      <td>90.29</td>\n",
       "      <td>15.658431</td>\n",
       "      <td>26.855286</td>\n",
       "      <td>20.385</td>\n",
       "      <td>46.21</td>\n",
       "      <td>34.608869</td>\n",
       "      <td>19.650</td>\n",
       "      <td>20.510</td>\n",
       "      <td>86.59</td>\n",
       "      <td>56.08</td>\n",
       "      <td>28.893333</td>\n",
       "      <td>11.40</td>\n",
       "      <td>18.89</td>\n",
       "      <td>30.738565</td>\n",
       "      <td>12.00</td>\n",
       "      <td>27.815</td>\n",
       "      <td>13.920</td>\n",
       "      <td>78.09</td>\n",
       "      <td>51.28</td>\n",
       "      <td>8.3525</td>\n",
       "      <td>16.36</td>\n",
       "      <td>13.904819</td>\n",
       "      <td>28.950680</td>\n",
       "      <td>22.442509</td>\n",
       "      <td>41.12</td>\n",
       "      <td>83.75</td>\n",
       "      <td>48.352587</td>\n",
       "      <td>8.9075</td>\n",
       "      <td>17.140</td>\n",
       "      <td>30.66</td>\n",
       "      <td>86.200773</td>\n",
       "      <td>32.086049</td>\n",
       "      <td>52.94</td>\n",
       "      <td>10.5875</td>\n",
       "      <td>24.499677</td>\n",
       "      <td>32.910524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>29.64</td>\n",
       "      <td>23.55</td>\n",
       "      <td>52.00</td>\n",
       "      <td>41.05</td>\n",
       "      <td>24.18</td>\n",
       "      <td>81.46</td>\n",
       "      <td>5.380312</td>\n",
       "      <td>82.45</td>\n",
       "      <td>50.12</td>\n",
       "      <td>57.36</td>\n",
       "      <td>28.1550</td>\n",
       "      <td>18.163352</td>\n",
       "      <td>47.70</td>\n",
       "      <td>10.1225</td>\n",
       "      <td>19.652397</td>\n",
       "      <td>11.928088</td>\n",
       "      <td>16.498944</td>\n",
       "      <td>25.69</td>\n",
       "      <td>1130.346268</td>\n",
       "      <td>54.977800</td>\n",
       "      <td>12.33</td>\n",
       "      <td>26.21</td>\n",
       "      <td>21.42</td>\n",
       "      <td>47.02</td>\n",
       "      <td>17.52</td>\n",
       "      <td>18.85</td>\n",
       "      <td>19.1300</td>\n",
       "      <td>9.366671</td>\n",
       "      <td>32.63</td>\n",
       "      <td>13.39</td>\n",
       "      <td>25.51</td>\n",
       "      <td>37.11</td>\n",
       "      <td>22.200</td>\n",
       "      <td>27.705</td>\n",
       "      <td>22.800</td>\n",
       "      <td>32.605</td>\n",
       "      <td>57.480</td>\n",
       "      <td>48.50</td>\n",
       "      <td>12.800006</td>\n",
       "      <td>12.9000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.865</td>\n",
       "      <td>15.985162</td>\n",
       "      <td>25.18</td>\n",
       "      <td>1.578570</td>\n",
       "      <td>89.55</td>\n",
       "      <td>15.314808</td>\n",
       "      <td>27.242645</td>\n",
       "      <td>20.550</td>\n",
       "      <td>46.41</td>\n",
       "      <td>35.297867</td>\n",
       "      <td>19.630</td>\n",
       "      <td>20.380</td>\n",
       "      <td>86.84</td>\n",
       "      <td>56.53</td>\n",
       "      <td>28.866666</td>\n",
       "      <td>11.74</td>\n",
       "      <td>18.88</td>\n",
       "      <td>30.738565</td>\n",
       "      <td>12.05</td>\n",
       "      <td>27.955</td>\n",
       "      <td>14.000</td>\n",
       "      <td>77.70</td>\n",
       "      <td>48.50</td>\n",
       "      <td>8.4050</td>\n",
       "      <td>16.40</td>\n",
       "      <td>13.949819</td>\n",
       "      <td>29.032771</td>\n",
       "      <td>22.613707</td>\n",
       "      <td>41.14</td>\n",
       "      <td>83.66</td>\n",
       "      <td>48.225582</td>\n",
       "      <td>9.0875</td>\n",
       "      <td>17.310</td>\n",
       "      <td>30.30</td>\n",
       "      <td>85.680385</td>\n",
       "      <td>32.551064</td>\n",
       "      <td>53.32</td>\n",
       "      <td>10.6725</td>\n",
       "      <td>24.599525</td>\n",
       "      <td>32.691565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>29.42</td>\n",
       "      <td>23.40</td>\n",
       "      <td>52.45</td>\n",
       "      <td>42.32</td>\n",
       "      <td>25.01</td>\n",
       "      <td>82.30</td>\n",
       "      <td>5.200312</td>\n",
       "      <td>81.96</td>\n",
       "      <td>49.50</td>\n",
       "      <td>57.50</td>\n",
       "      <td>28.0950</td>\n",
       "      <td>18.338536</td>\n",
       "      <td>47.10</td>\n",
       "      <td>10.1300</td>\n",
       "      <td>19.685193</td>\n",
       "      <td>11.901824</td>\n",
       "      <td>16.248960</td>\n",
       "      <td>25.77</td>\n",
       "      <td>1132.356964</td>\n",
       "      <td>54.750753</td>\n",
       "      <td>12.64</td>\n",
       "      <td>26.15</td>\n",
       "      <td>21.07</td>\n",
       "      <td>46.80</td>\n",
       "      <td>17.42</td>\n",
       "      <td>18.72</td>\n",
       "      <td>18.7625</td>\n",
       "      <td>9.283338</td>\n",
       "      <td>32.78</td>\n",
       "      <td>13.26</td>\n",
       "      <td>25.76</td>\n",
       "      <td>37.01</td>\n",
       "      <td>22.395</td>\n",
       "      <td>27.730</td>\n",
       "      <td>22.695</td>\n",
       "      <td>31.765</td>\n",
       "      <td>58.378</td>\n",
       "      <td>48.19</td>\n",
       "      <td>12.566673</td>\n",
       "      <td>12.8525</td>\n",
       "      <td>...</td>\n",
       "      <td>25.040</td>\n",
       "      <td>15.972651</td>\n",
       "      <td>25.46</td>\n",
       "      <td>1.588570</td>\n",
       "      <td>89.73</td>\n",
       "      <td>15.301333</td>\n",
       "      <td>26.907249</td>\n",
       "      <td>20.580</td>\n",
       "      <td>46.42</td>\n",
       "      <td>35.308467</td>\n",
       "      <td>19.935</td>\n",
       "      <td>20.635</td>\n",
       "      <td>86.70</td>\n",
       "      <td>56.26</td>\n",
       "      <td>28.700000</td>\n",
       "      <td>11.88</td>\n",
       "      <td>18.77</td>\n",
       "      <td>30.748171</td>\n",
       "      <td>12.00</td>\n",
       "      <td>28.225</td>\n",
       "      <td>14.155</td>\n",
       "      <td>77.19</td>\n",
       "      <td>49.02</td>\n",
       "      <td>8.3875</td>\n",
       "      <td>16.19</td>\n",
       "      <td>13.914819</td>\n",
       "      <td>29.051014</td>\n",
       "      <td>22.504763</td>\n",
       "      <td>41.50</td>\n",
       "      <td>83.50</td>\n",
       "      <td>48.125793</td>\n",
       "      <td>8.8650</td>\n",
       "      <td>17.275</td>\n",
       "      <td>31.50</td>\n",
       "      <td>85.251830</td>\n",
       "      <td>32.345238</td>\n",
       "      <td>52.62</td>\n",
       "      <td>10.5900</td>\n",
       "      <td>24.984124</td>\n",
       "      <td>32.567805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 496 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ravenpackId  00067A  003B70  013528  ...   FEE4B0     FF4BA4     FF6644\n",
       "date                                 ...                               \n",
       "2005-01-03    29.16   25.02   51.94  ...  10.8450  24.777032  33.605482\n",
       "2005-01-04    29.10   24.79   51.57  ...  10.8450  24.396131  33.005724\n",
       "2005-01-05    29.70   23.53   51.63  ...  10.5875  24.499677  32.910524\n",
       "2005-01-06    29.64   23.55   52.00  ...  10.6725  24.599525  32.691565\n",
       "2005-01-07    29.42   23.40   52.45  ...  10.5900  24.984124  32.567805\n",
       "\n",
       "[5 rows x 496 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
    "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
    "prices_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-TCT_OnqGWv"
   },
   "source": [
    "I volumi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "2EegGob7qGWv",
    "outputId": "80a9b8c7-23d2-465e-ee2e-468657abadf8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ravenpackId</th>\n",
       "      <th>00067A</th>\n",
       "      <th>003B70</th>\n",
       "      <th>013528</th>\n",
       "      <th>0157B1</th>\n",
       "      <th>034B61</th>\n",
       "      <th>03B8CF</th>\n",
       "      <th>048590</th>\n",
       "      <th>055018</th>\n",
       "      <th>0624BE</th>\n",
       "      <th>067779</th>\n",
       "      <th>06EF42</th>\n",
       "      <th>071860</th>\n",
       "      <th>07CA6A</th>\n",
       "      <th>07EC43</th>\n",
       "      <th>095294</th>\n",
       "      <th>09F623</th>\n",
       "      <th>0A9D0A</th>\n",
       "      <th>0B57D7</th>\n",
       "      <th>0BC29E</th>\n",
       "      <th>0BE0AE</th>\n",
       "      <th>0BF4BA</th>\n",
       "      <th>0CE7F6</th>\n",
       "      <th>0E431C</th>\n",
       "      <th>0F0440</th>\n",
       "      <th>1151F4</th>\n",
       "      <th>12DE76</th>\n",
       "      <th>131443</th>\n",
       "      <th>143C52</th>\n",
       "      <th>147C38</th>\n",
       "      <th>1490F3</th>\n",
       "      <th>14A113</th>\n",
       "      <th>14BA06</th>\n",
       "      <th>14ED2B</th>\n",
       "      <th>159AE4</th>\n",
       "      <th>15ABD0</th>\n",
       "      <th>164D72</th>\n",
       "      <th>168A5D</th>\n",
       "      <th>16B183</th>\n",
       "      <th>1782D5</th>\n",
       "      <th>1791E7</th>\n",
       "      <th>...</th>\n",
       "      <th>EB6965</th>\n",
       "      <th>EC70EC</th>\n",
       "      <th>EC99D0</th>\n",
       "      <th>ECD263</th>\n",
       "      <th>ECF709</th>\n",
       "      <th>ED79D9</th>\n",
       "      <th>EE6F1C</th>\n",
       "      <th>EEA6B3</th>\n",
       "      <th>EF1AD9</th>\n",
       "      <th>EF5BED</th>\n",
       "      <th>EFD406</th>\n",
       "      <th>F11638</th>\n",
       "      <th>F1FA25</th>\n",
       "      <th>F30508</th>\n",
       "      <th>F39E1E</th>\n",
       "      <th>F40EE2</th>\n",
       "      <th>F4E882</th>\n",
       "      <th>F50BAB</th>\n",
       "      <th>F57F6F</th>\n",
       "      <th>F5C8AB</th>\n",
       "      <th>F5D410</th>\n",
       "      <th>F67165</th>\n",
       "      <th>F6DCE4</th>\n",
       "      <th>F6E248</th>\n",
       "      <th>F82D37</th>\n",
       "      <th>F93C8A</th>\n",
       "      <th>F97C81</th>\n",
       "      <th>FA40E2</th>\n",
       "      <th>FACF19</th>\n",
       "      <th>FAE021</th>\n",
       "      <th>FC1B7B</th>\n",
       "      <th>FC1F9D</th>\n",
       "      <th>FD39EB</th>\n",
       "      <th>FD5C77</th>\n",
       "      <th>FE7A63</th>\n",
       "      <th>FE89E0</th>\n",
       "      <th>FEC475</th>\n",
       "      <th>FEE4B0</th>\n",
       "      <th>FF4BA4</th>\n",
       "      <th>FF6644</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>784200.0</td>\n",
       "      <td>342400.0</td>\n",
       "      <td>3513100.0</td>\n",
       "      <td>10481801.0</td>\n",
       "      <td>1128420.0</td>\n",
       "      <td>2470500.0</td>\n",
       "      <td>2013090.0</td>\n",
       "      <td>1332100.0</td>\n",
       "      <td>1389500.0</td>\n",
       "      <td>1582700.0</td>\n",
       "      <td>1378900.0</td>\n",
       "      <td>2319253.0</td>\n",
       "      <td>761300.0</td>\n",
       "      <td>2370116.0</td>\n",
       "      <td>744300.0</td>\n",
       "      <td>504400.0</td>\n",
       "      <td>434010.0</td>\n",
       "      <td>2006126.0</td>\n",
       "      <td>6297800.0</td>\n",
       "      <td>3528900.0</td>\n",
       "      <td>21100.0</td>\n",
       "      <td>846600.0</td>\n",
       "      <td>4316178.0</td>\n",
       "      <td>3357500.0</td>\n",
       "      <td>876800.0</td>\n",
       "      <td>57030624.0</td>\n",
       "      <td>411700.0</td>\n",
       "      <td>2194007.0</td>\n",
       "      <td>1589600.0</td>\n",
       "      <td>178500.0</td>\n",
       "      <td>1047300.0</td>\n",
       "      <td>3532500.0</td>\n",
       "      <td>486000.0</td>\n",
       "      <td>340600.0</td>\n",
       "      <td>302300.0</td>\n",
       "      <td>4226800.0</td>\n",
       "      <td>11610.0</td>\n",
       "      <td>2504400.0</td>\n",
       "      <td>1654700.0</td>\n",
       "      <td>244400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1961300.0</td>\n",
       "      <td>1138600.0</td>\n",
       "      <td>4659817.0</td>\n",
       "      <td>1637955.0</td>\n",
       "      <td>851200.0</td>\n",
       "      <td>2234800.0</td>\n",
       "      <td>335700.0</td>\n",
       "      <td>5395500.0</td>\n",
       "      <td>715200.0</td>\n",
       "      <td>2061000.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2358324.0</td>\n",
       "      <td>337900.0</td>\n",
       "      <td>2579600.0</td>\n",
       "      <td>948100.0</td>\n",
       "      <td>4435700.0</td>\n",
       "      <td>2185800.0</td>\n",
       "      <td>579500.0</td>\n",
       "      <td>261800.0</td>\n",
       "      <td>607149.0</td>\n",
       "      <td>441300.0</td>\n",
       "      <td>238300.0</td>\n",
       "      <td>3654200.0</td>\n",
       "      <td>3143289.0</td>\n",
       "      <td>218500.0</td>\n",
       "      <td>222000.0</td>\n",
       "      <td>409900.0</td>\n",
       "      <td>1439800.0</td>\n",
       "      <td>9755500.0</td>\n",
       "      <td>1779000.0</td>\n",
       "      <td>1897800.0</td>\n",
       "      <td>582397.0</td>\n",
       "      <td>433600.0</td>\n",
       "      <td>1570500.0</td>\n",
       "      <td>599400.0</td>\n",
       "      <td>2501700.0</td>\n",
       "      <td>1890100.0</td>\n",
       "      <td>336900.0</td>\n",
       "      <td>914500.0</td>\n",
       "      <td>3347400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>1452100.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>3282500.0</td>\n",
       "      <td>19511331.0</td>\n",
       "      <td>2967534.0</td>\n",
       "      <td>2871400.0</td>\n",
       "      <td>2108554.0</td>\n",
       "      <td>1164200.0</td>\n",
       "      <td>2069800.0</td>\n",
       "      <td>2997800.0</td>\n",
       "      <td>2111100.0</td>\n",
       "      <td>3149591.0</td>\n",
       "      <td>514000.0</td>\n",
       "      <td>2590269.0</td>\n",
       "      <td>887200.0</td>\n",
       "      <td>530700.0</td>\n",
       "      <td>466235.0</td>\n",
       "      <td>2682223.0</td>\n",
       "      <td>6323400.0</td>\n",
       "      <td>1537600.0</td>\n",
       "      <td>71800.0</td>\n",
       "      <td>692000.0</td>\n",
       "      <td>10986212.0</td>\n",
       "      <td>3032700.0</td>\n",
       "      <td>1218900.0</td>\n",
       "      <td>106359426.0</td>\n",
       "      <td>635200.0</td>\n",
       "      <td>1818470.0</td>\n",
       "      <td>2011700.0</td>\n",
       "      <td>137100.0</td>\n",
       "      <td>1092700.0</td>\n",
       "      <td>2542000.0</td>\n",
       "      <td>643700.0</td>\n",
       "      <td>346600.0</td>\n",
       "      <td>373200.0</td>\n",
       "      <td>3503600.0</td>\n",
       "      <td>15280.0</td>\n",
       "      <td>2857800.0</td>\n",
       "      <td>760300.0</td>\n",
       "      <td>338800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1675100.0</td>\n",
       "      <td>802200.0</td>\n",
       "      <td>7618972.0</td>\n",
       "      <td>2482076.0</td>\n",
       "      <td>600100.0</td>\n",
       "      <td>2449800.0</td>\n",
       "      <td>831000.0</td>\n",
       "      <td>6643100.0</td>\n",
       "      <td>687100.0</td>\n",
       "      <td>3148500.0</td>\n",
       "      <td>554800.0</td>\n",
       "      <td>2218273.0</td>\n",
       "      <td>209800.0</td>\n",
       "      <td>3518700.0</td>\n",
       "      <td>624000.0</td>\n",
       "      <td>5924000.0</td>\n",
       "      <td>1242500.0</td>\n",
       "      <td>667400.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>618626.0</td>\n",
       "      <td>347900.0</td>\n",
       "      <td>216700.0</td>\n",
       "      <td>2081200.0</td>\n",
       "      <td>5035632.0</td>\n",
       "      <td>363800.0</td>\n",
       "      <td>179000.0</td>\n",
       "      <td>477000.0</td>\n",
       "      <td>1644100.0</td>\n",
       "      <td>6594800.0</td>\n",
       "      <td>2491400.0</td>\n",
       "      <td>3311300.0</td>\n",
       "      <td>307987.0</td>\n",
       "      <td>880500.0</td>\n",
       "      <td>1757700.0</td>\n",
       "      <td>589500.0</td>\n",
       "      <td>2231200.0</td>\n",
       "      <td>1665000.0</td>\n",
       "      <td>566500.0</td>\n",
       "      <td>1084500.0</td>\n",
       "      <td>4165600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>1564400.0</td>\n",
       "      <td>507400.0</td>\n",
       "      <td>3343500.0</td>\n",
       "      <td>8362057.0</td>\n",
       "      <td>887324.0</td>\n",
       "      <td>2484300.0</td>\n",
       "      <td>1556511.0</td>\n",
       "      <td>1233600.0</td>\n",
       "      <td>1762400.0</td>\n",
       "      <td>2376400.0</td>\n",
       "      <td>2104100.0</td>\n",
       "      <td>2489874.0</td>\n",
       "      <td>653100.0</td>\n",
       "      <td>1489251.0</td>\n",
       "      <td>735000.0</td>\n",
       "      <td>511500.0</td>\n",
       "      <td>215965.0</td>\n",
       "      <td>3422863.0</td>\n",
       "      <td>9202400.0</td>\n",
       "      <td>1417600.0</td>\n",
       "      <td>96900.0</td>\n",
       "      <td>546300.0</td>\n",
       "      <td>10074626.0</td>\n",
       "      <td>2559500.0</td>\n",
       "      <td>1426900.0</td>\n",
       "      <td>66139402.0</td>\n",
       "      <td>1524600.0</td>\n",
       "      <td>1912952.0</td>\n",
       "      <td>1497600.0</td>\n",
       "      <td>231400.0</td>\n",
       "      <td>1868000.0</td>\n",
       "      <td>3399400.0</td>\n",
       "      <td>1080800.0</td>\n",
       "      <td>589500.0</td>\n",
       "      <td>222300.0</td>\n",
       "      <td>3293400.0</td>\n",
       "      <td>27460.0</td>\n",
       "      <td>2713300.0</td>\n",
       "      <td>768100.0</td>\n",
       "      <td>241800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3642400.0</td>\n",
       "      <td>850500.0</td>\n",
       "      <td>4380475.0</td>\n",
       "      <td>1820097.0</td>\n",
       "      <td>691500.0</td>\n",
       "      <td>2420300.0</td>\n",
       "      <td>790000.0</td>\n",
       "      <td>5451800.0</td>\n",
       "      <td>322200.0</td>\n",
       "      <td>2161900.0</td>\n",
       "      <td>465600.0</td>\n",
       "      <td>1928287.0</td>\n",
       "      <td>310400.0</td>\n",
       "      <td>2999800.0</td>\n",
       "      <td>464100.0</td>\n",
       "      <td>5628500.0</td>\n",
       "      <td>1973900.0</td>\n",
       "      <td>888200.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>1028227.0</td>\n",
       "      <td>610600.0</td>\n",
       "      <td>371000.0</td>\n",
       "      <td>2229200.0</td>\n",
       "      <td>10857632.0</td>\n",
       "      <td>290200.0</td>\n",
       "      <td>222900.0</td>\n",
       "      <td>709400.0</td>\n",
       "      <td>1895800.0</td>\n",
       "      <td>6704700.0</td>\n",
       "      <td>1790900.0</td>\n",
       "      <td>2148500.0</td>\n",
       "      <td>189450.0</td>\n",
       "      <td>526200.0</td>\n",
       "      <td>1409700.0</td>\n",
       "      <td>411700.0</td>\n",
       "      <td>3258900.0</td>\n",
       "      <td>1626500.0</td>\n",
       "      <td>563500.0</td>\n",
       "      <td>1155100.0</td>\n",
       "      <td>4733900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>1321600.0</td>\n",
       "      <td>317500.0</td>\n",
       "      <td>2961200.0</td>\n",
       "      <td>8704606.0</td>\n",
       "      <td>2636821.0</td>\n",
       "      <td>2388400.0</td>\n",
       "      <td>1524140.0</td>\n",
       "      <td>1102000.0</td>\n",
       "      <td>5392400.0</td>\n",
       "      <td>3279800.0</td>\n",
       "      <td>1921500.0</td>\n",
       "      <td>2717232.0</td>\n",
       "      <td>652100.0</td>\n",
       "      <td>1411730.0</td>\n",
       "      <td>965900.0</td>\n",
       "      <td>390400.0</td>\n",
       "      <td>290141.0</td>\n",
       "      <td>1979287.0</td>\n",
       "      <td>6467700.0</td>\n",
       "      <td>880300.0</td>\n",
       "      <td>154300.0</td>\n",
       "      <td>496100.0</td>\n",
       "      <td>7346534.0</td>\n",
       "      <td>3870600.0</td>\n",
       "      <td>1001800.0</td>\n",
       "      <td>63820315.0</td>\n",
       "      <td>794200.0</td>\n",
       "      <td>747931.0</td>\n",
       "      <td>2231400.0</td>\n",
       "      <td>73200.0</td>\n",
       "      <td>1616400.0</td>\n",
       "      <td>3671700.0</td>\n",
       "      <td>1371200.0</td>\n",
       "      <td>326400.0</td>\n",
       "      <td>161900.0</td>\n",
       "      <td>3616700.0</td>\n",
       "      <td>15860.0</td>\n",
       "      <td>3020400.0</td>\n",
       "      <td>882700.0</td>\n",
       "      <td>145300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2519200.0</td>\n",
       "      <td>704600.0</td>\n",
       "      <td>4811616.0</td>\n",
       "      <td>1181874.0</td>\n",
       "      <td>431600.0</td>\n",
       "      <td>1893000.0</td>\n",
       "      <td>561100.0</td>\n",
       "      <td>4025100.0</td>\n",
       "      <td>337100.0</td>\n",
       "      <td>2088300.0</td>\n",
       "      <td>486200.0</td>\n",
       "      <td>1697628.0</td>\n",
       "      <td>367200.0</td>\n",
       "      <td>2232500.0</td>\n",
       "      <td>578900.0</td>\n",
       "      <td>7545100.0</td>\n",
       "      <td>1065100.0</td>\n",
       "      <td>500200.0</td>\n",
       "      <td>204300.0</td>\n",
       "      <td>760314.0</td>\n",
       "      <td>368300.0</td>\n",
       "      <td>499100.0</td>\n",
       "      <td>11677600.0</td>\n",
       "      <td>3929724.0</td>\n",
       "      <td>283900.0</td>\n",
       "      <td>146100.0</td>\n",
       "      <td>380500.0</td>\n",
       "      <td>933100.0</td>\n",
       "      <td>4613000.0</td>\n",
       "      <td>1280600.0</td>\n",
       "      <td>1905500.0</td>\n",
       "      <td>264535.0</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>1942900.0</td>\n",
       "      <td>496900.0</td>\n",
       "      <td>2827500.0</td>\n",
       "      <td>1355800.0</td>\n",
       "      <td>422200.0</td>\n",
       "      <td>776300.0</td>\n",
       "      <td>3475600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>808400.0</td>\n",
       "      <td>305600.0</td>\n",
       "      <td>3734800.0</td>\n",
       "      <td>9874272.0</td>\n",
       "      <td>2252683.0</td>\n",
       "      <td>2885900.0</td>\n",
       "      <td>2515670.0</td>\n",
       "      <td>1002400.0</td>\n",
       "      <td>2541600.0</td>\n",
       "      <td>2091700.0</td>\n",
       "      <td>1268700.0</td>\n",
       "      <td>2180680.0</td>\n",
       "      <td>607900.0</td>\n",
       "      <td>1242442.0</td>\n",
       "      <td>835800.0</td>\n",
       "      <td>258500.0</td>\n",
       "      <td>356248.0</td>\n",
       "      <td>2679310.0</td>\n",
       "      <td>5359200.0</td>\n",
       "      <td>555300.0</td>\n",
       "      <td>239800.0</td>\n",
       "      <td>530600.0</td>\n",
       "      <td>6663126.0</td>\n",
       "      <td>2452400.0</td>\n",
       "      <td>1267600.0</td>\n",
       "      <td>54457811.0</td>\n",
       "      <td>1667100.0</td>\n",
       "      <td>625495.0</td>\n",
       "      <td>1283500.0</td>\n",
       "      <td>124100.0</td>\n",
       "      <td>1067800.0</td>\n",
       "      <td>2052700.0</td>\n",
       "      <td>800900.0</td>\n",
       "      <td>426300.0</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>3692100.0</td>\n",
       "      <td>11700.0</td>\n",
       "      <td>1559500.0</td>\n",
       "      <td>1003600.0</td>\n",
       "      <td>293400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1640600.0</td>\n",
       "      <td>541300.0</td>\n",
       "      <td>2911807.0</td>\n",
       "      <td>1075288.0</td>\n",
       "      <td>379700.0</td>\n",
       "      <td>1669000.0</td>\n",
       "      <td>330400.0</td>\n",
       "      <td>4114000.0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>2551500.0</td>\n",
       "      <td>597000.0</td>\n",
       "      <td>1566351.0</td>\n",
       "      <td>258800.0</td>\n",
       "      <td>2297100.0</td>\n",
       "      <td>672600.0</td>\n",
       "      <td>6809800.0</td>\n",
       "      <td>1171700.0</td>\n",
       "      <td>288200.0</td>\n",
       "      <td>212400.0</td>\n",
       "      <td>1179891.0</td>\n",
       "      <td>433200.0</td>\n",
       "      <td>212500.0</td>\n",
       "      <td>5769000.0</td>\n",
       "      <td>3666628.0</td>\n",
       "      <td>285400.0</td>\n",
       "      <td>154600.0</td>\n",
       "      <td>518800.0</td>\n",
       "      <td>931900.0</td>\n",
       "      <td>2946400.0</td>\n",
       "      <td>2046200.0</td>\n",
       "      <td>1035300.0</td>\n",
       "      <td>230802.0</td>\n",
       "      <td>355100.0</td>\n",
       "      <td>5453800.0</td>\n",
       "      <td>340100.0</td>\n",
       "      <td>2132700.0</td>\n",
       "      <td>1544200.0</td>\n",
       "      <td>489600.0</td>\n",
       "      <td>1198400.0</td>\n",
       "      <td>3077300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 496 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ravenpackId     00067A    003B70     013528  ...    FEE4B0     FF4BA4     FF6644\n",
       "date                                         ...                                \n",
       "2005-01-03    784200.0  342400.0  3513100.0  ...  336900.0   914500.0  3347400.0\n",
       "2005-01-04   1452100.0  365000.0  3282500.0  ...  566500.0  1084500.0  4165600.0\n",
       "2005-01-05   1564400.0  507400.0  3343500.0  ...  563500.0  1155100.0  4733900.0\n",
       "2005-01-06   1321600.0  317500.0  2961200.0  ...  422200.0   776300.0  3475600.0\n",
       "2005-01-07    808400.0  305600.0  3734800.0  ...  489600.0  1198400.0  3077300.0\n",
       "\n",
       "[5 rows x 496 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
    "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
    "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
    "volume_ts_no_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D11Ov9SBqGWy"
   },
   "source": [
    "Ora calcoliamo i log-returns e le direzioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-hSvXqxqGW0"
   },
   "outputs": [],
   "source": [
    "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
    "directions_ts_no_nan = prices_ts_no_nan.diff(periods=1).iloc[1:, :]\n",
    "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
    "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
    "\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)\n",
    "pt.assert_index_equal(prices_ts_no_nan.index, directions_ts_no_nan.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3N3bQbJkr3c"
   },
   "source": [
    "Mi conviene creare una funzione che standardizzi le features, visto che poi ne avrò più di una (es: returns + volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqZ2KaSJk2Kt"
   },
   "outputs": [],
   "source": [
    "def only_train_notime(feature: pd.Series) -> pd.Series:\n",
    "    \"\"\"Just return the training part of a Series.\"\"\"\n",
    "    f = feature[np.logical_or(\n",
    "        feature.index < split_dates['subprime-crisis-halfway'],\n",
    "        np.logical_and(\n",
    "            feature.index >= split_dates['eu-debt-halfway'],\n",
    "            feature.index < split_dates['last_train']\n",
    "        )\n",
    "    )]\n",
    "\n",
    "    return f\n",
    "\n",
    "def standardize(feature: pd.Series) -> pd.Series:\n",
    "    \"\"\"Standardize a feature by computing the statistics on the training set.\"\"\"\n",
    "    # prendo solo la parte di training, perdendo ogni riferimento alla\n",
    "    # sequenza temporale\n",
    "    tmp_feature_train = only_train_notime(feature)\n",
    "\n",
    "    scaler = skpp.RobustScaler()\n",
    "    scaler.fit(tmp_feature_train.values.reshape(-1, 1))\n",
    "\n",
    "    result = pd.Series(\n",
    "        data=scaler.transform(feature.values.reshape(-1, 1)).flatten(),\n",
    "        index=feature.index\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVoCHZZurG4j"
   },
   "source": [
    "Ora creo i thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soghP5HhqGW3"
   },
   "outputs": [],
   "source": [
    "# ora creo i dati per i returns (non standardizzati), i thresholds e i volumi (standardizzati)\n",
    "lr_train_notime = dict()\n",
    "lr_test_notime = dict()\n",
    "returns_train_notime = dict()\n",
    "\n",
    "# aggiungiamo i dati in modalità taglia-e-cuci\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # training set\n",
    "    lr_current = log_returns.loc[:, s_code]\n",
    "    lr_train_notime[s_type] = only_train_notime(lr_current)\n",
    "    \n",
    "    # returns train, tutti POSITIVI\n",
    "    returns_train_notime[s_type] = {\n",
    "        'pos': lr_train_notime[s_type][lr_train_notime[s_type] > 0.0],\n",
    "        'neg': -(lr_train_notime[s_type][lr_train_notime[s_type] < 0.0]),\n",
    "        'abs': lr_train_notime[s_type].abs()\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# ora creo i threshold\n",
    "thresholds = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: returns_train_notime[s_type][ret_type].quantile(0.95)\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lr51ts-sjf6b"
   },
   "source": [
    "ed infine creo i DataFrame e gli arrays che contengono tutti gli estremi e tutti i dati.\n",
    "\n",
    "Le features che qui utilizziamo sono:\n",
    "\n",
    "- log-returns standardizzati\n",
    "- volume scambiato standardizzato\n",
    "- tutte le features di TA che ci sono nel white paper di Douglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "FzG8axsrrSSa",
    "outputId": "ebc5ddb6-9ca2-42ea-c39f-cc57b6cb0065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock type: min_vol\n",
      "------------------------------\n",
      "standardizing adx\n",
      "dividing aroon_down\n",
      "dividing aroon_up\n",
      "standardizing atr\n",
      "standardizing bb_lower\n",
      "standardizing bb_middle\n",
      "standardizing bb_upper\n",
      "standardizing cci\n",
      "dividing cmo\n",
      "standardizing ema5\n",
      "standardizing ema10\n",
      "standardizing ema15\n",
      "standardizing macd\n",
      "dividing rsi\n",
      "standardizing sma5\n",
      "standardizing sma10\n",
      "standardizing sma15\n",
      "------------------------------\n",
      "\n",
      "Stock type: max_vol\n",
      "------------------------------\n",
      "standardizing adx\n",
      "dividing aroon_down\n",
      "dividing aroon_up\n",
      "standardizing atr\n",
      "standardizing bb_lower\n",
      "standardizing bb_middle\n",
      "standardizing bb_upper\n",
      "standardizing cci\n",
      "dividing cmo\n",
      "standardizing ema5\n",
      "standardizing ema10\n",
      "standardizing ema15\n",
      "standardizing macd\n",
      "dividing rsi\n",
      "standardizing sma5\n",
      "standardizing sma10\n",
      "standardizing sma15\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\n",
    "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
    "]\n",
    "\n",
    "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
    "\n",
    "features = dict()\n",
    "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
    "\n",
    "to_standardize = {\n",
    "    'sma5', 'sma10', 'sma15',\n",
    "    'ema5', 'ema10', 'ema15',\n",
    "    'macd',\n",
    "    'bb_lower', 'bb_middle', 'bb_upper',\n",
    "    'roc', 'atr', 'cci', 'adx',\n",
    "    }\n",
    "\n",
    "to_divide = {\n",
    "    'rsi': 100.0,\n",
    "    'aroon_down': 100.0,\n",
    "    'aroon_up': 100.0,\n",
    "    'cmo': 100.0,\n",
    "}\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    print(f\"Stock type: {s_type}\")\n",
    "    print(\"-\"*30)\n",
    "    features[s_type] = dict()\n",
    "\n",
    "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
    "        feature = pd.read_hdf(feature_path)\n",
    "\n",
    "        if feature_name in to_standardize:\n",
    "            print(f\"standardizing {feature_name}\")\n",
    "            feature_transformed = standardize(feature.loc[:, s_code])\n",
    "            features[s_type][feature_name] = feature_transformed\n",
    "        elif feature_name in to_divide.keys():\n",
    "            print(f\"dividing {feature_name}\")\n",
    "            features[s_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
    "        else:\n",
    "            raise ValueError(f\"unknown feature {feature_name}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90lESpICqGW9"
   },
   "outputs": [],
   "source": [
    "extremes_all = dict()  # keys: s_type, q_type\n",
    "data_all = dict()  # keys: s_type\n",
    "volumes = dict()  # keys: s_type\n",
    "directions_all = dict()  # keys: s_type\n",
    "\n",
    "for s_type, s_code in stock_codes.items():\n",
    "    # i returns\n",
    "    lr = log_returns.loc[:, s_code]\n",
    "    lr_transformed = standardize(lr)\n",
    "\n",
    "    # i volumi\n",
    "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
    "    volume_transformed = standardize(stock_volume)\n",
    "    volumes[s_type] = volume_transformed\n",
    "\n",
    "    # le features tecniche\n",
    "    all_features = [lr_transformed, volume_transformed] + \\\n",
    "              [features[s_type][name] for name in feature_names]\n",
    "\n",
    "    # tutte le features in un unico DataFrame\n",
    "    tmp_df = pd.concat(\n",
    "        all_features,\n",
    "        axis=1,\n",
    "        keys=['log_return', 'volume'] + feature_names\n",
    "    )\n",
    "\n",
    "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
    "    \n",
    "    data_all[s_type] = tmp_df\n",
    "    extremes_all[s_type] = dict()\n",
    "    \n",
    "    ext = np.logical_or(\n",
    "        lr >= thresholds[s_type]['pos'][q_type],\n",
    "        lr <= -thresholds[s_type]['neg'][q_type],\n",
    "    )\n",
    "    \n",
    "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
    "    \n",
    "    # le direzioni\n",
    "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
    "    directions_all[s_type] = direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbKxpt4zqGW8"
   },
   "source": [
    "## 2. Creazione dataset train-test per TensorFlow\n",
    "\n",
    "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
    "\n",
    "- X: cubo dati\n",
    "- y: estremo si/no\n",
    "\n",
    "Per prima cosa, creo delle funzioni che mi creano i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sg7SbAHBqGXA"
   },
   "outputs": [],
   "source": [
    "# testata, funziona con array, Series e DataFrame\n",
    "def rolling_window(data: np.ndarray,\n",
    "                   start: int,\n",
    "                   end: int,\n",
    "                   lookback: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a rolling window view of data, starting at index start, finishing\n",
    "    at index end, with loockback days of bptt.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: series, dataframe or array\n",
    "        the data, containing one row for each time point and one column for each feature\n",
    "        \n",
    "    start: int\n",
    "        starting index in the data\n",
    "        \n",
    "    end: int\n",
    "        index where the whole thing ends, data[end] is **excluded**\n",
    "        \n",
    "    lookback: int\n",
    "        length of the lookback period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray\n",
    "        array of shape(n_points, lookback, n_features)\n",
    "    \"\"\"\n",
    "    assert lookback < data.shape[0]  # lookback sano\n",
    "    assert start - lookback + 1 >= 0  # lookback sano\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    n_points = end - start\n",
    "    \n",
    "    X = np.zeros((n_points, lookback, n_features), dtype = data.dtype)\n",
    "    \n",
    "    # range strano per l'indicizzazione numpy\n",
    "    for i, t in enumerate(range(start + 1, end + 1)):\n",
    "        X[i, :, :] = data[t - lookback:t, :]\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# testata, funziona hehehe\n",
    "def rolling_window_xyd(data: Union[pd.Series, pd.DataFrame],\n",
    "                      targets: List[pd.Series],\n",
    "                      start: int,\n",
    "                      end: int,\n",
    "                      lookback: int) -> Tuple[np.ndarray, List[np.ndarray], pd.Series]:\n",
    "    \"\"\"\n",
    "    Create X, y and dates in a single shot.\n",
    "    The returned dates are relative to the y array(s).\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        my_data = data.values.reshape(-1, 1)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        my_data = data.values\n",
    "    else:\n",
    "        raise TypeError(\"data should be a pandas Series or Dataframe\")\n",
    "\n",
    "    X = rolling_window(my_data, start, end, lookback)\n",
    "    \n",
    "    if not isinstance(targets, list):\n",
    "        raise TypeError(\"target must be a list of pandas Series\")\n",
    "    if not all(isinstance(t, pd.Series) for t in targets):\n",
    "        raise TypeError(\"all targets should be pandas Series\")\n",
    "    if not all(isinstance(t.index, pd.DatetimeIndex) for t in targets):\n",
    "        raise TypeError(\"index of target should be a pandas DatetimeIndex\")\n",
    "        \n",
    "    y = [t.values[start + 1:end + 1] for t in targets]\n",
    "    dates = pd.Series(data=targets[0].index[start + 1: end + 1])\n",
    "        \n",
    "    return X, y, dates\n",
    "\n",
    "\n",
    "# TESTATO: funziona\n",
    "def create_Xyd(returns: Union[pd.Series, pd.DataFrame],\n",
    "               extremes: pd.Series,\n",
    "               directions: pd.Series,\n",
    "               lookback: int) -> Tuple[\n",
    "    np.ndarray, np.ndarray, List[np.ndarray], List[np.ndarray], pd.Series, pd.Series\n",
    "]:\n",
    "    \"\"\"\n",
    "    Create the X, y and dates arrays for the ANN.\n",
    "    \"\"\"\n",
    "    test_start_1 = returns.index.get_loc(split_dates['subprime-crisis-halfway'])\n",
    "    test_end_1 = returns.index.get_loc(split_dates['eu-debt-halfway'])\n",
    "    test_start_2 = returns.index.get_loc(split_dates['last_train'])\n",
    "\n",
    "    # TRAIN\n",
    "    tmp_X_train_1, tmp_y_train_1, tmp_dates_train_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=lookback - 1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "\n",
    "    tmp_X_train_2, tmp_y_train_2, tmp_dates_train_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_end_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_start_2,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    assert len(tmp_y_train_1) == len(tmp_y_train_2)\n",
    "    \n",
    "    X_train = np.concatenate([tmp_X_train_1, tmp_X_train_2])\n",
    "    y_train = [np.concatenate([tmp_y_train_1[i], tmp_y_train_2[i]]) for i in range(len(tmp_y_train_1))]\n",
    "    dates_train = pd.concat([tmp_dates_train_1, tmp_dates_train_2], axis=0, ignore_index=True).values\n",
    "    assert X_train.shape[0] == dates_train.shape[0]\n",
    "    assert all(yy.shape[0] == X_train.shape[0] for yy in y_train)\n",
    "\n",
    "    # TEST\n",
    "    tmp_X_test_1, tmp_y_test_1, tmp_dates_test_1 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_start_1,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=test_end_1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "    \n",
    "    tmp_X_test_2, tmp_y_test_2, tmp_dates_test_2 = rolling_window_xyd(\n",
    "        returns,\n",
    "        [extremes, directions],\n",
    "        start=test_start_2,  # sempre lookback - 1 se il primo iniziale\n",
    "        end=returns.shape[0] - 1,\n",
    "        lookback=lookback\n",
    "    )\n",
    "  \n",
    "    X_test = np.concatenate([tmp_X_test_1, tmp_X_test_2])\n",
    "    y_test = [np.concatenate([tmp_y_test_1[i], tmp_y_test_2[i]]) for i in range(len(tmp_y_test_1))]\n",
    "    dates_test = pd.concat([tmp_dates_test_1, tmp_dates_test_2], axis=0, ignore_index=True).values\n",
    "    assert X_test.shape[0] == dates_test.shape[0]\n",
    "    assert all(yy.shape[0] == X_test.shape[0] for yy in y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
    "\n",
    "\n",
    "def split_stratified(X: np.ndarray,\n",
    "                     y: List[np.ndarray],\n",
    "                     dates: np.ndarray,\n",
    "                     test_size=0.2,\n",
    "                     random_state=rs,\n",
    "                     verbose=False):\n",
    "    \"\"\"\n",
    "    Split a dataset in a stratified fashion on the target variable y[0].\n",
    "    \"\"\"\n",
    "    assert X.ndim == 3\n",
    "    # divido in train-validation, lo faccio prendendo gli indici dagli estremi y/n con un\n",
    "    # ShuffleSplit che divide a caso\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[2]\n",
    "    \n",
    "    XX = np.zeros(n_samples, dtype=np.int8)\n",
    "    \n",
    "    if verbose:\n",
    "        for i in range(len(y)):\n",
    "            vals, counts = np.unique(y[i], return_counts=True)\n",
    "            for v, c in zip(vals, counts):\n",
    "                print(f\"y[{i}] has {c} elements of class {v}\")\n",
    "    \n",
    "    train_index, test_index = next(splitter.split(XX, y[0]))\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    X_validation = X[test_index]\n",
    "    \n",
    "    y_train = [yy[train_index] for yy in y]\n",
    "    y_validation = [yy[test_index] for yy in y]\n",
    "    \n",
    "    dates_train = dates[train_index]\n",
    "    dates_validation = dates[test_index]\n",
    "\n",
    "    return X_train, X_validation, y_train, y_validation, dates_train, dates_validation\n",
    "\n",
    "\n",
    "def oversample_mtl(X: np.ndarray, y: List[np.ndarray], random_state=rs, dt=np.float32):\n",
    "    \"\"\"Oversample a dataset on the positive 1 class.\"\"\"\n",
    "    assert X.dtype == dt\n",
    "    assert X.ndim == 3\n",
    "    assert isinstance(y, list) and all(yy.ndim == 1 for yy in y) and all(yy.dtype == dt for yy in y)\n",
    "    \n",
    "    # oversample\n",
    "    ro = RandomOverSampler(random_state=random_state)\n",
    "    nx = X.shape[0]\n",
    "    indexes = np.arange(nx).reshape(nx, 1)\n",
    "    \n",
    "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y[0])\n",
    "    ir = indexes_resampled.flatten()\n",
    "    \n",
    "    X_resampled = X[ir]\n",
    "    y_resampled = [yy[ir] for yy in y]\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UVy3gkvqGXG"
   },
   "source": [
    "## 3. Addestramento rete\n",
    "\n",
    "Visto che serve l'ottimizzazione degli iperparametri, è importante avere una funzione da ottimizzare con Hyperopt. Aggiungiamo le funzioni che servono.\n",
    "\n",
    "La prima crea il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyNkr3CjOH6t"
   },
   "outputs": [],
   "source": [
    "def create_model_mtl(space: Dict[str, Any],\n",
    "                     bptt: int,\n",
    "                     n_features: int) -> keras.models.Model:\n",
    "    \"\"\"Create a model using the parameters in the search space.\"\"\"\n",
    "    l = space['layers']\n",
    "\n",
    "    input_dropout = float(l['input_dropout'])\n",
    "    assert input_dropout >= 0.0 and input_dropout <= 1.0\n",
    "\n",
    "    n_layers = int(l['num_layers']['how_many'])\n",
    "    assert n_layers <= 2 and n_layers > 0\n",
    "\n",
    "#     n_cells_1 = int(l['num_layers']['n_cells_1'])\n",
    "#     assert n_cells_1 >= 1\n",
    "\n",
    "    # creo il modello\n",
    "    model_input = keras.Input(shape=(bptt, n_features), name='model_input')\n",
    "\n",
    "    if n_layers == 1:\n",
    "        if input_dropout > 0.0:\n",
    "            x = keras.layers.LSTM(n_features, dropout=input_dropout)(model_input)\n",
    "        else:\n",
    "            x = keras.layers.LSTM(n_features)(model_input)\n",
    "    elif n_layers == 2:\n",
    "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
    "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
    "        x = keras.layers.LSTM(n_cells_2)(x)\n",
    "    elif n_layers == 3:\n",
    "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
    "        n_cells_3 = int(l['num_layers']['n_cells_3'])\n",
    "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
    "        x = keras.layers.LSTM(n_cells_2, return_sequences=True)(x)\n",
    "        x = keras.layers.LSTM(n_cells_3)(x)\n",
    "\n",
    "    output_is_extreme = keras.layers.Dense(\n",
    "        2, activation='softmax', name='extreme')(x)\n",
    "    output_is_up_down = keras.layers.Dense(\n",
    "        2, activation='softmax', name='up_down')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=model_input,\n",
    "        outputs=[output_is_extreme, output_is_up_down],\n",
    "        name='MTL_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNGl_yGeOH6v"
   },
   "source": [
    "La seconda valuta le performance e trova i TP, FP, TN, FN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFpAsD-WOH6v"
   },
   "outputs": [],
   "source": [
    "def get_tptnfpfn(y_true, y_pred, labels=[0, 1]) -> Dict[str, int]:\n",
    "    \"\"\"Get the tp, tn, fp, fn count for y_true and y_pred.\"\"\"\n",
    "    cm = sm.confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    tp = cm[1, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    fn = cm[1, 0]\n",
    "\n",
    "    return {\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_performance(tp, tn, fp, fn, weighted=False):\n",
    "    support_1 = tp + fn\n",
    "    support_0 = tn + fp\n",
    "    \n",
    "    if tp + fp > 0:\n",
    "        prec_1 = tp / (tp + fp)\n",
    "    else:\n",
    "        prec_1 = np.nan\n",
    "        \n",
    "    if tn + fn > 0:\n",
    "        prec_0 = tn / (tn + fn)\n",
    "    else:\n",
    "        prec_0 = np.nan\n",
    "    \n",
    "    rec_1 = tp / (tp + fn)\n",
    "    rec_0 = tn / (tn + fp)\n",
    "    \n",
    "    if any(np.isnan(x) for x in [prec_1, prec_0, rec_1, rec_0]):\n",
    "        return np.nan\n",
    "    else:\n",
    "        f1_1 = 2.0 * (prec_1 * rec_1) / (prec_1 + rec_1)\n",
    "        f1_0 = 2.0 * (prec_0 * rec_0) / (prec_0 + rec_0)\n",
    "        \n",
    "        if weighted: # pesa di più la classe di maggioranza\n",
    "            f1_tot = np.average([f1_0, f1_1], weights=[support_0, support_1])\n",
    "        else:\n",
    "            f1_tot = np.mean([f1_0, f1_1])\n",
    "            \n",
    "        return f1_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5c6x_4IOH6y"
   },
   "source": [
    "La terza mi crea il dataframe dei risultati che poi posso analizzare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiCeLJ4lOH60"
   },
   "outputs": [],
   "source": [
    "def get_new_results() -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        data=None,\n",
    "        columns=[\n",
    "            'dataset',\n",
    "            'optimizer',\n",
    "            'start_time',\n",
    "            'experiment_id',\n",
    "            'trial',\n",
    "            'bptt',\n",
    "            'lr',\n",
    "            'n_layers',\n",
    "            'n_cells_1',\n",
    "            'n_cells_2',\n",
    "            'n_cells_3',\n",
    "            'dropout',\n",
    "            'loss_extreme',\n",
    "            'loss_up_down',\n",
    "            'loss_volume',\n",
    "            'f1_validation_extreme',\n",
    "            'f1_validation_up_down',\n",
    "            'tp_extreme',\n",
    "            'tn_extreme',\n",
    "            'fp_extreme',\n",
    "            'fn_extreme',\n",
    "            'tp_up_down',\n",
    "            'tn_up_down',\n",
    "            'fp_up_down',\n",
    "            'fn_up_down',\n",
    "            'train_epochs',\n",
    "            'es_min_delta',\n",
    "            'es_patience',\n",
    "            'use_class_weight',\n",
    "            'output',\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEkG4X9sOH63"
   },
   "source": [
    "Infine l'ultima esegue l'esperimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WR-j2l13OH64"
   },
   "outputs": [],
   "source": [
    "def run_experiment(space,\n",
    "                   stock_type: str,\n",
    "                   max_epochs: int,\n",
    "                   data: Union[pd.Series, pd.DataFrame],\n",
    "                   extremes: pd.Series,\n",
    "                   directions: pd.Series,\n",
    "                   n_runs: int,\n",
    "                   verbose: int,\n",
    "                   results_path: str):\n",
    "    \"\"\"Fai girare un singolo esperimento.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    space: \n",
    "        hyperopt search space\n",
    "        \n",
    "    stock_type:\n",
    "        type of stock to use\n",
    "    \n",
    "    max_epochs: int\n",
    "        number of max epochs to tun the model for\n",
    "        \n",
    "    data: pd.Series of shape (n_timepoints,), or pd.DataFrame of shape (n_timepoints, n_features)\n",
    "        data containing returns, volume and all other things, where every row is\n",
    "        a timepoint and every column a different feature\n",
    "        \n",
    "    extremes: pd.Series of shape (n_timepoints,)\n",
    "        target for the extremes, binary 1/0, \n",
    "        \n",
    "    directions: pd.Series of shape (n_timepoints,)\n",
    "        target for the directions\n",
    "        \n",
    "    n_runs: int\n",
    "        how many times to run a model with the same structure to get a reliable\n",
    "        estimate of the loss function\n",
    "        \n",
    "    verbose: int\n",
    "        verbosity for Keras\n",
    "        \n",
    "    results_path: str or path\n",
    "        path where to save the intermediate runs\n",
    "    \"\"\"\n",
    "    sigmoid_or_softmax = 'softmax'\n",
    "    if data.ndim == 1:\n",
    "        n_features = 1\n",
    "    else:\n",
    "        n_features = data.shape[1]\n",
    "\n",
    "    lookback = bptt = int(space['bptt'])\n",
    "    batch_size = data.shape[0]\n",
    "    \n",
    "    # 1. creazione dataset per questo lookback\n",
    "    X_trv, X_test, y_trv, y_test, dates_trv, dates_test = create_Xyd(\n",
    "        data.astype(np.float32),\n",
    "        extremes.astype(np.float32),\n",
    "        directions.astype(np.float32),\n",
    "        lookback=lookback\n",
    "    )\n",
    "\n",
    "    # divido in train-validation\n",
    "    X_train, X_validation, y_train, y_validation, dates_train, dates_validation = split_stratified(\n",
    "        X_trv,\n",
    "        y_trv,\n",
    "        dates_trv,\n",
    "        test_size=0.2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # bilancio con oversampling della classe di minoranza (1)\n",
    "    X_train_bal, y_train_bal = oversample_mtl(X_train, y_train)  # bal = balanced\n",
    "    \n",
    "    # 2. creo il file dei risultati\n",
    "    if not os.path.exists(results_path):\n",
    "        print(f\"Creating new result file at {results_path}\")\n",
    "        results = get_new_results()\n",
    "    else:\n",
    "        print(f\"Loading result file at {results_path}\")\n",
    "        results = pd.read_csv(results_path)\n",
    "\n",
    "    # 3. creo le variabili che servono per il training (dati e parametri)\n",
    "    print(space)\n",
    "    try:\n",
    "        use_class_weight = space['use_class_weight']\n",
    "        if use_class_weight:\n",
    "            print(\"Using class weight for training\")\n",
    "    except KeyError:\n",
    "        use_class_weight = False\n",
    "        \n",
    "    \n",
    "    y_train_bal_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_train_bal]\n",
    "    y_validation_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_validation]\n",
    "    y_test_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_test]\n",
    "\n",
    "    # 4. inizializza le loss a 0 e crea i tempi di inizio e l'id esperimento\n",
    "    train_losses: List[float] = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    optimizer_name = space['optimizer']['name']\n",
    "    assert optimizer_name in {'adam', 'adadelta'}\n",
    "\n",
    "    start_time = int(round(time.time()))\n",
    "    experiment_id = str(uuid.uuid4())\n",
    "\n",
    "    # 5. addestra e testa il modello per n_runs volte\n",
    "    for i in range(n_runs):\n",
    "        model = create_model_mtl(space, lookback, n_features)\n",
    "\n",
    "        # 5.1 crea l'optimizer\n",
    "        if optimizer_name == 'adam':\n",
    "            learning_rate = space['optimizer']['lr']\n",
    "            optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "        elif optimizer_name == 'adadelta':\n",
    "            optimizer = 'adadelta'\n",
    "            learning_rate = 1.0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid optimizer name {optimizer_name}\")\n",
    "\n",
    "        if i == 0:\n",
    "            print(\n",
    "                f\"Hyper parameters: bptt={bptt}, optimizer={optimizer_name}, learning_rate={learning_rate}\"\n",
    "            )\n",
    "            # model.summary()\n",
    "\n",
    "        # 5.2 compila il modello\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "        )\n",
    "\n",
    "        # 5.3 parametri per l'Early Stopping\n",
    "        min_delta = float(space['early_stop']['min_delta'])\n",
    "        patience = int(space['early_stop']['patience'])\n",
    "\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=min_delta,\n",
    "            patience=patience,\n",
    "            restore_best_weights=True)\n",
    "\n",
    "        print(f\"Iteration {i}\")\n",
    "        print(\"Fitting model\")\n",
    "\n",
    "#         if use_class_weight:\n",
    "#             print(f\"Class weights: {class_weights}\")\n",
    "#             history: keras.callbacks.History = model.fit(\n",
    "#                 x=X_train,\n",
    "#                 y=y_t,\n",
    "#                 epochs=max_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(X_validation, y_v),\n",
    "#                 callbacks=[early_stop_cb],\n",
    "#                 shuffle=True,\n",
    "#                 verbose=verbose,\n",
    "#                 class_weight=class_weights)\n",
    "#         else:\n",
    "#             history: keras.callbacks.History = model.fit(  # type: ignore\n",
    "#                 x=X_train,\n",
    "#                 y=y_t,\n",
    "#                 epochs=max_epochs,\n",
    "#                 batch_size=batch_size,\n",
    "#                 validation_data=(X_validation, y_v),\n",
    "#                 callbacks=[early_stop_cb],\n",
    "#                 shuffle=True,\n",
    "#                 verbose=verbose)\n",
    "\n",
    "        # 5.4 addestra il modello\n",
    "        history: keras.callbacks.History = model.fit(  # type: ignore\n",
    "            x=X_train_bal,\n",
    "            y=y_train_bal_cat,\n",
    "            epochs=max_epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_validation, y_validation_cat),\n",
    "            callbacks=[early_stop_cb],\n",
    "            shuffle=True,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        # 5.5 valutalo su training e validation\n",
    "        train_epochs = len(history.history['loss'])\n",
    "\n",
    "        min_index = np.argmin(history.history['val_loss'])\n",
    "\n",
    "        curr_train_loss_total = history.history['loss'][min_index]\n",
    "        curr_train_loss_extreme = history.history['extreme_loss'][min_index]\n",
    "        curr_train_loss_up_down = history.history['up_down_loss'][min_index]\n",
    "        train_losses.append(curr_train_loss_total)\n",
    "\n",
    "        curr_validation_loss_total = history.history['val_loss'][min_index]\n",
    "        curr_validation_loss_extreme = history.history['val_extreme_loss'][\n",
    "            min_index]\n",
    "        curr_validation_loss_up_down = history.history['val_up_down_loss'][\n",
    "            min_index]\n",
    "\n",
    "        # 5.6 valutalo su test set\n",
    "        current_test_loss = model.evaluate(X_test, y_test_cat, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        curr_test_loss_total, curr_test_loss_extreme, curr_test_loss_up_down = current_test_loss\n",
    "        test_losses.append(curr_test_loss_total)\n",
    "\n",
    "        # 5.7 salva train, validation e test set performance\n",
    "        # train\n",
    "        print(\"Evaluating the model on the training set\")\n",
    "        probs = model.predict(X_train_bal, batch_size=batch_size)\n",
    "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
    "        \n",
    "        y_true_extreme, y_true_up_down = y_train_bal\n",
    "\n",
    "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
    "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
    "\n",
    "        row = pd.Series({\n",
    "            'dataset': 'train',\n",
    "            'optimizer': optimizer_name,\n",
    "            'start_time': start_time,\n",
    "            'experiment_id': experiment_id,\n",
    "            'trial': i,\n",
    "            'bptt': bptt,\n",
    "            'lr': learning_rate,\n",
    "            'n_layers': space['layers']['num_layers']['how_many'],\n",
    "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
    "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
    "            'dropout': space['layers']['input_dropout'],\n",
    "            'loss_extreme': curr_train_loss_extreme,\n",
    "            'loss_up_down': curr_train_loss_up_down,\n",
    "            'loss_volume': np.nan,\n",
    "            'f1_validation_extreme': np.nan,\n",
    "            'f1_validation_up_down': np.nan,\n",
    "            'tp_extreme': r_extreme['tp'],\n",
    "            'tn_extreme': r_extreme['tn'],\n",
    "            'fp_extreme': r_extreme['fp'],\n",
    "            'fn_extreme': r_extreme['fn'],\n",
    "            'tp_up_down': r_up_down['tp'],\n",
    "            'tn_up_down': r_up_down['tn'],\n",
    "            'fp_up_down': r_up_down['fp'],\n",
    "            'fn_up_down': r_up_down['fn'],\n",
    "            'train_epochs': train_epochs,\n",
    "            'es_min_delta': min_delta,\n",
    "            'es_patience': patience,\n",
    "            'use_class_weight': use_class_weight,\n",
    "            'output': sigmoid_or_softmax,\n",
    "        })\n",
    "        results = results.append(row, ignore_index=True)\n",
    "\n",
    "        # validation\n",
    "        print(\"Evaluating the model on the validation set\")\n",
    "        probs = model.predict(X_validation, batch_size=batch_size)\n",
    "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
    "        \n",
    "        y_true_extreme, y_true_up_down = y_validation\n",
    "        \n",
    "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
    "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
    "\n",
    "        # ora voglio ottimizzare rispetto all'F1 score della classe +1 -->\n",
    "        # utilizzo questo come metrica\n",
    "        f1_extreme = compute_performance(r_extreme['tp'], r_extreme['tn'],\n",
    "                                         r_extreme['fp'], r_extreme['fn'])\n",
    "        f1_up_down = compute_performance(r_up_down['tp'], r_up_down['tn'],\n",
    "                                         r_up_down['fp'], r_up_down['fn'])\n",
    "\n",
    "#         # se l'f1 è zero, allora l'esperimento è fallito\n",
    "#         if np.isnan(f1_extreme) or np.isnan(f1_up_down):\n",
    "#             return {\n",
    "#                 'status': STATUS_FAIL,\n",
    "#                 'loss': 1e+30,\n",
    "#             }\n",
    "\n",
    "        val_losses.append(curr_validation_loss_total)\n",
    "\n",
    "        row = pd.Series({\n",
    "            'dataset': 'validation',\n",
    "            'optimizer': optimizer_name,\n",
    "            'start_time': start_time,\n",
    "            'experiment_id': experiment_id,\n",
    "            'trial': i,\n",
    "            'bptt': bptt,\n",
    "            'lr': learning_rate,\n",
    "            'n_layers': space['layers']['num_layers']['how_many'],\n",
    "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
    "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
    "            'dropout': space['layers']['input_dropout'],\n",
    "            'loss_extreme': curr_validation_loss_extreme,\n",
    "            'loss_up_down': curr_validation_loss_up_down,\n",
    "            'loss_volume': np.nan,\n",
    "            'f1_validation_extreme': f1_extreme,\n",
    "            'f1_validation_up_down': f1_up_down,\n",
    "            'tp_extreme': r_extreme['tp'],\n",
    "            'tn_extreme': r_extreme['tn'],\n",
    "            'fp_extreme': r_extreme['fp'],\n",
    "            'fn_extreme': r_extreme['fn'],\n",
    "            'tp_up_down': r_up_down['tp'],\n",
    "            'tn_up_down': r_up_down['tn'],\n",
    "            'fp_up_down': r_up_down['fp'],\n",
    "            'fn_up_down': r_up_down['fn'],\n",
    "            'train_epochs': train_epochs,\n",
    "            'es_min_delta': min_delta,\n",
    "            'es_patience': patience,\n",
    "            'use_class_weight': use_class_weight,\n",
    "            'output': sigmoid_or_softmax,\n",
    "        })\n",
    "        results = results.append(row, ignore_index=True)\n",
    "\n",
    "        # testing\n",
    "        print(\"Evaluating the model on the test set\")\n",
    "        probs = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
    "        \n",
    "        y_true_extreme, y_true_up_down = y_test\n",
    "\n",
    "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
    "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
    "\n",
    "        row = pd.Series({\n",
    "            'dataset': 'test',\n",
    "            'optimizer': optimizer_name,\n",
    "            'start_time': start_time,\n",
    "            'experiment_id': experiment_id,\n",
    "            'trial': i,\n",
    "            'bptt': bptt,\n",
    "            'lr': learning_rate,\n",
    "            'n_layers': space['layers']['num_layers']['how_many'],\n",
    "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
    "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
    "            'dropout': space['layers']['input_dropout'],\n",
    "            'loss_extreme': curr_test_loss_extreme,\n",
    "            'loss_up_down': curr_test_loss_up_down,\n",
    "            'loss_volume': np.nan,\n",
    "            'f1_validation_extreme': np.nan,\n",
    "            'f1_validation_up_down': np.nan,\n",
    "            'tp_extreme': r_extreme['tp'],\n",
    "            'tn_extreme': r_extreme['tn'],\n",
    "            'fp_extreme': r_extreme['fp'],\n",
    "            'fn_extreme': r_extreme['fn'],\n",
    "            'tp_up_down': r_up_down['tp'],\n",
    "            'tn_up_down': r_up_down['tn'],\n",
    "            'fp_up_down': r_up_down['fp'],\n",
    "            'fn_up_down': r_up_down['fn'],\n",
    "            'train_epochs': train_epochs,\n",
    "            'es_min_delta': min_delta,\n",
    "            'es_patience': patience,\n",
    "            'use_class_weight': use_class_weight,\n",
    "            'output': sigmoid_or_softmax,\n",
    "        })\n",
    "        results = results.append(row, ignore_index=True)\n",
    "\n",
    "        # scrivo sul CSV\n",
    "        print(\"Scrivo i risultati\")\n",
    "        results.to_csv(results_path, index=None)\n",
    "\n",
    "        # cancello la memoria\n",
    "        keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    # 6. media delle loss\n",
    "    train_loss_mean = np.mean(train_losses)\n",
    "    train_loss_variance = np.var(train_losses)\n",
    "\n",
    "    val_loss_mean = np.mean(val_losses)\n",
    "    val_loss_variance = np.var(val_losses)\n",
    "\n",
    "    test_loss_mean = np.mean(test_losses)\n",
    "    test_loss_variance = np.var(test_losses)\n",
    "\n",
    "    # 7. return del dict di loss\n",
    "    return {\n",
    "        'status': STATUS_OK,\n",
    "        'loss': val_loss_mean,\n",
    "        'loss_variance': val_loss_variance,\n",
    "        'true_loss': test_loss_mean,\n",
    "        'true_loss_variance': test_loss_variance,\n",
    "        'train_loss_mean': train_loss_mean,\n",
    "        'train_loss_variance': train_loss_variance,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_tVJY9rOH66"
   },
   "source": [
    "### 3.1 Azione con minima volatilità - ottimizzazione iperparametri\n",
    "\n",
    "Cominciamo con l'azione meno volatile. Per prima cosa, bisogna trovare la struttura migliore della rete, quindi usiamo l'ottimizzazione degli iperparametri per farlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxKDWjIAqGXG"
   },
   "outputs": [],
   "source": [
    "s_type = 'min_vol'\n",
    "results_path = os.path.join(base_path, 'results', f\"results_{s_type}_no_sentiment.csv\")\n",
    "print(f\"RESULTS path: {results_path}\")\n",
    "\n",
    "def objective_fn(space, **kwargs):\n",
    "    return run_experiment(\n",
    "        space,\n",
    "        stock_type=s_type,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        data=data_all[s_type],\n",
    "        extremes=extremes_all[s_type]['95'],\n",
    "        directions=directions_all[s_type],\n",
    "        n_runs=5,\n",
    "        verbose=2,\n",
    "        results_path=results_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RDEDjiqOH69"
   },
   "source": [
    "Ora creo lo spazio di ricerca per questa azione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEMQYlzQOH6-"
   },
   "outputs": [],
   "source": [
    "optimizer_space = hp.choice('opt_name', [\n",
    "    {\n",
    "        'name': 'adam',\n",
    "        'lr': hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
    "    },\n",
    "   {\n",
    "       'name': 'adadelta',\n",
    "   },\n",
    "])\n",
    "\n",
    "layer_space = {\n",
    "    'num_layers': {\n",
    "        'how_many': 1,\n",
    "        'n_cells_1': 0, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "        'n_cells_2': 0,\n",
    "    },\n",
    "#     hp.choice('number_of_layers', [\n",
    "#         {\n",
    "#             'how_many': 1,\n",
    "#             'n_cells_1': 0, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "#             'n_cells_2': 0,\n",
    "#         },\n",
    "#        {\n",
    "#            'how_many': 2,\n",
    "#            'n_cells_1': hp.quniform('number_of_cells_1', low=10, high=100, q=2),\n",
    "#            'n_cells_2': hp.quniform('number_of_cells_2', low=10, high=50, q=2),\n",
    "#        }\n",
    "#     ]),\n",
    "    'input_dropout': hp.uniform('dropout_kill_rate', low=0.0, high=0.4),\n",
    "}\n",
    "\n",
    "early_stop_space = {\n",
    "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
    "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
    "}\n",
    "\n",
    "opt_space = {\n",
    "    'optimizer': optimizer_space,\n",
    "    'layers': layer_space,\n",
    "    'bptt': hp.quniform('bptt_len', low=10, high=120, q=1),\n",
    "    'early_stop': early_stop_space,\n",
    "    'use_class_weight': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3bOciYgOH7A"
   },
   "source": [
    "Ora facciamo la ricerca degli iperparametri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtmthHGGOH7B"
   },
   "outputs": [],
   "source": [
    "print(\"COMINCIAMO\\n\")\n",
    "n_trials = 50\n",
    "\n",
    "load_trials = True\n",
    "trials_filename = os.path.join(base_path, 'results', f\"trials_{s_type}_no_sentiment.pickle\")\n",
    "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_no_sentiment.pickle\")\n",
    "\n",
    "if load_trials:\n",
    "    print(\"Loading trials from last execution\")\n",
    "    try:\n",
    "        with open(trials_filename, 'rb') as infile:\n",
    "            trials = pickle.load(infile)\n",
    "            print(f\"len(trials) = {len(trials)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Trials file not found, starting with new trial object\")\n",
    "        trials = hy.Trials()\n",
    "else:\n",
    "    print(\"Starting new experiment with new trials object\")\n",
    "    trials = hy.Trials()\n",
    "\n",
    "\n",
    "best = fmin(\n",
    "    objective_fn,\n",
    "    opt_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=len(trials) + n_trials,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "with open(trials_filename, 'wb') as outfile:\n",
    "    pickle.dump(trials, outfile)\n",
    "\n",
    "with open(best_filename, 'wb') as outfile:\n",
    "    pickle.dump(best, outfile)\n",
    "\n",
    "print(\"FINITO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okVkBuNFqGXN"
   },
   "source": [
    "### 3.3 Azione massima volatilità - ottimizzazione iperparametri\n",
    "\n",
    "Ora, facciamo la stessa cosa per l'azione con più volatilità. Ricorda che qui stiamo solo cercando la configurazione ottimale della rete e degli iperparametri, mentre il vero training-testing lo farò in un altro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "98LZmDXv__8c",
    "outputId": "428f7ea6-eaf5-4eee-f012-7c6f5c820489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS path: /gdrive/My Drive/OptiRisk Thesis/experiments/11_final_experiment/results/results_max_vol_no_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "s_type = 'max_vol'\n",
    "results_path = os.path.join(base_path, 'results', f\"results_{s_type}_no_sentiment.csv\")\n",
    "print(f\"RESULTS path: {results_path}\")\n",
    "\n",
    "def objective_fn(space, **kwargs):\n",
    "    return run_experiment(\n",
    "        space,\n",
    "        stock_type=s_type,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        data=data_all[s_type],\n",
    "        extremes=extremes_all[s_type]['95'],\n",
    "        directions=directions_all[s_type],\n",
    "        n_runs=5,\n",
    "        verbose=0,\n",
    "        results_path=results_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyxjqnBnqGXT"
   },
   "outputs": [],
   "source": [
    "optimizer_space = hp.choice('opt_name', [\n",
    "    {\n",
    "        'name': 'adam',\n",
    "        'lr': hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
    "    },\n",
    "   {\n",
    "       'name': 'adadelta',\n",
    "   },\n",
    "])\n",
    "\n",
    "layer_space = {\n",
    "    'num_layers': {\n",
    "        'how_many': 1,\n",
    "        'n_cells_1': 0, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "        'n_cells_2': 0,\n",
    "    },\n",
    "#     hp.choice('number_of_layers', [\n",
    "#         {\n",
    "#             'how_many': 1,\n",
    "#             'n_cells_1': 0, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
    "#             'n_cells_2': 0,\n",
    "#         },\n",
    "#        {\n",
    "#            'how_many': 2,\n",
    "#            'n_cells_1': hp.quniform('number_of_cells_1', low=10, high=100, q=2),\n",
    "#            'n_cells_2': hp.quniform('number_of_cells_2', low=10, high=50, q=2),\n",
    "#        }\n",
    "#     ]),\n",
    "    'input_dropout': hp.uniform('dropout_kill_rate', low=0.0, high=0.4),\n",
    "}\n",
    "\n",
    "early_stop_space = {\n",
    "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
    "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
    "}\n",
    "\n",
    "opt_space = {\n",
    "    'optimizer': optimizer_space,\n",
    "    'layers': layer_space,\n",
    "    'bptt': hp.quniform('bptt_len', low=10, high=120, q=1),\n",
    "    'early_stop': early_stop_space,\n",
    "    'use_class_weight': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "ErGLxD7fAPRF",
    "outputId": "1849d9e3-e5e0-4227-93c6-e7f5c0383354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMINCIAMO\n",
      "\n",
      "Loading trials from last execution\n",
      "Trials file not found, starting with new trial object\n",
      "y[0] has 1954 elements of class 0.0\n",
      "y[0] has 101 elements of class 1.0\n",
      "y[1] has 1078 elements of class 0.0\n",
      "y[1] has 977 elements of class 1.0\n",
      "Loading result file at /gdrive/My Drive/OptiRisk Thesis/experiments/11_final_experiment/results/results_max_vol_no_sentiment.csv\n",
      "{'bptt': 93.0, 'early_stop': {'min_delta': 0.0064, 'patience': 21.0}, 'layers': {'input_dropout': 0.2935786779396727, 'num_layers': {'how_many': 1, 'n_cells_1': 0, 'n_cells_2': 0}}, 'optimizer': {'lr': 0.004329954105583538, 'name': 'adam'}, 'use_class_weight': False}\n",
      "Hyper parameters: bptt=93, optimizer=adam, learning_rate=0.004329954105583538\n",
      "Iteration 0\n",
      "Fitting model\n",
      "Evaluating the model on the training set\n",
      "Evaluating the model on the validation set\n",
      "Evaluating the model on the test set\n",
      "Scrivo i risultati\n",
      "Iteration 1\n",
      "Fitting model\n",
      "Evaluating the model on the training set\n",
      "Evaluating the model on the validation set\n",
      "Evaluating the model on the test set\n",
      "Scrivo i risultati\n",
      "Iteration 2\n",
      "Fitting model\n",
      "Evaluating the model on the training set\n",
      "Evaluating the model on the validation set\n",
      "Evaluating the model on the test set\n",
      "Scrivo i risultati\n",
      "Iteration 3\n",
      "Fitting model\n",
      "  0%|          | 0/50 [00:42<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "print(\"COMINCIAMO\\n\")\n",
    "n_trials = 50\n",
    "\n",
    "load_trials = True\n",
    "trials_filename = os.path.join(base_path, 'results', f\"trials_{s_type}_no_sentiment.pickle\")\n",
    "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_no_sentiment.pickle\")\n",
    "\n",
    "if load_trials:\n",
    "    print(\"Loading trials from last execution\")\n",
    "    try:\n",
    "        with open(trials_filename, 'rb') as infile:\n",
    "            trials = pickle.load(infile)\n",
    "            print(f\"len(trials) = {len(trials)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Trials file not found, starting with new trial object\")\n",
    "        trials = hy.Trials()\n",
    "else:\n",
    "    print(\"Starting new experiment with new trials object\")\n",
    "    trials = hy.Trials()\n",
    "\n",
    "\n",
    "best = fmin(\n",
    "    objective_fn,\n",
    "    opt_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=len(trials) + n_trials,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "with open(trials_filename, 'wb') as outfile:\n",
    "    pickle.dump(trials, outfile)\n",
    "\n",
    "with open(best_filename, 'wb') as outfile:\n",
    "    pickle.dump(best, outfile)\n",
    "\n",
    "print(\"FINITO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzTD8h5jAaL6"
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5gX8eBRZeU4"
   },
   "source": [
    "Ora vedo come variare il threshold per ottenere le curve ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5nXyquVVsRW"
   },
   "outputs": [],
   "source": [
    "def loss_function(theta, recall, fpr):\n",
    "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
    "    assert theta >= 0.0 and theta <= 1.0\n",
    "    \n",
    "    return theta * (1 - recall) + (1 - theta) * fpr\n",
    "\n",
    "\n",
    "def utility_function(theta, loss):\n",
    "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
    "    return min(theta, 1 - theta) - loss\n",
    "\n",
    "\n",
    "def to_binary(prob: np.ndarray, thresh: float):\n",
    "    assert thresh <= 1.0 and thresh >= 0.0\n",
    "    \n",
    "    return (prob >= thresh).astype(np.int8)\n",
    "\n",
    "\n",
    "def recall_fpr_kss_precision(y_true, y_pred):\n",
    "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
    "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "    tn = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    fp = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        y_pred\n",
    "    ))\n",
    "    fn = np.sum(np.logical_and(\n",
    "        y_true,\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    \n",
    "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
    "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    kss = recall - fpr\n",
    "    \n",
    "    return recall, fpr, kss, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0ylM6MWZjHg"
   },
   "outputs": [],
   "source": [
    "w_t = np.arange(0, 1, 1e-3)\n",
    "theta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBtZN0LrZ4_R"
   },
   "outputs": [],
   "source": [
    "def optimize_wt(w, theta, probabilities, y_true, verbose=False):\n",
    "    \"\"\"Get the best threshold for the class 1 probability.\"\"\"\n",
    "    recalls = np.zeros((w.shape[0], ), dtype=np.float64)\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    precisions = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    for i, thresh in enumerate(w):\n",
    "        if i % 200 == 0 and verbose:\n",
    "            print(f\"iteration {i} / {len(w_t)}\")\n",
    "\n",
    "        y_pred = to_binary(probabilities, thresh).astype(np.int8)\n",
    "        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
    "        loss = loss_function(theta, recall, fpr)\n",
    "        utility = utility_function(theta, loss)\n",
    "\n",
    "        recalls[i] = recall\n",
    "        precisions[i] = precision\n",
    "        ksss[i] = kss\n",
    "        fprs[i] = fpr\n",
    "        losses[i] = loss\n",
    "        utilities[i] = utility\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished!\")\n",
    "\n",
    "    return recalls, fprs, ksss, precisions, losses, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i6fKRXteWS0"
   },
   "outputs": [],
   "source": [
    "probabilities_extreme_train = model.predict(X_train, batch_size=batch_size)[0][:, 1]\n",
    "probabilities_up_down_train = model.predict(X_train, batch_size=batch_size)[1][:, 1]\n",
    "\n",
    "probabilities_extreme_validation = model.predict(X_validation, batch_size=batch_size)[0][:, 1]\n",
    "probabilities_up_down_validation = model.predict(X_validation, batch_size=batch_size)[1][:, 1]\n",
    "\n",
    "# per le ROC sugli estremi\n",
    "recalls_train, fprs_train, ksss_train, precisions_train, losses_train, utilities_train = \\\n",
    "optimize_wt(w_t, theta, probabilities_extreme_train, y_train[0].astype(np.int8))\n",
    "\n",
    "recalls_validation, fprs_validation, ksss_validation, \\\n",
    "precisions_validation, losses_validation, utilities_validation = \\\n",
    "optimize_wt(w_t, theta, probabilities_extreme_validation, y_validation[0].astype(np.int8))\n",
    "\n",
    "# per le ROC sul su-giù\n",
    "recalls_train_ud, fprs_train_ud, ksss_train_ud, precisions_train_ud, losses_train_ud, utilities_train_ud = \\\n",
    "optimize_wt(w_t, theta, probabilities_up_down_train, y_train[1].astype(np.int8))\n",
    "\n",
    "recalls_validation_ud, fprs_validation_ud, ksss_validation_ud, \\\n",
    "precisions_validation_ud, losses_validation_ud, utilities_validation_ud = \\\n",
    "optimize_wt(w_t, theta, probabilities_up_down_validation, y_validation[1].astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcQpqDonZ9BB"
   },
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "fig.suptitle(f\"{s_type} stock\", fontsize=16)\n",
    "\n",
    "# primo plot: EXTREMES\n",
    "# train set\n",
    "i_sorted = np.argsort(fprs_train)\n",
    "\n",
    "x_extreme = fprs_train[i_sorted]\n",
    "y_extreme = recalls_train[i_sorted]\n",
    "ax[0].plot(\n",
    "    x_extreme,\n",
    "    y_extreme,\n",
    "    color='navy',\n",
    "    label='train'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_train)\n",
    "best_x = fprs_train[i_sweet]\n",
    "best_y = recalls_train[i_sweet]\n",
    "\n",
    "ax[0].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='navy',\n",
    "    label='train - best'\n",
    ")\n",
    "\n",
    "# validation set\n",
    "i_sorted = np.argsort(fprs_validation)\n",
    "\n",
    "x = fprs_validation[i_sorted]\n",
    "y = recalls_validation[i_sorted]\n",
    "ax[0].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='forestgreen',\n",
    "    label='validation'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_validation)\n",
    "best_x = fprs_validation[i_sweet]\n",
    "best_y = recalls_validation[i_sweet]\n",
    "\n",
    "ax[0].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='forestgreen',\n",
    "    label='validation - best'\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Extreme prediction\")\n",
    "\n",
    "# ------------------------------------------------------- #\n",
    "# secondo plot: UP-DOWN\n",
    "# train set\n",
    "i_sorted = np.argsort(fprs_train_ud)\n",
    "\n",
    "x = fprs_train_ud[i_sorted]\n",
    "y = recalls_train_ud[i_sorted]\n",
    "ax[1].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='navy',\n",
    "    label='train'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_train)\n",
    "best_x = fprs_train_ud[i_sweet]\n",
    "best_y = recalls_train_ud[i_sweet]\n",
    "\n",
    "ax[1].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='navy',\n",
    "    label='train - best'\n",
    ")\n",
    "\n",
    "# validation set\n",
    "i_sorted = np.argsort(fprs_validation_ud)\n",
    "\n",
    "x = fprs_validation_ud[i_sorted]\n",
    "y = recalls_validation_ud[i_sorted]\n",
    "ax[1].plot(\n",
    "    x,\n",
    "    y,\n",
    "    color='forestgreen',\n",
    "    label='validation'\n",
    ")\n",
    "\n",
    "i_sweet = np.argmax(utilities_validation)\n",
    "best_x = fprs_validation_ud[i_sweet]\n",
    "best_y = recalls_validation_ud[i_sweet]\n",
    "\n",
    "ax[1].plot(\n",
    "    best_x,\n",
    "    best_y,\n",
    "    marker='s',\n",
    "    markersize=5,\n",
    "    color='forestgreen',\n",
    "    label='validation - best'\n",
    ")\n",
    "\n",
    "ax[1].set_title(\"Up-Down prediction\")\n",
    "\n",
    "# la linea del random classifier\n",
    "for a in ax:\n",
    "    a.plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "    a.legend(loc='lower right', fontsize=14)\n",
    "    a.set_xlim([0, 1.1])\n",
    "    a.set_ylim([0, 1.1])\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ANN_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
