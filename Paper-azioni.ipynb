{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi statistica intervalli di ricorrenza extreme returns\n",
    "\n",
    "Questo notebook contiene le analisi statistiche fatte per il paper.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
    "- [x] calcolo dei log returns\n",
    "- [x] divisione in training e testing set prima di calcolare la volatilità\n",
    "- [x] calcolo della volatilità per tutte le stocks, sul *training set*:\n",
    "    - [x] come deviazione standard del prezzo di chiusura\n",
    "    - [x] come $\\beta$ coefficient\n",
    "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in tutto il periodo considerato\n",
    "- [x] plot degli intervalli di ricorrenza per il Dow Jones, con metodi diversi:\n",
    "    - [x] quantile threshold al\n",
    "        - [x] 95%\n",
    "        - [x] 97.5%\n",
    "        - [x] 99%\n",
    "        - [x] verifica della relazione $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio, e confronto con l'evidenza dei dati\n",
    "    - [x] peak-over-threshold definito come $pot = \\mu \\pm m \\cdot \\sigma$: ricavare analiticamente m come $m = \\frac{q_x - \\mu}{\\sigma}$ dove $q_x$ è il quantile di ordine $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from typing import List\n",
    "import itertools\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.testing as pt\n",
    "import scipy.stats\n",
    "import scipy.special as sfun\n",
    "from scipy.stats import genextreme as gev\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "from statsmodels.tsa import stattools\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "\n",
    "import numba\n",
    "\n",
    "# import dello stimatore di Hill e del @timeit\n",
    "from my_timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_type = ['pos', 'neg', 'abs']\n",
    "quantile_type = ['95', '97.5', '99', 'evt']\n",
    "distribution_type = ['weibull', 's-exp', 'q-exp']\n",
    "stock_type = ['min_vol', 'max_vol']\n",
    "\n",
    "colors = {\n",
    "    'pos': 'seagreen',\n",
    "    'neg': 'darkred',\n",
    "    'abs': 'royalblue',\n",
    "}\n",
    "\n",
    "stock_colors = {\n",
    "    'min_vol': 'palegoldenrod',\n",
    "    'max_vol': 'coral',\n",
    "}\n",
    "\n",
    "legend_labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}\n",
    "\n",
    "dist_colors = {\n",
    "    'weibull': 'orchid',\n",
    "    's-exp': 'orangered',\n",
    "    'q-exp': 'mediumblue'\n",
    "}\n",
    "\n",
    "dist_labels = {\n",
    "    'weibull': r'Weibull',\n",
    "    's-exp': r's-exp',\n",
    "    'q-exp': r'q-exp',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importazione dati\n",
    "\n",
    "### 1.1 Caricamento dati e unione in un singolo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
    "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
    "index_path = os.path.join(data_path, 'prices', 'index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversione delle date e settaggio dell'index del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(prices_path)\n",
    "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
    "prices.index = prices['date']\n",
    "prices.drop(columns=['date'], inplace=True)\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_ts = prices.pivot(columns='ravenpackId', values=['close'])\n",
    "prices_ts.columns = prices_ts.columns.levels[1]\n",
    "prices_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora carico l'indice S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv(index_path)\n",
    "index.loc[:, 'date'] = pd.to_datetime(index['date'], format=\"%Y%m%d\")\n",
    "index.index = index['date']\n",
    "index.drop(columns=['date'], inplace=True)\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora carichiamo la corrispondenza codice - azienda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_code_path = os.path.join(data_path, 'ravenpackId_name_ticker_sector.csv')\n",
    "company_codes = pd.read_csv(company_code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora dichiariamo le date in cui bisogna dividere i dati, visto che ci sono state crisi nei mesi/anni successivi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dates = {\n",
    "    'insurance': datetime.datetime(1987, 1, 1), # insurance companies crisis\n",
    "    'dot-com': datetime.datetime(2000, 1, 1), # dot-com bubble explodes\n",
    "    'subprime-crisis': datetime.datetime(2007, 1, 1), # subprime crisis\n",
    "    'subprime-crisis-start': datetime.datetime(2007, 1, 1), # subprime crisis\n",
    "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 1),\n",
    "    'subprime-crisis-end': datetime.datetime(2010, 1, 1),\n",
    "    'eu-debt': datetime.datetime(2011, 1, 1), # EU sovereign debt crisis\n",
    "    'eu-debt-halfway': datetime.datetime(2012, 1, 1), # EU sovereign debt crisis\n",
    "    'last_train': datetime.datetime(2017, 1, 1), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funzione per dividere il dataset prima e dopo gli eventi critici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(data: pd.DataFrame, before_date: datetime.datetime):\n",
    "    \"\"\"Split the data before and after the before_date.\"\"\"\n",
    "    before = data[data.index < before_date]\n",
    "    after = data[data.index >= before_date]\n",
    "    \n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, se splitto sulla crisi finanziaria del 2007-2008 c'è il problema dei percentili, quindi utilizzo questa strategia:\n",
    "\n",
    "- per il training set seleziono tutti i giorni compresi tra il 2005-01-03 (inizio dataset) e il 2008-09-01, cioè circa a metà della crisi, e dal 2012-01-01 al 2017-01-01\n",
    "- il restante per il test set, cioè dal 2008-09-01 al 2012-01-01 e dal 2017-01-01 in avanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calcolo log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
    "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
    "print(log_returns.shape)\n",
    "log_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_1 = log_returns[log_returns.index < split_dates['subprime-crisis-halfway']]\n",
    "lr_train_2 = log_returns[(log_returns.index >= split_dates['eu-debt-halfway']) & (log_returns.index < split_dates['last_train'])]\n",
    "\n",
    "lr_train = pd.concat([lr_train_1, lr_train_2], axis='index')\n",
    "\n",
    "lr_test_1 = log_returns[(log_returns.index >= split_dates['subprime-crisis-halfway']) & \\\n",
    "                        (log_returns.index < split_dates['eu-debt-halfway'])]\n",
    "lr_test_2 = log_returns[log_returns.index >= split_dates['last_train']]\n",
    "\n",
    "lr_test = pd.concat([lr_test_1, lr_test_2], axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcolo volatilità\n",
    "\n",
    "Calcoliamo la volatilità in due modi:\n",
    "\n",
    "- [x] con la standard deviation dei log-returns\n",
    "- [x] con il beta coefficient rispetto all'indice S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Volatilità con std. dev dei log-returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility = lr_train.std()\n",
    "volatility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max_vol = volatility.idxmax()\n",
    "idx_min_vol = volatility.idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora capiamo a che stock appartengono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_max_vol = company_codes[company_codes.ravenpackId == idx_max_vol]\n",
    "company_min_vol = company_codes[company_codes.ravenpackId == idx_min_vol]\n",
    "\n",
    "print(f\"Company with max volatility is \\n{company_max_vol}\\n\")\n",
    "print(f\"Company with min volatility is \\n{company_min_vol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(18, 10))\n",
    "\n",
    "# max volatility\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_train_1.index, idx_max_vol], color='coral', label='train')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_train_2.index, idx_max_vol], color='coral', label='__None__')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_test_1.index, idx_max_vol], color='burlywood', label='test')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_test_2.index, idx_max_vol], color='burlywood', label='__None__')\n",
    "ax[0, 0].set_title(f'Most volatile stock | {company_max_vol.iloc[0, 1]}')\n",
    "\n",
    "ax[1, 0].plot(log_returns.loc[lr_train_1.index, idx_max_vol], color='coral', label='train')\n",
    "ax[1, 0].plot(log_returns.loc[lr_train_2.index, idx_max_vol], color='coral', label='__None__')\n",
    "ax[1, 0].plot(log_returns.loc[lr_test_1.index, idx_max_vol], color='burlywood', label='test')\n",
    "ax[1, 0].plot(log_returns.loc[lr_test_2.index, idx_max_vol], color='burlywood', label='__None__')\n",
    "\n",
    "# min volatility\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_train_1.index, idx_min_vol], color='gold', label='train')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_train_2.index, idx_min_vol], color='gold', label='__None__')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_test_1.index, idx_min_vol], color='darkkhaki', label='test')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_test_2.index, idx_min_vol], color='darkkhaki', label='__None__')\n",
    "ax[0, 1].set_title(f'Least volatile stock | {company_min_vol.iloc[0, 1]}')\n",
    "\n",
    "ax[1, 1].plot(log_returns.loc[lr_train_1.index, idx_min_vol], color='gold', label='train')\n",
    "ax[1, 1].plot(log_returns.loc[lr_train_2.index, idx_min_vol], color='gold', label='__None__')\n",
    "ax[1, 1].plot(log_returns.loc[lr_test_1.index, idx_min_vol], color='darkkhaki', label='test')\n",
    "ax[1, 1].plot(log_returns.loc[lr_test_2.index, idx_min_vol], color='darkkhaki', label='__None__')\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    for j in range(ax.shape[1]):\n",
    "        ax[i, j].legend()\n",
    "        ax[i, j].set_xlabel('Year')\n",
    "        if i == 0:\n",
    "            ax[i, j].set_ylabel('Price in $')\n",
    "        else:\n",
    "            ax[i, j].set_ylabel('Log-return in $')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Volatilità con beta coefficient\n",
    "\n",
    "Il beta coefficient misura quanto un'asset è più o meno volatile rispetto a un benchmark di riferimento. In questo caso, è l'indice stesso S&P500, e si calcola così:\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{Cov(r_a, r_b)}{Var(r_b)}\n",
    "$$\n",
    "\n",
    "dove $r_a$ sono i log-returns dell'asset, $r_b$ quelli del benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_log_returns = np.log(index.loc[:, ['close']]).diff(periods=1).iloc[1:, :]\n",
    "index_log_returns.rename(columns={'close': 'index_SP500'}, inplace=True)\n",
    "pt.assert_index_equal(index_log_returns.index, log_returns.index, check_names=False)\n",
    "\n",
    "index_lr_train_1 = index_log_returns[index_log_returns.index < split_dates['subprime-crisis-halfway']]\n",
    "index_lr_train_2 = index_log_returns[(index_log_returns.index >= split_dates['eu-debt-halfway']) & \\\n",
    "                                     (index_log_returns.index < split_dates['last_train'])]\n",
    "\n",
    "index_lr_train = pd.concat([index_lr_train_1, index_lr_train_2], axis='index')\n",
    "\n",
    "index_lr_test_1 = index_log_returns[(index_log_returns.index >= split_dates['subprime-crisis-halfway']) & \\\n",
    "                        (index_log_returns.index < split_dates['eu-debt-halfway'])]\n",
    "index_lr_test_2 = index_log_returns[index_log_returns.index >= split_dates['last_train']]\n",
    "\n",
    "index_lr_test = pd.concat([index_lr_test_1, index_lr_test_2], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_together = pd.concat([lr_train, index_lr_train], axis='columns')\n",
    "test_all_together = pd.concat([lr_test, index_lr_test], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = train_all_together.cov()\n",
    "index_variance = train_all_together.loc[:, 'index_SP500'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = pd.DataFrame(\n",
    "    data=[\n",
    "        covariance.loc[company, 'index_SP500'] / index_variance\n",
    "        for company in train_all_together.columns[:-1]\n",
    "    ],\n",
    "    index=train_all_together.columns[:-1],\n",
    "    columns=['beta']\n",
    ")\n",
    "\n",
    "idx_max_beta = beta.idxmax()\n",
    "idx_min_beta = beta.idxmin()\n",
    "\n",
    "company_max_beta = company_codes[company_codes.ravenpackId == idx_max_beta.iloc[0]]\n",
    "company_min_beta = company_codes[company_codes.ravenpackId == idx_min_beta.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Company with highest beta is \\n{company_max_beta}\\nwith beta {beta.loc[idx_max_beta, 'beta'][0]:5.3f}\\n\")\n",
    "print(f\"Company with lowest beta is \\n{company_min_beta}\\nwith beta {beta.loc[idx_min_beta, 'beta'][0]:5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(18, 10))\n",
    "\n",
    "# max beta\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_train_1.index, idx_max_beta], color='coral', label='train')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_train_2.index, idx_max_beta], color='coral', label='__None__')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_test_1.index, idx_max_beta], color='burlywood', label='test')\n",
    "ax[0, 0].plot(prices_ts_no_nan.loc[lr_test_2.index, idx_max_beta], color='burlywood', label='__None__')\n",
    "ax[0, 0].set_title(f'Most volatile stock | {company_max_beta.iloc[0, 1]}')\n",
    "\n",
    "ax[1, 0].plot(log_returns.loc[lr_train_1.index, idx_max_beta], color='coral', label='train')\n",
    "ax[1, 0].plot(log_returns.loc[lr_train_2.index, idx_max_beta], color='coral', label='__None__')\n",
    "ax[1, 0].plot(log_returns.loc[lr_test_1.index, idx_max_beta], color='burlywood', label='test')\n",
    "ax[1, 0].plot(log_returns.loc[lr_test_2.index, idx_max_beta], color='burlywood', label='__None__')\n",
    "\n",
    "# min beta\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_train_1.index, idx_min_beta], color='gold', label='train')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_train_2.index, idx_min_beta], color='gold', label='__None__')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_test_1.index, idx_min_beta], color='darkkhaki', label='test')\n",
    "ax[0, 1].plot(prices_ts_no_nan.loc[lr_test_2.index, idx_min_beta], color='darkkhaki', label='__None__')\n",
    "ax[0, 1].set_title(f'Least volatile stock | {company_min_beta.iloc[0, 1]}')\n",
    "\n",
    "ax[1, 1].plot(log_returns.loc[lr_train_1.index, idx_min_beta], color='gold', label='train')\n",
    "ax[1, 1].plot(log_returns.loc[lr_train_2.index, idx_min_beta], color='gold', label='__None__')\n",
    "ax[1, 1].plot(log_returns.loc[lr_test_1.index, idx_min_beta], color='darkkhaki', label='test')\n",
    "ax[1, 1].plot(log_returns.loc[lr_test_2.index, idx_min_beta], color='darkkhaki', label='__None__')\n",
    "\n",
    "for i in range(ax.shape[0]):\n",
    "    for j in range(ax.shape[1]):\n",
    "        ax[i, j].legend()\n",
    "        ax[i, j].set_xlabel('Year')\n",
    "        if i == 0:\n",
    "            ax[i, j].set_ylabel('Price in $')\n",
    "        else:\n",
    "            ax[i, j].set_ylabel('Log-return in $')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, vediamo qual è la differenza tra l'azione più stabile identificata con la volatilità (Johnson & Johnson) e quella identificata con il $\\beta$ (SLM Corp.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'std': [\n",
    "        volatility[idx_min_vol], # std J&J\n",
    "        volatility[idx_min_beta[0]] # std SLM Corp\n",
    "    ],\n",
    "    'beta': [\n",
    "        beta.loc[idx_min_vol, 'beta'], # beta J&J\n",
    "        beta.loc[idx_min_beta[0], 'beta'] # beta SLM Corp\n",
    "    ]\n",
    "}\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=data,\n",
    "    index=['Johnson & Johnson', 'SLM Corp.']\n",
    ")  # devo guardare i minimi eh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vede che a livello di *std*, Johnson & Johnson è quasi la metà di SLM corp, mentre a livello di $\\beta$ SLM Corp è il 50% in meno circa.\n",
    "\n",
    "Dunque scegliamo come azioni:\n",
    "\n",
    "- più volatile (beta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_codes[company_codes.ravenpackId == idx_max_beta[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- meno volatile (beta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_codes[company_codes.ravenpackId == idx_min_beta[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo quindi i dati di training e testing per queste due azioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_key = 'eu-debt'\n",
    "\n",
    "# divisione dataset in training (in-sample) e testing (out-of-sample), qui prima e dopo la crisi finanziaria 2007-2008\n",
    "company_min_beta = company_codes[company_codes.ravenpackId == idx_min_beta['beta']]['ravenpackId'].iloc[0]\n",
    "company_max_beta = company_codes[company_codes.ravenpackId == idx_max_beta['beta']]['ravenpackId'].iloc[0]\n",
    "\n",
    "lr_train_max_vol = lr_train[company_max_beta]\n",
    "lr_test_max_vol = lr_test[company_max_beta]\n",
    "\n",
    "lr_train_min_vol = lr_train[company_min_beta]\n",
    "lr_test_min_vol = lr_test[company_min_beta]\n",
    "\n",
    "# returns contiene il training set\n",
    "returns = {\n",
    "    'max_vol': {\n",
    "        'pos': lr_train_max_vol[lr_train_max_vol > 0.0],\n",
    "        'neg': lr_train_max_vol[lr_train_max_vol < 0.0],\n",
    "        'abs': lr_train_max_vol.abs()\n",
    "    },\n",
    "    'min_vol': {\n",
    "        'pos': lr_train_min_vol[lr_train_min_vol > 0.0],\n",
    "        'neg': lr_train_min_vol[lr_train_min_vol < 0.0],\n",
    "        'abs': lr_train_min_vol.abs()\n",
    "    }\n",
    "}\n",
    "\n",
    "returns_test = {\n",
    "    'max_vol': {\n",
    "        'pos': lr_test_max_vol[lr_test_max_vol > 0.0],\n",
    "        'neg': lr_test_max_vol[lr_test_max_vol < 0.0],\n",
    "        'abs': lr_test_max_vol.abs(),\n",
    "    },\n",
    "    'min_vol': {\n",
    "        'pos': lr_test_min_vol[lr_test_min_vol > 0.0],\n",
    "        'neg': lr_test_min_vol[lr_test_min_vol < 0.0],\n",
    "        'abs': lr_test_min_vol.abs(),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stima dei parametri della distribuzione GEV\n",
    "\n",
    "In questa sezione si replica la sezione 4.1 del paper.\n",
    "\n",
    "Secondo la [Extreme Value Theory](https://en.wikipedia.org/wiki/Extreme_value_theory), trovare gli estremi significa trovare un gruppo di dati $x \\geq x_t$ dove $x_t$ è l'*extreme value threshold*, e che soddisfi la GEV ([Generalized Extreme Values Distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution)) che ha distribuzione cumulativa:\n",
    "\n",
    "\\begin{align*}\n",
    "    G(x) &= exp\\left[- \\left(1 + \\xi \\frac{x - \\mu}{\\sigma}\\right)^{-1/\\xi}\\right] \\; for \\; \\xi \\neq 0\\\\ \n",
    "    G(x) &= exp\\left[-exp\\left(\\frac{x - \\mu}{\\sigma}\\right)\\right] \\; for \\; \\xi = 0\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "dove $\\xi$ è lo *shape parameter* che determina la forma della coda, $1/\\xi$ è il *tail exponent* della distribuzione.\n",
    "\n",
    "Per trovare il threshold $x_t$, usiamo questo metodo:\n",
    "\n",
    "1. sort dei dati (tutti i log-returns) in ordine discendente (o non-ascendente) per avere la sequenza $x_1 \\geq x_2 \\geq \\ldots \\geq x_n$\n",
    "2. applicare lo [stimatore di Hill](https://en.wikipedia.org/wiki/Heavy-tailed_distribution#Hill's_tail-index_estimator) dove $n$ è il numero di samples, $k$ l'indice del k-esimo dato più grande (posizione k nella sequenza ordinata) chiamato *k-th order statistic*\n",
    "\n",
    "$$\\frac{1}{\\hat{\\xi}_{k,n}} = \\frac{1}{\\hat{\\gamma}} = \\frac{1}{k}\\sum_{i=1}^{k}log\\left(\\frac{x_i}{x_{k+1}}\\right)$$\n",
    "\n",
    "3. calcolare la statistica di [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic) per quantificare il fitting tra la distribuzione GEV così ricavata (con $\\hat{\\xi}_{k,n}$ come esponente) e quella empirica della coda, dove la coda è rappresentata dai returns che sono $x \\geq x_t$, cioé quelli ordinati discendenti con indice $i < k$\n",
    "4. scegliere $k$ (e di conseguenza $x_t$ che non è altro che il k-esimo elemento dei return ordinati discendenti) come il valore associato alla *minima* statistica di Kolmogorov-Smirnov\n",
    "\n",
    "Questo flusso viene applicato a tutti e 3 i tipi di returns considerati: positivi, negativi e assoluti.\n",
    "\n",
    "Inoltre, usiamo una seconda versione di questo flusso, che è:\n",
    "\n",
    "1. sort dei dati (tutti i log-returns) in ordine discendente (o non-ascendente) per avere la sequenza $x_1 \\geq x_2 \\geq \\ldots \\geq x_n$\n",
    "2. scorrere nei returns ordinati con un indice $k$ che identifica il threshold scelto e fitting di una GEV sui *tail data*, dove per tail data si intendono tutti i returns con indice $i < k$\n",
    "3. calcolare la statistica di [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic) per quantificare il fitting tra la distribuzione GEV e i dati\n",
    "4. scegliere $k$ (e di conseguenza $x_t$ che non è altro che il k-esimo elemento dei return ordinati discendenti) come il valore associato alla *minima* statistica di Kolmogorov-Smirnov\n",
    "\n",
    "Ciò che cambia tra i due flussi è che nel primo, calcoliamo *separatamente* $\\gamma$ e gli altri parametri $\\mu$ e $\\sigma$, mentre nel secondo caso tutti insieme con una maximum-likelihood estimation.\n",
    "\n",
    "Calcolo i return ordinati dal più grande al più piccolo per i positivi, negativi e assoluti per prima cosa e poi procedo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sorting dei returns - va fatto sia per il positivo, che per il negativo, che per gli assoluti\n",
    "sorted_positive_lr_max_vol = returns['max_vol']['pos'].sort_values(ascending=False)\n",
    "sorted_negative_lr_max_vol = (-returns['max_vol']['neg']).sort_values(ascending=False)\n",
    "sorted_absolute_lr_max_vol = returns['max_vol']['abs'].sort_values(ascending=False)\n",
    "\n",
    "sorted_positive_lr_min_vol = returns['min_vol']['pos'].sort_values(ascending=False)\n",
    "sorted_negative_lr_min_vol = (-returns['min_vol']['neg']).sort_values(ascending=False)\n",
    "sorted_absolute_lr_min_vol = returns['min_vol']['abs'].sort_values(ascending=False)\n",
    "\n",
    "eps = 5e-6\n",
    "\n",
    "sorted_lr = {\n",
    "    'max_vol': {\n",
    "        'pos': sorted_positive_lr_max_vol[sorted_positive_lr_max_vol >= eps],\n",
    "        'neg': sorted_negative_lr_max_vol[sorted_negative_lr_max_vol >= eps],\n",
    "        'abs': sorted_absolute_lr_max_vol[sorted_absolute_lr_max_vol >= eps],\n",
    "    },\n",
    "    'min_vol': {\n",
    "        'pos': sorted_positive_lr_min_vol[sorted_positive_lr_min_vol >= eps],\n",
    "        'neg': sorted_negative_lr_min_vol[sorted_negative_lr_min_vol >= eps],\n",
    "        'abs': sorted_absolute_lr_min_vol[sorted_absolute_lr_min_vol >= eps],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stima di $\\gamma$ separata\n",
    "\n",
    "Cominciamo stimando $\\gamma$.\n",
    "\n",
    "Posso creare la mia funzione per stimare $\\gamma$ esattamente come nel paper, cioè:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{k} \\sum_{i = 1}{k} \\left[ \\ln x_{n + 1 - k} - \\ln x_k \\right]\n",
    "$$\n",
    "\n",
    "che però richiede i returns ordinati **ascendenti**.\n",
    "\n",
    "Invece, uso la formula nell'altro paper [Tail index estimation, concentration and adaptivity](http://arxiv.org/abs/1503.05077), cioé\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}(k) = \\frac{1}{k} \\sum_{i = 1}^{k} \\ln \\left( \\frac{x_i}{x_{k + 1}} \\right)\n",
    "$$\n",
    "\n",
    "che richiede i returns ordinati **discendenti** come già sono, quindi meglio (ed è anche più facile da capire ed interpretare).\n",
    "\n",
    "#### 3.1.1 Calcolo di $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hill_estimator(lr: np.ndarray):\n",
    "    \"\"\"Get the estimation of gamma.\"\"\"\n",
    "    assert np.all(np.diff(lr) <= 0.0)  # ordinati discendenti, lr[i] - lr[i - 1] <= 0\n",
    "    \n",
    "    k_max = lr.shape[0]\n",
    "    kappas = np.arange(1, k_max)\n",
    "    \n",
    "    gammas = np.zeros((kappas.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for index, k in enumerate(kappas):  # index = k - 1\n",
    "        ssum = np.sum(np.log(lr[:k - 1] / lr[k - 1 + 1]))        \n",
    "        gammas[index] = (1 / k) * ssum\n",
    "        \n",
    "    return {\n",
    "        'xis': gammas,\n",
    "        'kappas': kappas\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora, calcoliamo $\\xi = \\gamma = -c$ con la mia funzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hill_estimation = {\n",
    "    s_type: {\n",
    "        ret_type: my_hill_estimator(sorted_lr[s_type][ret_type].values)\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Plot di $\\gamma$ al variare di k e dei returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot di $\\gamma$ al variare di $k$, cioè del threshold, e plot di $\\gamma$ al variare dei return, per vedere come cambia a seconda di quale return si prenda come threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot delle stime di xi e gamma al variare di k\n",
    "truncation = 100\n",
    "\n",
    "# sulle righe in funzione di k o dei returns\n",
    "# sulle colonne le due azioni, prima colonna la min_vol e seconda la max_vol\n",
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(18, 14))\n",
    "\n",
    "for i in range(2):  # sulle righe il tipo, gamma(k) o gamma(lr)\n",
    "    \n",
    "    for j, s_type in enumerate(stock_type):  # lavoriamo per colonne (azioni)\n",
    "        for ret_type in return_type:  # tutti i tipi di return\n",
    "            est = hill_estimation[s_type][ret_type]\n",
    "            x = est['kappas'][truncation:] if i == 0 else sorted_lr[s_type][ret_type].values[1:][::-1][truncation:]\n",
    "            y = est['xis'][truncation:]\n",
    "            \n",
    "            if i == 0:\n",
    "                ax[i, j].plot(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "                ax[i, j].set(\n",
    "                    title=r'{} - $\\gamma$ as function of $k$'.format(s_type),\n",
    "                    xlabel=r'$k$',\n",
    "                    ylabel=r'$\\gamma(k)$'\n",
    "                )\n",
    "            else:\n",
    "                ax[i, j].semilogx(x, y, color=colors[ret_type], label=legend_labels[ret_type])\n",
    "                ax[i, j].set(\n",
    "                    title=r'{} - $\\gamma$ as function of $r$'.format(s_type),\n",
    "                    xlabel=r'$r$, $-r$, $|r|$',\n",
    "                    ylabel=r'$\\gamma(r)$'\n",
    "                )\n",
    "            \n",
    "            ax[i, j].set_ylim([0, 5])\n",
    "            ax[i, j].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Fitting della GEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora biogna passare a fittare la GEV e calcolare la statistica KS per ogni valore di $k$ (e quindi del return che funge da threshold, $x_t$).\n",
    "\n",
    "Andremo poi a scegliere il valore di $k$ che minimizza la KS.\n",
    "\n",
    "Creiamo allora una funzione apposita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "#@numba.jit(nopython=False, parallel=True, nogil=True)\n",
    "def fit_gev(lr, xi, k, size=None, n_tries=1):\n",
    "    \"\"\"Find the best fitting GEV to the tail distribution of data in lr.\n",
    "    lr MUST BE in descending order\n",
    "    \"\"\"\n",
    "#     assert isinstance(xi, np.ndarray)\n",
    "#     assert isinstance(k, np.ndarray)\n",
    "#     assert xi.shape == k.shape\n",
    "    \n",
    "    # check descending\n",
    "#     assert np.all(np.diff(lr) <= 0.0)\n",
    "    n_xi = xi.shape[0]\n",
    "    \n",
    "    ks = np.zeros((n_xi, ))\n",
    "    pvals = np.zeros((n_xi, ))\n",
    "    fits = []\n",
    "    \n",
    "    print(\"Start fitting\")\n",
    "#     for i in numba.prange(n_xi):\n",
    "    for i in range(n_xi):\n",
    "        current_k = k[i]\n",
    "        current_xi = xi[i]\n",
    "        \n",
    "        threshold = lr[current_k]\n",
    "        tail_data = lr[:current_k]\n",
    "        \n",
    "        if size:\n",
    "            ss = size\n",
    "        else:\n",
    "            ss = current_k\n",
    "        \n",
    "        fit = gev.fit(tail_data, fix_c=-current_xi)  # convenzioni diverse in SciPy\n",
    "        fits.append(fit)\n",
    "        \n",
    "        k_stat_temp = np.zeros((n_tries, ))\n",
    "        pval_temp = np.zeros((n_tries, ))\n",
    "        \n",
    "        c = fit[0]\n",
    "        loc = fit[1]\n",
    "        scale = fit[2]\n",
    "        \n",
    "        for j in range(n_tries):\n",
    "            rvs = gev.rvs(c, loc, scale, ss)\n",
    "\n",
    "            kk, pv = scipy.stats.ks_2samp(tail_data, rvs)\n",
    "            k_stat_temp[j] = kk\n",
    "            pval_temp[j] = pv\n",
    "        \n",
    "        ks[i] = np.mean(k_stat_temp)\n",
    "        pvals[i] = np.mean(pval_temp)\n",
    "        \n",
    "    print(\"Fitting done\")\n",
    "    \n",
    "    return {\n",
    "        'ks': ks,\n",
    "        'p': pvals,\n",
    "        'fits': fits,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ed applichiamola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "npz_filename = f\"./sp500/ks_stat-pvals_manual_split.npz\"\n",
    "pickle_filename = f\"./sp500/fits_manual_split.pickle\"\n",
    "\n",
    "if load:  # just load the already computed KS and p-values\n",
    "    loaded = np.load(npz_filename)\n",
    "    \n",
    "    with open(pickle_filename, 'rb') as infile:\n",
    "        fits = pickle.load(infile)\n",
    "    \n",
    "    kolmog_smirn = {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                'ks': loaded[s_type + '_ks_' + ret_type],\n",
    "                'p': loaded[s_type + '_pvals_' + ret_type],\n",
    "                'fits': fits[s_type][ret_type],\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    \n",
    "    \n",
    "else:  # compute them and save them\n",
    "    size = 9999\n",
    "    n_tries = 10\n",
    "    print(\"\\nFitting returns\")\n",
    "\n",
    "    kolmog_smirn = {\n",
    "        s_type: {\n",
    "            ret_type: fit_gev(\n",
    "                sorted_lr[s_type][ret_type].values,\n",
    "                hill_estimation[s_type][ret_type]['xis'],\n",
    "                hill_estimation[s_type][ret_type]['kappas'],\n",
    "                size=size,\n",
    "                n_tries=n_tries,\n",
    "            )\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    \n",
    "    to_save = dict()\n",
    "    for s_type in stock_type:\n",
    "        for ret_type in return_type:\n",
    "            to_save[s_type + '_ks_' + ret_type] = kolmog_smirn[s_type][ret_type]['ks']\n",
    "            to_save[s_type + '_pvals_' + ret_type] = kolmog_smirn[s_type][ret_type]['p']\n",
    "    \n",
    "    np.savez_compressed(npz_filename, **to_save)\n",
    "    \n",
    "    fits = {\n",
    "        s_type: {\n",
    "            ret_type: kolmog_smirn[s_type][ret_type]['fits']\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    \n",
    "    with open(pickle_filename, 'wb') as outfile:\n",
    "        pickle.dump(fits, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pval = 0.05\n",
    "\n",
    "valid_pvals = {\n",
    "    s_type: {\n",
    "        ret_type: kolmog_smirn[s_type][ret_type]['p'] <= min_pval\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "# trovo gli indici minimi\n",
    "i_min_ks = dict()\n",
    "\n",
    "for s_type in stock_type:\n",
    "    i_min_ks[s_type] = dict()\n",
    "    for ret_type in return_type:\n",
    "        # indici booleani di validità del p-value\n",
    "        mask = valid_pvals[s_type][ret_type]\n",
    "\n",
    "        # copio la KS per quel return\n",
    "        y = copy.deepcopy(kolmog_smirn[s_type][ret_type]['ks'])\n",
    "\n",
    "        # dove il p-value > 0.05, setto la KS al massimo così non viene considerata\n",
    "        y[np.logical_not(mask)] = np.max(y)\n",
    "\n",
    "        # trovo gli indici di minima distanza KS per questo return\n",
    "        i_min_ks[s_type][ret_type] = np.argmin(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le figure seguenti mostrano l'andamento di $d_{KS}$ in funzione di k e dei returns ordinati, su scala $x$ semilogaritmica, per entrambi i modi di calcolare il fitting della GEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot della statistica KS al variare del sorted return e di k\n",
    "truncation = 100\n",
    "end = -170\n",
    "p_labels = {\n",
    "    'pos': r'valid $p$ for $r$',\n",
    "    'neg': r'valid $p$ for $-r$',\n",
    "    'abs': r'valid $p$ for $|r|$',\n",
    "}\n",
    "\n",
    "p_height = {\n",
    "    'pos': 0.0,\n",
    "    'neg': -0.01,\n",
    "    'abs': -0.02\n",
    "}\n",
    "\n",
    "markersize = 0.7\n",
    "p_markersize = 0.7\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=len(stock_type), figsize=(22, 14))\n",
    "\n",
    "for i in range(ax.shape[0]):  # sulle righe d_KS(r) e d_KS(k)\n",
    "    for j, s_type in enumerate(stock_type):\n",
    "        for ret_type in return_type:\n",
    "            x = sorted_lr[s_type][ret_type].values[1:] if i == 0 else hill_estimation[s_type][ret_type]['kappas']\n",
    "            y = kolmog_smirn[s_type][ret_type]['ks']\n",
    "\n",
    "            mask = valid_pvals[s_type][ret_type]\n",
    "\n",
    "            x_ok = x[mask]\n",
    "            y_ok = y[mask]\n",
    "\n",
    "            if i == 0:\n",
    "                ax[i, j].semilogx(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=colors[ret_type],\n",
    "                    linestyle='',\n",
    "                    marker='.',\n",
    "                    markersize=markersize,\n",
    "                    label=legend_labels[ret_type]\n",
    "                )\n",
    "\n",
    "                ax[i, j].semilogx(\n",
    "                    x_ok,\n",
    "                    p_height[ret_type] * np.ones((len(x_ok, ))),\n",
    "                    color=colors[ret_type],\n",
    "                    linestyle='',\n",
    "                    marker='.',\n",
    "                    markersize=p_markersize\n",
    "                )\n",
    "\n",
    "                ax[i, j].axvline(\n",
    "                    x[i_min_ks[s_type][ret_type]],\n",
    "                    linestyle='-.',\n",
    "                    color=colors[ret_type],\n",
    "                    alpha=0.7,\n",
    "                    label=r'min $d_{ks}$ for ' + legend_labels[ret_type]\n",
    "                )\n",
    "            else:\n",
    "                ax[i, j].plot(\n",
    "                    x,\n",
    "                    y,\n",
    "                    color=colors[ret_type],\n",
    "                    linestyle='',\n",
    "                    marker='.',\n",
    "                    markersize=markersize,\n",
    "                    label=legend_labels[ret_type]\n",
    "                )\n",
    "\n",
    "                ax[i, j].plot(\n",
    "                    x_ok,\n",
    "                    p_height[ret_type] * np.ones((len(x_ok, ))),\n",
    "                    color=colors[ret_type],\n",
    "                    linestyle='',\n",
    "                    marker='.',\n",
    "                    markersize=p_markersize\n",
    "                )\n",
    "\n",
    "                ax[i, j].axvline(\n",
    "                    x[i_min_ks[s_type][ret_type]],\n",
    "                    linestyle='-.',\n",
    "                    color=colors[ret_type],\n",
    "                    alpha=0.7,\n",
    "                    label=r'min $d_{ks}$ for ' + legend_labels[ret_type]\n",
    "                )\n",
    "            \n",
    "        ax[i, j].legend()\n",
    "\n",
    "ax[0, 0].set_ylim([-0.03, 0.5])\n",
    "ax[0, 1].set_ylim([-0.03, 0.5])\n",
    "\n",
    "ax[0, 0].set_xlim([1e-4, 1e-1])\n",
    "ax[0, 1].set_xlim([1e-4, 1e-1])\n",
    "\n",
    "ax[1, 0].set_ylim([-0.03, 0.5])\n",
    "ax[1, 1].set_ylim([-0.03, 0.5]);\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le linee sottostanti le figure, composte in realtà da punti, identificano quei valori per cui il test di Kolmogorov-Smirnov ha dato un _p_-value $p \\leq 0.05$, ed è quindi ritenuto statisticamente valido. Si nota come agli estremi di $k$ e dei returns il *p*-value non sia significativo e ci siano grosse oscillazioni, probabilmente dovute a instabilità numeriche nel calcolo della maximum likelihood.\n",
    "\n",
    "### 3.2 Minima $d_{KS}$ per trovare il miglior fit della GEV\n",
    "\n",
    "Ottimo, ora bisogna selezionare il minimo valore della statistica KS $d_{KS}$ che abbia un p-value valido ($p < 0.05$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_dks(d_ks, pvals, min_pval=min_pval):\n",
    "    dks = d_ks.copy()\n",
    "    invalid = pvals > min_pval\n",
    "    \n",
    "    dks[invalid] = np.min(dks) + 1\n",
    "    i_min = np.argmin(dks)\n",
    "    \n",
    "    return dks[i_min], i_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_kolmog_smirn = {\n",
    "    s_type: {\n",
    "        ret_type: find_min_dks(kolmog_smirn[s_type][ret_type]['ks'], kolmog_smirn[s_type][ret_type]['p'])\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "threshold_evt = {\n",
    "    s_type: {\n",
    "        'pos': sorted_lr[s_type]['pos'][min_kolmog_smirn[s_type]['pos'][1]],\n",
    "        'neg': sorted_lr[s_type]['neg'][min_kolmog_smirn[s_type]['neg'][1]],\n",
    "        'abs': sorted_lr[s_type]['abs'][min_kolmog_smirn[s_type]['abs'][1]],\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "title_format = \"{:>15}\"*5\n",
    "row_format = \"{:>15}{:>15}{:>15.4f}{:>15}{:>15.6f}\"\n",
    "\n",
    "print(title_format.format('Stock type', 'Return type', 'Min d_KS', 'i', 'Return'))\n",
    "print(\"-\"*15 * 5)\n",
    "for s_type in stock_type:\n",
    "    print(\"-\"*15 * 5)\n",
    "    for ret_type in return_type:\n",
    "        print(row_format.format(\n",
    "            s_type,\n",
    "            ret_type,\n",
    "            min_kolmog_smirn[s_type][ret_type][0],\n",
    "            min_kolmog_smirn[s_type][ret_type][1],\n",
    "            threshold_evt[s_type][ret_type]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora vediamo le GEV fittate sui 3 returns, per le due azioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparo i dati per plottare\n",
    "titles = {\n",
    "    'pos': 'Positive extreme returns',\n",
    "    'neg': 'Negative extreme returns',\n",
    "    'abs': 'Absolute extreme returns',\n",
    "}\n",
    "\n",
    "xlabels = {\n",
    "    'pos': r'Positive extreme returns $r$',\n",
    "    'neg': r'Negative extreme returns $-r$',\n",
    "    'abs': r'Absolute extreme returns  $|r|$',\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'pos': r'$r$',\n",
    "    'neg': r'$-r$',\n",
    "    'abs': r'$|r|$',\n",
    "}\n",
    "\n",
    "# plot\n",
    "fig, ax = pl.subplots(nrows=len(stock_type), ncols=len(return_type), figsize=(18, 10), sharex=False, sharey=True)\n",
    "\n",
    "# sulle righe i due flussi\n",
    "for i, s_type in enumerate(stock_type):\n",
    "    \n",
    "    # sulle colonne i 3 tipi di returns e le loro GEV fittate\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        i_min = min_kolmog_smirn[s_type][ret_type][1]\n",
    "\n",
    "        data = sorted_lr[s_type][ret_type].values[:i_min]\n",
    "        best_fit = fits[s_type][ret_type][i_min]\n",
    "        \n",
    "        sns.distplot(\n",
    "            data,\n",
    "            color=colors[ret_type],\n",
    "            label=legend_labels[ret_type],\n",
    "            kde=False,\n",
    "            norm_hist=True,\n",
    "            ax=ax[i, j]\n",
    "        )\n",
    "        \n",
    "        title = s_type\n",
    "        _, b = ax[i, j].xaxis.get_data_interval()\n",
    "        x = np.linspace(0, b, 1000)\n",
    "        pdf = gev.pdf(x, *best_fit)\n",
    "        ax[i, j].plot(x, pdf, color=colors[ret_type], label='GEV pdf')\n",
    "        \n",
    "        ax[i, j].set_title(title)\n",
    "        ax[i, j].set_xlabel(xlabels[ret_type])\n",
    "        ax[i, j].legend()\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finora abbiamo quindi ottenuto:\n",
    "\n",
    "- le distribuzioni di probabilità degli extreme returns (positivi, negativi, assoluti)\n",
    "- i threshold che massimizzano il fitting della distribuzione *GEV* sugli extreme returns. Tali threshold possono essere quindi usati per determinare quali movimenti siano estremi e quali no\n",
    "\n",
    "Concludiamo quindi confrontando i threshold così ottenuti con i threshold del 95% percentile che abbiamo utilizzato finora per le azioni S&P500, e con quello che si otterrebbe ad utilizzare il $k^*$ calcolato con lo stimatore di Hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo dei threshold\n",
    "# quantili: attenzione che sono TUTTI POSITIVI\n",
    "thresholds = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: returns[s_type][ret_type].abs().quantile(float(q_type) / 100)\n",
    "            for q_type in quantile_type[:-1]\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "for s_type in stock_type:\n",
    "    for ret_type in return_type:\n",
    "        thresholds[s_type][ret_type]['evt'] = abs(threshold_evt[s_type][ret_type])\n",
    "\n",
    "extremes = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: (returns[s_type][ret_type].abs() >= thresholds[s_type][ret_type][q_type]).astype(np.int8)\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "extremes_test = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: (returns_test[s_type][ret_type].abs() >= thresholds[s_type][ret_type][q_type]).astype(np.int8)\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come sono i threshold graficamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=2, ncols=1, figsize=(18, 10))\n",
    "\n",
    "for i, s_type in enumerate(stock_type):\n",
    "    if s_type == 'min_vol':\n",
    "        y = log_returns[idx_min_beta['beta']]\n",
    "    else:\n",
    "        y = log_returns[idx_max_beta['beta']]\n",
    "        \n",
    "    x = y.index\n",
    "        \n",
    "    y_train = y[x < split_dates[split_key]]\n",
    "    y_test = y[x >= split_dates[split_key]]\n",
    "    \n",
    "    ax[i].plot(\n",
    "        y_train,\n",
    "        label=s_type,\n",
    "        color=stock_colors[s_type],\n",
    "        alpha=0.75,\n",
    "        linestyle='',\n",
    "        marker='.'\n",
    "    )\n",
    "    \n",
    "    ax[i].plot(\n",
    "        y_test,\n",
    "        label='__None__',\n",
    "        color=stock_colors[s_type],\n",
    "        alpha=0.75,\n",
    "        linestyle='',\n",
    "        marker='.'\n",
    "    )\n",
    "    \n",
    "    ax[i].axvline(\n",
    "        split_dates['subprime-crisis-halfway'],\n",
    "        color='grey',\n",
    "        linestyle='-',\n",
    "    )\n",
    "    \n",
    "    ax[i].axvline(\n",
    "        split_dates['eu-debt-halfway'],\n",
    "        color='grey',\n",
    "        linestyle='-',\n",
    "    )\n",
    "    \n",
    "    ax[i].axvline(\n",
    "        split_dates['last_train'],\n",
    "        color='grey',\n",
    "        linestyle='-',\n",
    "    )\n",
    "    \n",
    "    # ora i percentili\n",
    "    ax[i].plot(\n",
    "        x, -thresholds[s_type]['neg']['95'] * np.ones(len(x)), color=colors['neg'], linestyle='--', label=r'95% percentile, $-r$'\n",
    "    )\n",
    "    ax[i].plot(\n",
    "        x, thresholds[s_type]['pos']['95'] * np.ones(len(x)), color=colors['pos'], linestyle='--', label=r'95% percentile, $r$'\n",
    "    )\n",
    "    ax[i].plot(\n",
    "        x, -thresholds[s_type]['neg']['evt'] * np.ones(len(x)), color=colors['neg'], linestyle=':', label=r'EVT, $-r$'\n",
    "    )\n",
    "    ax[i].plot(\n",
    "        x, -thresholds[s_type]['pos']['evt'] * np.ones(len(x)), color=colors['pos'], linestyle=':', label=r'EVT, $r$'\n",
    "    )\n",
    "    \n",
    "    ax[i].set_title(str.capitalize(s_type))\n",
    "    ax[i].set_ylabel('Log Returns')\n",
    "    ax[i].legend()\n",
    "    \n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può vedere dal plot, i percentili sono molto più stringenti rispetto al valore che massimizza il fitting della *GEV*, mentre i threshold calcolati con il solo stimatore di Hill sono più stringenti dei percentili.\n",
    "\n",
    "Vediamo anche di confermarlo con un po' di numeri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent(data, thresh_low, thresh_up):\n",
    "    indexes = np.logical_or(data.values <= thresh_low, data.values >= thresh_up)\n",
    "    num = len(data[indexes])\n",
    "    denom = len(data)\n",
    "    \n",
    "    return num / denom\n",
    "\n",
    "perc = dict()\n",
    "extremes_with_percentiles = dict()\n",
    "extremes_with_evt = dict()\n",
    "\n",
    "for s_type in stock_type:\n",
    "    idx = idx_min_beta['beta'] if s_type == 'min_vol' else idx_max_beta['beta']\n",
    "    perc[s_type] = log_returns[idx].quantile(q=[0.05, 0.95])\n",
    "    \n",
    "    extremes_with_percentiles[s_type] = get_percent(log_returns[idx], perc[s_type][0.05], perc[s_type][0.95])\n",
    "    extremes_with_evt[s_type] = get_percent(log_returns[idx], -thresholds[s_type]['neg']['evt'], thresholds[s_type]['pos']['evt'])\n",
    "\n",
    "\n",
    "print(\"{:>20}{:>20}{:>15}\".format('Stock Type', 'Threshold type', 'Extremes %'))\n",
    "print(\"-\"*55)\n",
    "for s_type in stock_type:\n",
    "    print(\"-\" * 55)\n",
    "    print(\"{:>20}{:>20}{:>15.3f}\".format(s_type, 'percentile 5-95 %', extremes_with_percentiles[s_type]))\n",
    "    print(\"{:>20}{:>20}{:>15.3f}\".format(s_type, 'EVT', extremes_with_evt[s_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pratica, vuol dire che se usassimo i valori di threshold ricavati dalla *EVT* avremmo un dataset sicuramente più bilanciato, ma c'è da chiedersi se si possano effettivamente allora considerare \"estremi\". Non è troppo \"inclusivo\" un tale threshold?\n",
    "\n",
    "**I threshold della EVT non vanno bene!**\n",
    "\n",
    "### 3.5 Calcolo dei $\\tau_Q$ e delle $Q$\n",
    "\n",
    "Calcoliamoli per poi usarli nella maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo i tau e calcolo i Q\n",
    "tau_q_95 = 1.0 / (1.0 - 0.95)\n",
    "tau_q_975 = 1.0 / (1.0 - 0.975)\n",
    "tau_q_99 = 1.0 / (1.0 - 0.99)\n",
    "\n",
    "# calcolo i quantili equivalenti ai threshold della EVT\n",
    "Q = {\n",
    "    s_type: {\n",
    "        ret_type: 1.0 - (sum(returns['min_vol'][ret_type].abs() >= abs(threshold_evt[s_type][ret_type])) / len(returns[s_type][ret_type]))\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "tau_q = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            '95': tau_q_95,\n",
    "            '97.5': tau_q_975,\n",
    "            '99': tau_q_99,\n",
    "            'evt': 1.0 / (1.0 - Q[s_type][ret_type])\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcolo degli intervalli di ricorrenza e plot della loro distribuzione\n",
    "\n",
    "Ora che abbiamo i threshold possiamo calcolare gli intervalli di ricorrenza e vederne la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrence_intervals(is_extreme: pd.DataFrame):\n",
    "    \"\"\"Get the recurrence intervals durations between extremes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_extreme: pd.DataFrame\n",
    "        a DataFrame with the date on the index and 1 if the return at time t is extreme,\n",
    "        0 otherwise. Must contain a single column named 'extreme'\n",
    "    \"\"\"\n",
    "    assert isinstance(is_extreme.index, pd.DatetimeIndex)\n",
    "    assert len(is_extreme.columns) == 1\n",
    "    \n",
    "    # convert to int\n",
    "    data = is_extreme.astype(np.int8)\n",
    "    data.loc[:, 'date'] = data.index\n",
    "    data.index = pd.RangeIndex(len(is_extreme))\n",
    "    \n",
    "    data_is_extreme = data[data[data.columns[0]] == 1]\n",
    "    \n",
    "    intervals = []\n",
    "    for i in range(1, len(data_is_extreme)):\n",
    "        last_time = data_is_extreme.date.iloc[i - 1]\n",
    "        current_time = data_is_extreme.date.iloc[i]\n",
    "        \n",
    "        n_days = data_is_extreme.index[i] - data_is_extreme.index[i - 1]\n",
    "        \n",
    "        intervals.append((last_time, current_time, n_days))\n",
    "        \n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quantili vanno presi al 95%, 97.5%, 99%. Creo quindi il `dict` che contiene gli intervalli di ricorrenza, organizzati secondo il tipo di return ed il tipo di threshold (quantile o evt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo intervalli di ricorrenza - training set\n",
    "tmp_rec_int = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: get_recurrence_intervals(pd.DataFrame(extremes[s_type][ret_type][q_type]))\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "recurrence_intervals = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: pd.DataFrame(data={\n",
    "                'last_extreme': [x[0] for x in tmp_rec_int[s_type][ret_type][q_type]],\n",
    "                'current_extreme': [x[1] for x in tmp_rec_int[s_type][ret_type][q_type]],\n",
    "                'n_days': [x[2] for x in tmp_rec_int[s_type][ret_type][q_type]],\n",
    "            })\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "# calcolo intervalli di ricorrenza - testing set\n",
    "tmp_rec_int_test = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: get_recurrence_intervals(pd.DataFrame(extremes_test[s_type][ret_type][q_type]))\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "recurrence_intervals_test = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: pd.DataFrame(data={\n",
    "                'last_extreme': [x[0] for x in tmp_rec_int_test[s_type][ret_type][q_type]],\n",
    "                'current_extreme': [x[1] for x in tmp_rec_int_test[s_type][ret_type][q_type]],\n",
    "                'n_days': [x[2] for x in tmp_rec_int_test[s_type][ret_type][q_type]],\n",
    "            })\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plot istogrammi intervalli di ricorrenza\n",
    "\n",
    "Ora visualizzo graficamente la lunghezza degli intervalli di ricorrenza con degli istogrammi, rispettivamente per i returns positivi, negativi ed assoluti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_labels = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $r$',\n",
    "    'neg': r'Negative $-r$',\n",
    "    'abs': r'Absolute $|r|$',\n",
    "}\n",
    "\n",
    "y_lims = {\n",
    "    'pos': [0.0, 0.15],\n",
    "    'neg': [0.0, 0.15],\n",
    "    'abs': [0.0, 0.08],\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "# riga 1: training set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals['min_vol'][ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[0, i])\n",
    "\n",
    "    ax[0, i].legend()\n",
    "    ax[0, i].set(title='min_vol | ' + titles[ret_type] + \" | training set\", ylim=y_lims[ret_type])\n",
    "\n",
    "# riga 2: testing set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals_test['min_vol'][ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[1, i])\n",
    "\n",
    "    ax[1, i].legend()\n",
    "    ax[1, i].set(title='min_vol | ' + titles[ret_type] + \" | testing set\", ylim=y_lims[ret_type])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ora per la massima volatile\n",
    "fig, ax = pl.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "# riga 1: training set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals['max_vol'][ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[0, i])\n",
    "\n",
    "    ax[0, i].legend()\n",
    "    ax[0, i].set(title='max_vol | ' + titles[ret_type] + \" | training set\", ylim=y_lims[ret_type])\n",
    "\n",
    "# riga 2: testing set\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for q_type in quantile_type:\n",
    "        curr = recurrence_intervals_test['max_vol'][ret_type][q_type].n_days\n",
    "        sns.distplot(curr, kde=False, norm_hist=True, label=hist_labels[q_type], ax=ax[1, i])\n",
    "\n",
    "    ax[1, i].legend()\n",
    "    ax[1, i].set(title='max_vol | ' + titles[ret_type] + \" | testing set\", ylim=y_lims[ret_type])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Creazione delle tabelle come nel paper\n",
    "\n",
    "Ora creo le tabelle riassuntive come a pagina 9 del paper di [Jiang et al](https://doi.org/10.1080/14697688.2017.1373843).\n",
    "\n",
    "Prima mi creo due funzioncine e poi le chiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_table(intervals: pd.DataFrame, returns: pd.DataFrame, ret_type: str, thresh: float, col_name='perc'):\n",
    "    \"\"\"Get a single panel sub-table.\"\"\"\n",
    "    obsv = int(intervals.shape[0])\n",
    "    mean = intervals['n_days'].mean()\n",
    "    median = intervals['n_days'].median()\n",
    "    std_dev = intervals['n_days'].std()\n",
    "    skewness = intervals['n_days'].skew()\n",
    "    kurtosis = intervals['n_days'].kurt()\n",
    "    \n",
    "    ret_mean = returns.mean()\n",
    "    ret_std_dev = returns.std()\n",
    "    \n",
    "    m = (thresh - ret_mean) / ret_std_dev\n",
    "#     print(f\"\\nRet_type: {ret_type}, q_type: {col_name}\")\n",
    "#     print(f\"Threshold: {thresh:5.4f}, Mean: {ret_mean:5.4f}, m: {m:5.4f}\")\n",
    "        \n",
    "    acf, qstat, pvals = stattools.acf(intervals['n_days'].values, qstat=True, nlags=30)\n",
    "    rho1 = acf[1]\n",
    "    _, p_rho1 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[1:],\n",
    "        intervals['n_days'].shift(periods=1).values[1:],\n",
    "    )\n",
    "    \n",
    "    rho5 = acf[5]\n",
    "    _, p_rho5 = scipy.stats.pearsonr(\n",
    "        intervals['n_days'].values[5:],\n",
    "        intervals['n_days'].shift(periods=5).values[5:],\n",
    "    )\n",
    "\n",
    "    Q30 = qstat[-1]\n",
    "    p_Q30 = pvals[-1]\n",
    "    \n",
    "    index = pd.Index(data=[\n",
    "        'm',\n",
    "        'obsv',\n",
    "        'mean',\n",
    "        'median',\n",
    "        'stdev',\n",
    "        'skew',\n",
    "        'kurt',\n",
    "        'rho(1)',\n",
    "        'p-value(rho1)',\n",
    "        'rho(5)',\n",
    "        'p-value(rho5)',\n",
    "        'Q(30)',\n",
    "        'p-value(Q30)',\n",
    "    ])\n",
    "    \n",
    "    result = pd.DataFrame(data=[\n",
    "        [m],\n",
    "        [obsv],\n",
    "        [mean],\n",
    "        [median],\n",
    "        [std_dev],\n",
    "        [skewness],\n",
    "        [kurtosis],\n",
    "        [rho1],\n",
    "        [p_rho1],\n",
    "        [rho5],\n",
    "        [p_rho5],\n",
    "        [Q30],\n",
    "        [p_Q30],\n",
    "    ],\n",
    "    index=index,\n",
    "    columns=[col_name])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il titolo è la sezione della tabella (Negative/Positive/Absolute), i quantili sono quelli che mi interessano e finiranno sulle colonne della tabella ed il risultato è un `dict` che ha come chiavi i titoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: get_single_table(recurrence_intervals[s_type][ret_type][q_type],\n",
    "                                     returns[s_type][ret_type],\n",
    "                                     ret_type,\n",
    "                                     thresh=thresholds[s_type][ret_type][q_type],\n",
    "                                     col_name=q_type)\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "panels = {\n",
    "    s_type: {\n",
    "        ret_type: pd.concat([tables[s_type][ret_type][q_type] for q_type in quantile_type], axis='columns')\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo le tabelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['min_vol']['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['min_vol']['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['min_vol']['abs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['max_vol']['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['max_vol']['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels['max_vol']['abs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot degli autocorrelogrammi\n",
    "\n",
    "Vediamo con gli [autocorrelogammi](https://en.wikipedia.org/wiki/Correlogram) se c'è autocorrelazione nelle serie dei *recurrence interval*.\n",
    "\n",
    "Prima quelli dell'azione meno volatile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle righe il tipo di threshold, sulle colonne il tipo di return\n",
    "s_type = 'min_vol'\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=3, figsize=(27, 10))\n",
    "fig.suptitle(f\"{s_type} - Autocorrelation plots\", fontsize=16)\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['neg']['95']['n_days'].values, lags=30, ax=ax[0][0], title=r'$-r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['pos']['95']['n_days'].values, lags=30, ax=ax[0][1], title=r'$r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['abs']['95']['n_days'].values, lags=30, ax=ax[0][2], title=r'$|r|$, 95%')\n",
    "ax[0, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['neg']['evt']['n_days'].values, lags=30, ax=ax[1][0], title=r'$-r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['pos']['evt']['n_days'].values, lags=30, ax=ax[1][1], title=r'$r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['abs']['evt']['n_days'].values, lags=30, ax=ax[1][2], title=r'$|r|$, EVT')\n",
    "ax[1, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "for a in ax[1]:\n",
    "    a.set_xlabel('lag in the recurrence interval array')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle righe il tipo di threshold, sulle colonne il tipo di return\n",
    "s_type = 'max_vol'\n",
    "\n",
    "fig, ax = pl.subplots(nrows=2, ncols=3, figsize=(27, 10))\n",
    "fig.suptitle(f\"{s_type} - Autocorrelation plots\", fontsize=16)\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['neg']['95']['n_days'].values, lags=30, ax=ax[0][0], title=r'$-r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['pos']['95']['n_days'].values, lags=30, ax=ax[0][1], title=r'$r$, 95%')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['abs']['95']['n_days'].values, lags=30, ax=ax[0][2], title=r'$|r|$, 95%')\n",
    "ax[0, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['neg']['evt']['n_days'].values, lags=30, ax=ax[1][0], title=r'$-r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['pos']['evt']['n_days'].values, lags=30, ax=ax[1][1], title=r'$r$, EVT')\n",
    "tsaplots.plot_acf(recurrence_intervals[s_type]['abs']['evt']['n_days'].values, lags=30, ax=ax[1][2], title=r'$|r|$, EVT')\n",
    "ax[1, 0].set_ylabel(\"Pearson $R$\")\n",
    "\n",
    "for a in ax[1]:\n",
    "    a.set_xlabel('lag in the recurrence interval array')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per interpretare i plot, bisogna ricordare che:\n",
    "\n",
    "- sulle $x$ c'è il lag della serie temporale relativa ai giorni tra i movimenti estremi, cioè quella degli intervalli di ricorrenza. Vuol dire che $x=22$ significa il 22° intervallo di ricorrenza visto nel passato, prima di quello attuale, non 22 giorni prima di oggi. La distanza in giorni potrebbe anche essere un anno o più.\n",
    "- sulle y c'è la correlazione di Pearson $R$\n",
    "\n",
    "Apparentemente **non c'è autocorrelazione nei recurrence intervals**, con nessun quantile, a parte sporadici (es il 95%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verifica relazione empirica\n",
    "\n",
    "Verifichiamo ora la relazione empirica $\\tau_Q = \\frac{Q}{1 - Q}$ dove $Q$ è il quantile scelto (0.95, 0.975, 0.99), $\\tau_Q$ l'intervallo di ricorrenza medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_format = \"{:>15}\"*6\n",
    "row_format = \"{:>15}{:>15.3f}{:>15}{:>15.3f}{:>15.3f}\" + \"{:>14.3f}%\"\n",
    "print(title_format.format('Stock type', 'Quantile', 'Return type', 'tau_q', 'True mean', 'Error %'))\n",
    "print('-' * 15 * 6)\n",
    "\n",
    "for s_type in stock_type:\n",
    "    print('-' * 15 * 6)\n",
    "    for q_type in quantile_type[:-1]:\n",
    "        for i, name in enumerate(return_type):        \n",
    "            data_mean = recurrence_intervals[s_type][name][q_type]['n_days'].mean()\n",
    "\n",
    "            q = float(q_type) / 100.0\n",
    "            tau = 1.0 / (1.0 - q)\n",
    "\n",
    "            perc_diff = (tau - data_mean) / data_mean\n",
    "\n",
    "            print(row_format.format(s_type, q, name, tau, data_mean, perc_diff * 100))\n",
    "\n",
    "            if i == len(return_type) - 1:\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La relazione non vale in questo caso.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Determinazione della *Hazard Probability*\n",
    "\n",
    "Gli autori definiscono la *hazard probability* come\n",
    "\n",
    "\\begin{equation}\n",
    "    W(\\Delta t | t) = \\frac{\\int_t^{t + \\Delta t} p(\\tau)d\\tau}{\\int_t^{\\infty}p(\\tau)d\\tau}\n",
    "\\end{equation}\n",
    "\n",
    "dove $p(\\tau)$ è la distribuzione di probabilità (`pdf` per scipy).\n",
    "\n",
    "La hazard probability definisce la probabilità che, dato che si è verificato un evento estremo $t$ giorni nel passato, ci sia un tempo di attesa $\\Delta t$ ulteriore prima di un altro evento estremo.\n",
    "Se consideriamo $W(1 | t)$ è simile al problema che abbiamo affrontato con la rete neurale.\n",
    "\n",
    "Ora, nota la ditribuzione $p(\\tau)$, si può derivare analiticamente l'integrale. Il problema è quindi: come trovare $p(\\tau)$, e che forma ha?\n",
    "\n",
    "Gli autori utilizzano una [stretched exponential distribution](https://en.wikipedia.org/wiki/Stretched_exponential_function), una [*q*-exponential distribution](https://en.wikipedia.org/wiki/Q-exponential_distribution) ed una [Weibull distribution](https://it.wikipedia.org/wiki/Distribuzione_di_Weibull). I parametri delle 3 distribuzioni vengono stimati tramite MLE.\n",
    "\n",
    "Il flusso è il seguente:\n",
    "\n",
    "1. scegli una distribuzione (s-exp, q-exp, Weibull)\n",
    "2. riformula la parametrizzazione in funzione solo  dello *shape parameter*\n",
    "3. calcola la log-likelihood utilizzando una semplice ricerca a griglia sui parametri liberi\n",
    "4. i parametri che forniscono la massima log-likelihood sono quelli cercati, e trova la formula teorica della *hazard probability* con le equazioni del paper\n",
    "\n",
    "Cominciamo con la Weibull, ma prima creiamo una funzione che calcoli la *hazard probability* empirica, con la formula\n",
    "\n",
    "$$\n",
    "W_{emp}(\\Delta t | t) = \\frac{\\#(t < \\tau \\leq t + \\Delta t)}{\\#(\\tau > t)}\n",
    "$$\n",
    "\n",
    "dove al numeratore c'è il numero di recurrence intervals con valore compreso in $(t, t + \\Delta t]$, al denominatore il numero di recurrence intervals con valore maggiore di $t$, cioè nel range $(t, +\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empirical_hazard_prob(rec_ints: np.ndarray, t, delta_t):\n",
    "    \"\"\"Compute the empirical hazard probability.\"\"\"\n",
    "    assert isinstance(rec_ints, np.ndarray)\n",
    "    num = np.sum(np.logical_and(rec_ints > t, rec_ints <= t + delta_t))\n",
    "    denom = np.sum(rec_ints > t)\n",
    "    \n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Fitting della Weibull\n",
    "\n",
    "In `scipy.stats` è definita come\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&f(x, c) = c x^{c - 1} e^{-x^{c}} \\\\\n",
    "&f(x, c, loc, scale) = \\frac{1}{scale}f\\left(\\frac{x - loc}{scale}, c\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "dove $c$ è lo *shape parameter*. Nel paper invece è\n",
    "\n",
    "\\begin{equation}\n",
    "f(x, \\beta, \\alpha) = \\frac{\\alpha}{\\beta} \\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha - 1} e^{-\\left( \\frac{\\tau}{\\beta} \\right)^{\\alpha}}\n",
    "\\end{equation}\n",
    "\n",
    "quindi la corrispondenza è\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&loc = 0 \\\\\n",
    "&\\beta = scale \\\\\n",
    "&shape = c = \\alpha\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ora dobbiamo stimare i parametri della Weibull con una maximum log-likelihood estimation (*MLE*). Riscrivendoli in funzione di $\\tau_Q$ e $\\beta = scale$ abbiamo\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)} \\\\\n",
    "cioè \\\\\n",
    "&\\beta = scale = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{c} \\right)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Ricordiamo che $\\tau_Q = \\frac{1}{1 - Q}$ dove $Q$ è il quantile.\n",
    "\n",
    "A questo punto la MLE ha formula:\n",
    "\n",
    "$$ ln(L_w) = n \\cdot ln\\left( \\frac{c}{\\beta} \\right) + \\sum_{i=1}^{n} \\left[ (c - 1) ln\\left( \\frac{\\tau_i}{\\beta} \\right) - \\left( \\frac{\\tau_i}{\\beta} \\right)^c \\right] $$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $t_i$ il corrispondente valore dell'intervallo di ricorrenza (es: 14 giorni, 4 giorni...).\n",
    "\n",
    "Il flow è quindi, in questo caso:\n",
    "\n",
    "1. a seconda del percentile (95% o EVT) calcolare $Q$ e quindi $\\tau_Q$\n",
    "2. utilizzare una ricerca con step $1e-6$ sul parametro $c = \\alpha$, il quale risulta in un certo valore di $\\beta$\n",
    "3. utilizzare quei valori di $c$ e di $\\beta$ nella MLE\n",
    "4. trovare il massimo della MLE ed i corrispondenti valori di $c$ e $\\beta$\n",
    "5. urrà! Ora possiamo usarli nella *pdf* della distribuzione Weibull per ottenere l'hazard $W(\\Delta t | t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@numba.jit(nopython=True, parallel=True, nogil=True)\n",
    "def mle_weibull(rec_ints: np.ndarray, c: np.ndarray, beta: np.ndarray):\n",
    "    \"\"\"MLE estimation for weibull distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"       \n",
    "    m = beta.shape[0]\n",
    "    n = rec_ints.shape[0]\n",
    "\n",
    "    # log-likelihood    \n",
    "    log_likelihoods = np.zeros_like(beta)\n",
    "    \n",
    "    # precompute matrices for tau_beta and ln_tau_beta\n",
    "    tau_beta = np.zeros((n, m), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            tau_beta[i, j] = rec_ints[i] / beta[j]\n",
    "            \n",
    "    ln_tau_beta = np.log(tau_beta)\n",
    "    \n",
    "    c_beta = c / beta\n",
    "    n_ln_c_beta = n * np.log(c_beta)\n",
    "    c_1 = c - 1.0\n",
    "\n",
    "    for j in numba.prange(m):  # no progress indication, it's a parallel for loop\n",
    "        summ = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            summ += c_1[j] * ln_tau_beta[i, j] - tau_beta[i, j] ** c[j]\n",
    "            \n",
    "        log_likelihoods[j] = n_ln_c_beta[j] + summ\n",
    "        \n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della Weibull per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.arange(0.25, 2, 1e-3)\n",
    "sfg = sfun.gamma(1.0 + (1.0 / c))\n",
    "\n",
    "beta = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: tau_q[s_type][ret_type][q_type] / sfg\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type        \n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "i_ok = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: np.argwhere(beta[s_type][ret_type][q_type] > 1e-6).flatten()\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "beta_ok = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: beta[s_type][ret_type][q_type][i_ok[s_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "c_ok = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: c[i_ok[s_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = False\n",
    "\n",
    "ll_weib_file = f\"./sp500/log-like-weib_manual_split.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_weib_file, 'rb') as infile:\n",
    "        log_like_weib = pickle.load(infile)\n",
    "else:\n",
    "    log_like_weib = dict()\n",
    "\n",
    "    for s_type in stock_type:\n",
    "        log_like_weib[s_type] = dict()\n",
    "        \n",
    "        for ret_type in return_type:\n",
    "            log_like_weib[s_type][ret_type] = dict()\n",
    "            print(f\"\\nStock type: {s_type} | Return type: {ret_type}\")\n",
    "\n",
    "            for q_type in quantile_type:\n",
    "                x = recurrence_intervals[s_type][ret_type][q_type]['n_days'].values\n",
    "                print(f\"Computing Weibull MLE on quantile: {q_type}, c={c_ok[s_type][ret_type][q_type].shape}, beta={beta_ok[s_type][ret_type][q_type].shape}\")\n",
    "\n",
    "                ll = mle_weibull(x, c_ok[s_type][ret_type][q_type], beta_ok[s_type][ret_type][q_type])\n",
    "\n",
    "                log_like_weib[s_type][ret_type][q_type] = ll\n",
    "\n",
    "    with open(ll_weib_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_weib, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=len(stock_type), figsize=(20, 12))\n",
    "\n",
    "# positive log-returns\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for j, s_type in enumerate(stock_type):\n",
    "        for q_type in quantile_type:\n",
    "            ax[i, j].plot(\n",
    "                c_ok[s_type][ret_type][q_type],\n",
    "                log_like_weib[s_type][ret_type][q_type],\n",
    "                color=colors_mle[q_type],\n",
    "                label=legend_labels_mle[q_type])\n",
    "\n",
    "            i_max = np.argmax(log_like_weib[s_type][ret_type][q_type])\n",
    "\n",
    "            ax[i, j].plot(\n",
    "                c_ok[s_type][ret_type][q_type][i_max],\n",
    "                log_like_weib[s_type][ret_type][q_type][i_max],\n",
    "                marker='o',\n",
    "                color=colors_mle[q_type]\n",
    "            )\n",
    "\n",
    "        ax[i, j].set(title=s_type + ' | ' + titles[ret_type], xlabel='c', ylabel=r'$\\log(L_W)$')\n",
    "        ax[i, j].legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ora che abbiamo le MLE per i tre tipi di returns e i minimi, possiamo fittare la Weibull sui recurrence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_min = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: np.argmax(log_like_weib[s_type][ret_type][q_type])\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_shape = {\n",
    "    'weibull': {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: c_ok[s_type][ret_type][q_type][i_min[s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_scale = {\n",
    "    'weibull': {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: tau_q[s_type][ret_type][q_type] / sfun.gamma(1.0 + 1.0 / best_shape['weibull'][s_type][ret_type][q_type])\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params = {\n",
    "    'weibull': {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: {\n",
    "                    'shape': best_shape['weibull'][s_type][ret_type][q_type],\n",
    "                    'scale': best_scale['weibull'][s_type][ret_type][q_type],\n",
    "                    'loc': 0.0,\n",
    "                }\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fitting della stretched-exponential (s-exp)\n",
    "\n",
    "Creiamo ora la classe per la s-exp, che ha *pdf*:\n",
    "\n",
    "$$\n",
    "p(x, c, a, b) = a e^{-\\left( bx \\right)^c}\n",
    "$$\n",
    "\n",
    "dove $c$, $a$ e $b$ sono *shape parameters*, con $0 < c < 1$, $b \\geq 0$ e $a > 0$.\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della s-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{s-exp}) = n \\cdot \\ln(a) - \\sum_{i=1}^{n} (b \\cdot x_i)^c\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $a = \\frac{c \\Gamma \\left( \\frac{2}{c} \\right)}{\\left[ \\Gamma \\left( \\frac{1}{c} \\right) \\right]^2 \\tau_Q}$ e $b = \\frac{ \\Gamma \\left( \\frac{2}{c} \\right)}{\\Gamma \\left( \\frac{1}{c} \\right) \\tau_Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_b_sexp(c, tau_q):\n",
    "    \"\"\"Get the a and b params.\"\"\"\n",
    "    gamma_2_c = sfun.gamma(2.0 / c)\n",
    "    gamma_1_c = sfun.gamma(1.0 / c)\n",
    "    \n",
    "    b_all = gamma_2_c / (gamma_1_c * tau_q)\n",
    "    a_all = b_all * c / gamma_1_c\n",
    "    \n",
    "    return a_all, b_all\n",
    "\n",
    "@timeit\n",
    "def mle_sexp(rec_ints: np.ndarray, c: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for s-exponential distribution, given an array of c shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    n = rec_ints.shape[0]\n",
    "    \n",
    "    a_all, b_all = get_a_b_sexp(c, tau_q)\n",
    "    \n",
    "    ln_a_all = np.log(a_all)\n",
    "    \n",
    "    ll = np.zeros((c.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j, c in enumerate(c):\n",
    "        ssum = 0\n",
    "        a = a_all[j]\n",
    "        b = b_all[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.power((b * rec_ints[i]), c)\n",
    "            \n",
    "        ll[j] = n * ln_a_all[j] - ssum\n",
    "    \n",
    "    ll[np.isnan(ll)] = -np.inf\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sexp = np.arange(1e-3, 1.0, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = False\n",
    "\n",
    "ll_sexp_file = f\"./sp500/log-like-sexp_manual_split.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_sexp_file, 'rb') as infile:\n",
    "        log_like_sexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_sexp = dict()\n",
    "\n",
    "    for s_type in stock_type:\n",
    "        log_like_sexp[s_type] = dict()\n",
    "        \n",
    "        for ret_type in return_type:\n",
    "            log_like_sexp[s_type][ret_type] = dict()\n",
    "            print(f\"\\nReturn type: {ret_type}\")\n",
    "\n",
    "            for q_type in quantile_type:\n",
    "                x = recurrence_intervals[s_type][ret_type][q_type]['n_days'].values\n",
    "                print(f\"Computing s-exp MLE on quantile: {q_type}, c={c_sexp.shape}\")\n",
    "\n",
    "                ll = mle_sexp(x, c_sexp, tau_q[s_type][ret_type][q_type])\n",
    "\n",
    "                log_like_sexp[s_type][ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_sexp['c'] = c_sexp\n",
    "\n",
    "    with open(ll_sexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_sexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_sexp = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: np.argmax(log_like_sexp[s_type][ret_type][q_type])\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_shape['s-exp'] = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: c_sexp[i_max_sexp[s_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_a_sexp = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: get_a_b_sexp(best_shape['s-exp'][s_type][ret_type][q_type], tau_q[s_type][ret_type][q_type])[0]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_b_sexp = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: get_a_b_sexp(best_shape['s-exp'][s_type][ret_type][q_type], tau_q[s_type][ret_type][q_type])[1]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_params['s-exp'] = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: {\n",
    "                'shape': best_shape['s-exp'][s_type][ret_type][q_type],\n",
    "                'a': best_a_sexp[s_type][ret_type][q_type],\n",
    "                'b': best_b_sexp[s_type][ret_type][q_type],\n",
    "                'loc': 0.0,\n",
    "            }\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=len(stock_type), figsize=(20, 12))\n",
    "\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for j, s_type in enumerate(stock_type):\n",
    "        for q_type in quantile_type:\n",
    "            ax[i, j].plot(\n",
    "                c_sexp[truncation:],\n",
    "                log_like_sexp[s_type][ret_type][q_type][truncation:],\n",
    "                color=colors_mle[q_type],\n",
    "                label=legend_labels_mle[q_type])\n",
    "\n",
    "            ax[i, j].plot(\n",
    "                c_sexp[i_max_sexp[s_type][ret_type][q_type]],\n",
    "                log_like_sexp[s_type][ret_type][q_type][i_max_sexp[s_type][ret_type][q_type]],\n",
    "                marker='o',\n",
    "                color=colors_mle[q_type]\n",
    "            )\n",
    "\n",
    "        ax[i, j].set(title=s_type + ' | ' + titles[ret_type], xlabel='c', ylabel=r'$\\log(L_{s-exp})$')\n",
    "        ax[i, j].legend(loc='lower right')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Fitting della q-exponential\n",
    "\n",
    "La terza distribuzione è la [q-exponential](https://en.wikipedia.org/wiki/Q-exponential_distribution).\n",
    "\n",
    "Creo allora la funzione che minimizza la log-likelihood della q-exp, che è\n",
    "\n",
    "$$\n",
    "ln(L_{q-exp}) = n \\cdot \\ln[\\lambda (2 - q)] - \\frac{1}{q - 1} \\sum_{i=1}^{n} \\ln[1 + (q - 1) \\lambda \\tau_i]\n",
    "$$\n",
    "\n",
    "dove $n$ è il numero di recurrence intervals, $\\tau_i$ il valore dell'i-esimo recurrence interval, e il parametro $\\lambda$ si stima così:\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{1}{\\tau_Q(3 - 2q)}\n",
    "$$\n",
    "\n",
    "il parametro libero $q$ ha il range $\\left( 0, \\frac{3}{2} \\right)$, ma noi lo cercheremo nel range $\\left( 1, \\frac{3}{2} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def mle_qexp(rec_ints: np.ndarray, q: np.ndarray, tau_q: float):\n",
    "    \"\"\"MLE estimation for q-exponential distribution, given an array of q shape parameters and the tau_q,\n",
    "    with the recurrence intervals rec_ints.\n",
    "    \"\"\"\n",
    "    assert np.all(q < 1.5)\n",
    "    \n",
    "    n = rec_ints.shape[0]\n",
    "    m = q.shape[0]\n",
    "    \n",
    "    lam = 1.0 / (tau_q * (3 - 2 * q))\n",
    "    \n",
    "    ll = np.zeros((q.shape[0], ), dtype=np.float64)\n",
    "    \n",
    "    for j in range(m):\n",
    "        ssum = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            ssum += np.log(1 + (q[j] - 1) * lam[j] * rec_ints[i])\n",
    "            \n",
    "        ll[j] = n * np.log(lam[j] * (2 - q[j])) - (1 / (q[j] - 1)) * ssum\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora usiamo la funzione per calcolarci il fitting della s-exp per i returns positivi, negativi ed assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_qexp = np.arange(1.0, 1.5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_log_like = False\n",
    "\n",
    "ll_qexp_file = f\"./sp500/log-like-qexp_manual_split.pickle\"\n",
    "\n",
    "if load_log_like:\n",
    "    with open(ll_qexp_file, 'rb') as infile:\n",
    "        log_like_qexp = pickle.load(infile)\n",
    "else:\n",
    "    log_like_qexp = dict()\n",
    "\n",
    "    for s_type in stock_type:\n",
    "        log_like_qexp[s_type] = dict()\n",
    "        \n",
    "        for ret_type in return_type:\n",
    "            log_like_qexp[s_type][ret_type] = dict()\n",
    "            print(f\"\\nStock type: {s_type} | Return type: {ret_type}\")\n",
    "\n",
    "            for q_type in quantile_type:\n",
    "                x = recurrence_intervals[s_type][ret_type][q_type]['n_days'].values\n",
    "                print(f\"Computing q-exp MLE on quantile: {q_type}, c={q_qexp.shape}\")\n",
    "\n",
    "                ll = mle_qexp(x, q_qexp, tau_q[s_type][ret_type][q_type])\n",
    "                ll[np.isnan(ll)] = -np.inf\n",
    "\n",
    "                log_like_qexp[s_type][ret_type][q_type] = ll\n",
    "            \n",
    "    log_like_qexp['c'] = q_qexp\n",
    "\n",
    "    with open(ll_qexp_file, 'wb') as outfile:\n",
    "          pickle.dump(log_like_qexp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora prendiamo la massima log-likelihoood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max_qexp = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: np.argmax(log_like_qexp[s_type][ret_type][q_type])\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_shape['q-exp'] = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: q_qexp[i_max_qexp[s_type][ret_type][q_type]]\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_lambda_qexp = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: 1.0 / (tau_q[s_type][ret_type][q_type] * (3 - 2 * best_shape['q-exp'][s_type][ret_type][q_type]))\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "best_params['q-exp'] = {\n",
    "    s_type: {\n",
    "        ret_type: {\n",
    "            q_type: {\n",
    "                'shape': best_shape['q-exp'][s_type][ret_type][q_type],\n",
    "                'lambda': best_lambda_qexp[s_type][ret_type][q_type],\n",
    "                'loc': 0.0,\n",
    "            }\n",
    "            for q_type in quantile_type\n",
    "        }\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottiamo quindi i risultati della MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_mle = {\n",
    "    'evt': 'lightskyblue',\n",
    "    '95': 'palegreen',\n",
    "    '97.5': 'limegreen',\n",
    "    '99': 'darkgreen',\n",
    "}\n",
    "\n",
    "legend_labels_mle = {\n",
    "    '95': '95%',\n",
    "    '97.5': '97.5%',\n",
    "    '99': '99%',\n",
    "    'evt': 'EVT',\n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'pos': r'Positive $log(r)$',\n",
    "    'neg': r'Negative $log(r)$',\n",
    "    'abs': r'Absolute $log(r)$',\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots(nrows=3, ncols=len(stock_type), figsize=(20, 12))\n",
    "\n",
    "truncation = 100\n",
    "for i, ret_type in enumerate(return_type):\n",
    "    for j, s_type in enumerate(stock_type):\n",
    "        for q_type in quantile_type:\n",
    "            ax[i, j].plot(\n",
    "                q_qexp,\n",
    "                log_like_qexp[s_type][ret_type][q_type],\n",
    "                color=colors_mle[q_type],\n",
    "                label=legend_labels_mle[q_type])\n",
    "\n",
    "            ax[i, j].plot(\n",
    "                q_qexp[i_max_qexp[s_type][ret_type][q_type]],\n",
    "                log_like_qexp[s_type][ret_type][q_type][i_max_qexp[s_type][ret_type][q_type]],\n",
    "                marker='o',\n",
    "                color=colors_mle[q_type]\n",
    "            )\n",
    "\n",
    "        ax[i, j].set(title=s_type + ' | ' + titles[ret_type], xlabel='c', ylabel=r'$\\log(L_{q-exp})$')\n",
    "        ax[i, j].legend(loc='lower right')\n",
    "\n",
    "sns.despine()\n",
    "    \n",
    "ax[0, 0].set_ylim([-1000, 0])\n",
    "ax[0, 1].set_ylim([-1000, 0])\n",
    "\n",
    "ax[1, 0].set_ylim([-1000, 0])\n",
    "ax[1, 1].set_ylim([-1000, 0])\n",
    "\n",
    "ax[2, 0].set_ylim([-1500, 0])\n",
    "ax[2, 1].set_ylim([-1500, 0])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Calcolo Hazard Probability\n",
    "\n",
    "Perfetto, ora ho i parametri della Weibull, della s-exp e della q-exp per ogni tipo di return e di threshold. Posso quindi ottenere la curva teorica per il fitting dei recurrence intervals.\n",
    "\n",
    "Per la Weibull è\n",
    "\n",
    "$$\n",
    "W_W(\\Delta t | t) = 1 - e^{\\left[ \\left( \\frac{t}{\\beta} \\right)^\\alpha - \\left( \\frac{t + \\Delta t}{\\beta} \\right)^\\alpha \\right]}\n",
    "$$\n",
    "\n",
    "dove $\\alpha = c^*$ lo *shape* ottimale, e $\\beta = \\frac{\\tau_Q}{\\Gamma \\left( 1 + \\frac{1}{\\alpha} \\right)}$ lo *scale* ottimale.\n",
    "\n",
    "Per la s-exp è:\n",
    "\n",
    "$$\n",
    "W_{s-exp}(\\Delta t | t) = \\frac{\\frac{bc}{a} - \\Gamma_l \\left( \\frac{1}{c}, (bt)^c \\right) - \\Gamma_u \\left( \\frac{1}{c}, [b(t + \\Delta t)]^c \\right)}{\\Gamma_u \\left( \\frac{1}{c}, (bt)^c \\right)}\n",
    "$$\n",
    "\n",
    "dove\n",
    "\n",
    "$$\n",
    "\\Gamma_u (a, x) = \\Gamma (a, x, +\\infty) = \\int_{x}^{+\\infty} t^{a - 1} e^{-t} dt\n",
    "$$\n",
    "\n",
    "è la *upper incomplete Gamma function* e\n",
    "\n",
    "$$\n",
    "\\Gamma_l (a, x) = \\Gamma (a, 0, x) = \\int_{0}^{x} t^{a - 1} e^{-t} dt\n",
    "$$\n",
    "\n",
    "è la *lower incomplete Gamma function*. Nel nostro caso, abbiamo quindi che $\\Gamma_l \\left( \\frac{1}{c}, (bt)^c \\right)$ si traduce in $a = 1/c$ e $x = (bt)^c$, mentre $\\Gamma_u \\left( \\frac{1}{c}, [b(t + \\Delta t)]^c \\right)$ in $a = 1/c$ e $x = [b(t + \\Delta t)]^c$.\n",
    "\n",
    "Per la q-exp è:\n",
    "\n",
    "$$\n",
    "W_{q-exp}(\\Delta t | t) = 1 - \\left[ 1 + \\frac{(q - 1)\\lambda \\Delta t}{1 + (q - 1)\\lambda t} \\right]^{1 - \\frac{1}{q - 1}}\n",
    "$$\n",
    "\n",
    "Mi creo allora le funzioni che le calcolano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_hazard(t, shape, scale, delta_t=1):\n",
    "    part_1 = np.power((t / scale), shape)\n",
    "    part_2 = np.power(((t + delta_t) / scale), shape)\n",
    "    \n",
    "    hazard = 1 - np.exp(part_1 - part_2)\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def sexp_hazard(t, c, a, b, delta_t=1):\n",
    "    num1 = (b * c / a)\n",
    "    num2 = sfun.gammainc((1.0 / c), np.power((b * t), c)) * sfun.gamma(1.0 / c)\n",
    "    num3 = sfun.gammaincc((1.0 / c), np.power(b * (t + delta_t), c)) * sfun.gamma(1.0 / c)\n",
    "    \n",
    "    num = num1 - num2 - num3\n",
    "    \n",
    "    denom = sfun.gammaincc((1.0 / c), np.power(b * t, c))\n",
    "    \n",
    "    hazard = num / denom\n",
    "    \n",
    "    return hazard\n",
    "\n",
    "def qexp_hazard(t, q, lam, delta_t=1):\n",
    "    num = (q - 1) * lam * delta_t\n",
    "    denom = 1 + (q - 1) * lam * t\n",
    "    exponent = 1 - (1 / (q - 1))\n",
    "    \n",
    "    hazard = 1 - np.power((1 + num / denom), exponent)\n",
    "    \n",
    "    return hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 60\n",
    "x = np.arange(1, max_t + 1)\n",
    "\n",
    "ret_type = 'neg'\n",
    "q_type = '99'\n",
    "\n",
    "th_weibull = {\n",
    "    s_type: weibull_hazard(\n",
    "            x,\n",
    "            best_params['weibull'][s_type][ret_type][q_type]['shape'],\n",
    "            best_params['weibull'][s_type][ret_type][q_type]['scale']\n",
    "        )\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "th_sexp = {\n",
    "    s_type: sexp_hazard(\n",
    "        x,\n",
    "        best_params['s-exp'][s_type][ret_type][q_type]['shape'],\n",
    "        best_params['s-exp'][s_type][ret_type][q_type]['a'],\n",
    "        best_params['s-exp'][s_type][ret_type][q_type]['b'],\n",
    "    )\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "th_qexp = {\n",
    "    s_type: qexp_hazard(\n",
    "        x,\n",
    "        best_params['q-exp'][s_type][ret_type][q_type]['shape'],\n",
    "        best_params['q-exp'][s_type][ret_type][q_type]['lambda'],\n",
    "    )\n",
    "    for s_type in stock_type\n",
    "}\n",
    "\n",
    "theoretical_hazard = {\n",
    "    'weibull': th_weibull,\n",
    "    's-exp': th_sexp,\n",
    "    'q-exp': th_qexp,\n",
    "}\n",
    "\n",
    "empirical_hazard = {\n",
    "    s_type: np.array([\n",
    "        get_empirical_hazard_prob(recurrence_intervals[s_type][ret_type][q_type]['n_days'].values, t, 1)\n",
    "        for t in x\n",
    "    ])\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo com'è il plot della hazard probability empirica e di quella vera per i return negativi al $q_{99}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "\n",
    "for j, s_type in enumerate(stock_type):\n",
    "    for dist_type in distribution_type:\n",
    "        ax[j].plot(\n",
    "            x,\n",
    "            theoretical_hazard[dist_type][s_type],\n",
    "            color=dist_colors[dist_type],\n",
    "            label=dist_labels[dist_type])\n",
    "\n",
    "    ax[j].plot(\n",
    "        x,\n",
    "        empirical_hazard[s_type],\n",
    "        label=r'Empirical',\n",
    "        color='black',\n",
    "        linestyle='-',\n",
    "        marker='o',\n",
    "        markersize=1,\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "    ax[j].legend(fontsize=14)\n",
    "    ax[j].set_xlabel(r'$t$', fontsize=16)\n",
    "    ax[j].set_ylabel(r'$W(1 | t)$', fontsize=16)\n",
    "    ax[j].set_title(s_type + ' | ' + r'Hazard prob for $q = 0.99$', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calcolo del miglior threshold $w_t$ massimizzando la *utility* $U(\\theta)$\n",
    "\n",
    "Ora dobbiamo calcolare il miglior threshold $w_t$ oltre il quale si dà il warning, cioè:\n",
    "\n",
    "- se $W(1|t) \\geq w_t$ --> warning --> 1\n",
    "- se $W(1|t) < w_t$ --> no warning --> 0\n",
    "\n",
    "Bisogna definire un peso $\\theta$ che si attribuisce alla Recall o al FPR, dove un valore di $\\theta$ maggiore dà più peso alla Recall. Inoltre, si definiscono due funzioni:\n",
    "\n",
    "- la *loss function*, che utilizza il peso:\n",
    "$$\n",
    "L(\\theta) = \\theta (1 - Recall) + (1 - \\theta)FPR\n",
    "$$\n",
    "- la *utility function*, che dipende dalla *loss*:\n",
    "$$\n",
    "U(\\theta) = \\min(\\theta, 1 - \\theta) - L(\\theta)\n",
    "$$\n",
    "che deve essere $U > 0$ per essere utile, e va massimizzata sul *training set*\n",
    "\n",
    "I passi per farlo sono i seguenti, utilizzando le distribuzioni fittate con i parametri migliori `best_params`:\n",
    "\n",
    "1. [x] fissare un valore $\\hat{\\theta}$ per il parametro $\\theta$\n",
    "2. [x] visto che la $Recall = f(w_t)$ e $FPR = g(w_t)$, far variare $w_t \\in [0, 1]$ per ottenere tutti i possibili valori di $U(\\theta^*)$, cioé:\n",
    "    1. [x] scegliere un $w_t \\in [0, 1] = w_t^i$\n",
    "    2. [x] calcolare le hazard probability delle tre distribuzioni $W_W(\\Delta t|t)$, $W_{s-exp}(\\Delta t|t)$ e $W_{q-exp}(\\Delta t|t)$ e trasformarle nel target binario $[0, 1]$ a seconda che siano minori o maggiori di $w_t$. Il $\\Delta t$ è il periodo tra un intervallo di ricorrenza e l'altro a questo punto\n",
    "    3. [x] calcolare le metriche $Recall(w_t^i)$, $FPR(w_t^i)$, $KSS(w_t^i)$ dove\n",
    "    $$\n",
    "    KSS = Recall - FPR\n",
    "    $$\n",
    "    4. [x] calcolare quindi la loss $L(\\hat{\\theta}, w_t^i) = (1 - Recall(w_t^i))\\hat{\\theta} + (1 - \\hat{\\theta})FPR(w_t^i)$\n",
    "    5. [x] calcolare di conseguenza il valore della utility $U(\\hat{\\theta})|_{w_t^i}$\n",
    "3. [x] dopo aver svolto il punto 2 per ogni valore di $w_t \\in [0, 1]$, selezionare il massimo $w_t$ con $argmax_{w_t} U(\\hat{\\theta})$\n",
    "4. [x] plot della ROC curve per il training ed il testing set\n",
    "    1. [x] training set\n",
    "    2. [x] testing set\n",
    "\n",
    "Cominciamo col definire $\\theta$, il range di $w_t$ e una funzione per la recall, l'FPR e il KSS score:\n",
    "\n",
    "## 6.1 Calcolo scores e performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scelta di theta\n",
    "theta = 0.5\n",
    "\n",
    "# w_t\n",
    "n_points = 1000\n",
    "w_t = np.linspace(0, 1, n_points + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definisco la *loss* e la *utility*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(theta, recall, fpr):\n",
    "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
    "    assert theta >= 0.0 and theta <= 1.0\n",
    "    \n",
    "    return theta * (1 - recall) + (1 - theta) * fpr\n",
    "\n",
    "def utility_function(theta, loss):\n",
    "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
    "    return min(theta, 1 - theta) - loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e una funzione che, data la funzione di *hazard* teorica, calcola la probabilità in ogni giorno che ci sia un estremo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione \n",
    "def hazard_prob(hazard_fn, extremes: np.ndarray):\n",
    "    \"\"\"\n",
    "    For every time t, predict the hazard probability using the supplied hazard function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hazard_fn: Callable[[int], float]\n",
    "        function that returns the hazard probability W(1|t), where t is the time passed\n",
    "        since the last extreme event\n",
    "    \n",
    "    extremes: np.ndarray\n",
    "        binary array of extremes, where 1 means extreme and 0 means normal, tim-ordered\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prob: np.ndarray\n",
    "        hazard probability, at every time t, that there will be an extreme event\n",
    "    \"\"\"\n",
    "    assert isinstance(extremes, np.ndarray)\n",
    "    \n",
    "    ext_ind = np.argwhere(extremes == 1).flatten()\n",
    "    assert ext_ind.shape[0] >= 2  # ci sono almeno 2 estremi, sennò tutto questo non ha senso\n",
    "    \n",
    "    n = extremes.shape[0]\n",
    "    probs = np.zeros((n, ), dtype=np.float64)\n",
    "    \n",
    "    # curr_extreme_ind è sempre sull'ultimo estremo visto, a partire dall'inizio, fino al penultimo\n",
    "    # next_extreme_ind è sempre sul prossimo estremo, fino all'ultimo\n",
    "    for curr_extreme_ind, next_extreme_ind in zip(ext_ind[:-1], ext_ind[1:]):\n",
    "        t = 1\n",
    "        \n",
    "        while curr_extreme_ind + t <= next_extreme_ind:\n",
    "            probs[curr_extreme_ind + t] = hazard_fn(t)\n",
    "            if probs[curr_extreme_ind + t] < 0.0:\n",
    "                ipdb.set_trace()\n",
    "            t = t + 1\n",
    "            \n",
    "    # ora gli ultimi, può capitare che l'ultimo estremo non sia l'ultimo elemento di extremes\n",
    "    curr_extreme_ind = ext_ind[-1]\n",
    "    if curr_extreme_ind < n - 1:\n",
    "        t = 1\n",
    "        \n",
    "        while curr_extreme_ind + t < n:\n",
    "            probs[curr_extreme_ind + t] = hazard_fn(t)\n",
    "            t = t + 1\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bene, ora calcolo le probabilità teoriche per ogni tipo di distribuzione, di return e di quantile.\n",
    "\n",
    "Ogni volta è la migliore versione per quel tipo di return, di quantile e distribuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hazard_probabilities(best_params, extremes, verbose=False):\n",
    "    \"\"\"Get all the hazard probabilities, for every distribution, return and threshold type.\"\"\"\n",
    "    # per ogni distribuzione, tipo di ritorno e quantile calcola l'hazard probability\n",
    "    hazard_probabilities = dict()\n",
    "\n",
    "    for dist_type in distribution_type:\n",
    "        hazard_probabilities[dist_type] = dict()\n",
    "        \n",
    "        for s_type in stock_type:\n",
    "            hazard_probabilities[dist_type][s_type] = dict()\n",
    "\n",
    "            for ret_type in return_type:\n",
    "                hazard_probabilities[dist_type][s_type][ret_type] = dict()\n",
    "\n",
    "                for q_type in quantile_type:\n",
    "                    if verbose:\n",
    "                        print(f\"\\nDist: {dist_type} | stock: {s_type} | ret_type: {ret_type} | q_type: {q_type}\")\n",
    "                    bp = best_params[dist_type][s_type][ret_type][q_type]\n",
    "                    ext = extremes[s_type][ret_type][q_type].values\n",
    "\n",
    "                    if dist_type == 'weibull':\n",
    "                        shape = bp['shape']\n",
    "                        scale = bp['scale']\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Using Weibull with params shape: {shape:4.3f}, scale: {scale:4.3f}\")\n",
    "\n",
    "                        def f(x):\n",
    "                            return weibull_hazard(x, shape, scale)\n",
    "\n",
    "\n",
    "                        hazard_probabilities[dist_type][s_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "\n",
    "                    elif dist_type == 's-exp':\n",
    "                        a = bp['a']\n",
    "                        b = bp['b']\n",
    "                        c = bp['shape']\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Using s-exp with params a: {a:4.3f}, b: {b:4.3f}, c: {c:4.3f}\")\n",
    "\n",
    "                        def f(x):\n",
    "                            return sexp_hazard(x, c, a, b)\n",
    "\n",
    "                        hazard_probabilities[dist_type][s_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "\n",
    "                    elif dist_type == 'q-exp':\n",
    "                        q = bp['shape']\n",
    "                        lam = bp['lambda']\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f\"Using q-exp with params q: {q:4.3f}, lambda: {lam:4.3f}\")\n",
    "\n",
    "                        def f(x):\n",
    "                            return qexp_hazard(x, q, lam)\n",
    "\n",
    "                        hazard_probabilities[dist_type][s_type][ret_type][q_type] = hazard_prob(f, ext)\n",
    "                    else:\n",
    "                        raise ValueError(f\"unrecognized distribution name {dist_type}\")\n",
    "                    \n",
    "    return hazard_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_probabilities = get_all_hazard_probabilities(best_params, extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifichiamo che non ci siano elementi negativi (quindi calcoli spurii):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist_type in distribution_type:\n",
    "    for s_type in stock_type:\n",
    "        for ret_type in return_type:\n",
    "            for q_type in quantile_type:\n",
    "                if np.any(hazard_probabilities[dist_type][s_type][ret_type][q_type] < 0.0):\n",
    "                    print(f\"Errori in dist: {dist_type}, stock: {s_type}, ret_type: {ret_type}, q_type: {q_type}\")\n",
    "\n",
    "                if np.any(hazard_probabilities[dist_type][s_type][ret_type][q_type] > 1.0):\n",
    "                    print(f\"Probabilità > 1 in dist: {dist_type}, stock: {s_type}, ret_type: {ret_type}, q_type: {q_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, prendiamoli come errori numerici.\n",
    "\n",
    "Ora, a seconda del threshold $w_t$ convertiamo le probabilità in un target binario e calcoliamo le performance, la loss e la utility per ogni valore di $w_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(prob: np.ndarray, thresh: float):\n",
    "    assert thresh <= 1.0 and thresh >= 0.0\n",
    "    \n",
    "    return (prob >= thresh).astype(np.int8)\n",
    "\n",
    "\n",
    "def recall_fpr_kss_precision(y_true, y_pred):\n",
    "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
    "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "    tn = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    fp = np.sum(np.logical_and(\n",
    "        np.logical_not(y_true),\n",
    "        y_pred\n",
    "    ))\n",
    "    fn = np.sum(np.logical_and(\n",
    "        y_true,\n",
    "        np.logical_not(y_pred)\n",
    "    ))\n",
    "    \n",
    "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
    "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    kss = recall - fpr\n",
    "    \n",
    "    return recall, fpr, kss, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per tutti i valori possibili del threshold\n",
    "def optimize_wt(w, haz_probs, extremes, theta):\n",
    "    \"\"\"Find the best threshold and the performance with the supplied w.\"\"\"\n",
    "    # devo salvarmi le recall e gli fpr per ogni valore di w, dist, return, quantile\n",
    "    recalls = {\n",
    "        dist_type: {\n",
    "            s_type: {\n",
    "                ret_type: {\n",
    "                    q_type: np.zeros((len(w), ), dtype=np.float64)\n",
    "                    for q_type in quantile_type\n",
    "                }\n",
    "                for ret_type in return_type\n",
    "            }\n",
    "            for s_type in stock_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    precisions = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    # per tutti i w_t = thresh\n",
    "    for i, thresh in enumerate(w):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"iteration: {i}/{len(w)}\")\n",
    "        # per tutte le distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            \n",
    "            # per tutte le stock\n",
    "            for s_type in stock_type:\n",
    "\n",
    "                # per tutti i tipi di returns\n",
    "                for ret_type in return_type:\n",
    "\n",
    "                    # per tutti i tipi di threshold/quantile\n",
    "                    for q_type in quantile_type:\n",
    "                        y_pred = to_binary(haz_probs[dist_type][s_type][ret_type][q_type], thresh)\n",
    "                        y_true = extremes[s_type][ret_type][q_type]\n",
    "\n",
    "                        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
    "                        loss = loss_function(theta, recall, fpr)\n",
    "                        utility = utility_function(theta, loss)\n",
    "\n",
    "                        recalls[dist_type][s_type][ret_type][q_type][i] = recall\n",
    "                        fprs[dist_type][s_type][ret_type][q_type][i] = fpr\n",
    "                        precisions[dist_type][s_type][ret_type][q_type][i] = precision\n",
    "                        ksss[dist_type][s_type][ret_type][q_type][i] = kss\n",
    "                        losses[dist_type][s_type][ret_type][q_type][i] = loss\n",
    "                        utilities[dist_type][s_type][ret_type][q_type][i] = utility\n",
    "             \n",
    "    print(\"Finished\")\n",
    "    \n",
    "    # gli indici in w_t dove c'è la combinazione migliore di distribuzione, return e quantili\n",
    "    best_indexes = {\n",
    "        dist_type: {\n",
    "            s_type: {\n",
    "                ret_type: {\n",
    "                    q_type: np.argmax(utilities[dist_type][s_type][ret_type][q_type])\n",
    "                    for q_type in quantile_type\n",
    "                }\n",
    "                for ret_type in return_type\n",
    "            }\n",
    "            for s_type in stock_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "    \n",
    "    best_w = {\n",
    "        dist_type: {\n",
    "            s_type: {\n",
    "                ret_type: {\n",
    "                    q_type: w[best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                    for q_type in quantile_type\n",
    "                }\n",
    "                for ret_type in return_type\n",
    "            }\n",
    "            for s_type in stock_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "    \n",
    "    return recalls, fprs, ksss, precisions, losses, utilities, best_indexes, best_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo le curve ROC per ogni return, threshold e distribuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls, fprs, ksss, precisions, losses, utilities, best_indexes, best_w = optimize_wt(w_t, hazard_probabilities, extremes, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Plot delle performance e utilità sul training set\n",
    "\n",
    "Vediamo allora le curve ROC e la utility per l'azione meno volatile, e poi quella più volatile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meno volatile\n",
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "s_type = 'min_vol'\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(f\"{s_type} | ROC curves for the training set\", fontsize=16)\n",
    "\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            i_sorted = np.argsort(fprs[dist_type][s_type][ret_type][q_type])\n",
    "\n",
    "            x = fprs[dist_type][s_type][ret_type][q_type][i_sorted]\n",
    "            y = recalls[dist_type][s_type][ret_type][q_type][i_sorted]\n",
    "\n",
    "            # la ROC\n",
    "            ax[i, j].plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=dist_colors[dist_type],\n",
    "                alpha=0.75,\n",
    "                label=str.title(dist_type),\n",
    "                marker='.',\n",
    "                markersize=1.5\n",
    "            )\n",
    "            \n",
    "            # il punto in cui è stato scelto il miglior w_t\n",
    "            i_sweet = best_indexes[dist_type][s_type][ret_type][q_type]\n",
    "            best_x = fprs[dist_type][s_type][ret_type][q_type][i_sweet]\n",
    "            best_y = recalls[dist_type][s_type][ret_type][q_type][i_sweet]\n",
    "            ax[i, j].plot(\n",
    "                best_x,\n",
    "                best_y,\n",
    "                linestyle='',\n",
    "                marker='s',\n",
    "                markersize=5,\n",
    "                alpha=0.5,\n",
    "                color=dist_colors[dist_type],\n",
    "                label=f'best for {str.title(dist_type)}'\n",
    "            )\n",
    "\n",
    "        ax[i, j].plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "\n",
    "        ax[i, j].legend(loc='lower right', fontsize=11)\n",
    "        \n",
    "        ax[i, j].set_xlim([0, 1])\n",
    "        ax[i, j].set_ylim([0, 1])\n",
    "        \n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo anche l'utilità, in funzione del threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(f\"{s_type} | Utility function on the training set\", fontsize=16)\n",
    "\n",
    "x = w_t\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            y = utilities[dist_type][s_type][ret_type][q_type]\n",
    "\n",
    "            ax[i, j].plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=dist_colors[dist_type],\n",
    "                alpha=0.75,\n",
    "                label=str.title(dist_type)\n",
    "            )\n",
    "\n",
    "        ax[i, j].legend(fontsize=11)\n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel(r'$w_t$', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel(r'$U(\\theta = {:3.2f})$'.format(theta), fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E ora l'azione più volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# più volatile\n",
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "s_type = 'max_vol'\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(f\"{s_type} | ROC curves for the training set\", fontsize=16)\n",
    "\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            i_sorted = np.argsort(fprs[dist_type][s_type][ret_type][q_type])\n",
    "\n",
    "            x = fprs[dist_type][s_type][ret_type][q_type][i_sorted]\n",
    "            y = recalls[dist_type][s_type][ret_type][q_type][i_sorted]\n",
    "\n",
    "            # la ROC\n",
    "            ax[i, j].plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=dist_colors[dist_type],\n",
    "                alpha=0.75,\n",
    "                label=str.title(dist_type),\n",
    "                marker='.',\n",
    "                markersize=1.5\n",
    "            )\n",
    "            \n",
    "            # il punto in cui è stato scelto il miglior w_t\n",
    "            i_sweet = best_indexes[dist_type][s_type][ret_type][q_type]\n",
    "            best_x = fprs[dist_type][s_type][ret_type][q_type][i_sweet]\n",
    "            best_y = recalls[dist_type][s_type][ret_type][q_type][i_sweet]\n",
    "            ax[i, j].plot(\n",
    "                best_x,\n",
    "                best_y,\n",
    "                linestyle='',\n",
    "                marker='s',\n",
    "                markersize=5,\n",
    "                alpha=0.5,\n",
    "                color=dist_colors[dist_type],\n",
    "                label=f'best for {str.title(dist_type)}'\n",
    "            )\n",
    "\n",
    "        ax[i, j].plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
    "\n",
    "        ax[i, j].legend(loc='lower right', fontsize=11)\n",
    "        \n",
    "        ax[i, j].set_xlim([0, 1])\n",
    "        ax[i, j].set_ylim([0, 1])\n",
    "        \n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel('FPR', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel('Recall', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo anche l'utilità, in funzione del threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7 * len(quantile_type), 7 * len(return_type))\n",
    "\n",
    "fig, ax = pl.subplots(nrows=len(quantile_type), ncols=(len(ret_type)), figsize=size)\n",
    "fig.suptitle(f\"{s_type} | Utility function on the training set\", fontsize=16)\n",
    "\n",
    "x = w_t\n",
    "# sulle righe i quantili\n",
    "for i, q_type in enumerate(quantile_type):\n",
    "    \n",
    "    # sulle colonne i return\n",
    "    for j, ret_type in enumerate(return_type):\n",
    "        \n",
    "        # in ogni grafico, le 3 distribuzioni\n",
    "        for dist_type in distribution_type:\n",
    "            y = utilities[dist_type][s_type][ret_type][q_type]\n",
    "\n",
    "            ax[i, j].plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=dist_colors[dist_type],\n",
    "                alpha=0.75,\n",
    "                label=str.title(dist_type)\n",
    "            )\n",
    "\n",
    "        ax[i, j].legend(fontsize=11)\n",
    "        ax[i, j].set_title(f\"returns = {ret_type} | threshold = {q_type}\")\n",
    "        \n",
    "for a in ax[-1, :]:\n",
    "    a.set_xlabel(r'$w_t$', fontsize=16)\n",
    "    \n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel(r'$U(\\theta = {:3.2f})$'.format(theta), fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predizione nel periodo out-of-sample (test set)\n",
    "\n",
    "Ora bisogna utilizzare le 3 distribuzioni, già fittate sul training set, per predire sul test set.\n",
    "\n",
    "Riutilizzo il codice appena usato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_probabilities_test = get_all_hazard_probabilities(best_params, extremes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(hazard_probs, extremes, best_w, theta):\n",
    "    \"\"\"Predict the classes and evaluate performance of the models.\n",
    "    best_w is the best w threshold, for every dist, return and quantile.\n",
    "    \"\"\"\n",
    "    # devo salvarmi le recall e gli fpr per ogni valore di w, dist, return, quantile\n",
    "    recalls = {\n",
    "        dist_type: {\n",
    "            s_type: {\n",
    "                ret_type: {\n",
    "                    q_type: 0.0\n",
    "                    for q_type in quantile_type\n",
    "                }\n",
    "                for ret_type in return_type\n",
    "            }\n",
    "            for s_type in stock_type\n",
    "        }\n",
    "        for dist_type in distribution_type\n",
    "    }\n",
    "\n",
    "    fprs = copy.deepcopy(recalls)\n",
    "    ksss = copy.deepcopy(recalls)\n",
    "    precisions = copy.deepcopy(recalls)\n",
    "    losses = copy.deepcopy(recalls)\n",
    "    utilities = copy.deepcopy(recalls)\n",
    "\n",
    "    # per tutte le distribuzioni\n",
    "    for dist_type in distribution_type:\n",
    "        \n",
    "        # per tutte le azioni\n",
    "        for s_type in stock_type:\n",
    "\n",
    "            # per tutti i tipi di returns\n",
    "            for ret_type in return_type:\n",
    "\n",
    "                # per tutti i tipi di threshold/quantile\n",
    "                for q_type in quantile_type:\n",
    "                    y_pred = to_binary(\n",
    "                        hazard_probs[dist_type][s_type][ret_type][q_type],\n",
    "                        best_w[dist_type][s_type][ret_type][q_type]\n",
    "                    )\n",
    "                    y_true = extremes[s_type][ret_type][q_type]\n",
    "\n",
    "                    recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
    "                    loss = loss_function(theta, recall, fpr)\n",
    "                    utility = utility_function(theta, loss)\n",
    "\n",
    "                    recalls[dist_type][s_type][ret_type][q_type] = recall\n",
    "                    fprs[dist_type][s_type][ret_type][q_type] = fpr\n",
    "                    ksss[dist_type][s_type][ret_type][q_type] = kss\n",
    "                    precisions[dist_type][s_type][ret_type][q_type] = precision\n",
    "                    losses[dist_type][s_type][ret_type][q_type] = loss\n",
    "                    utilities[dist_type][s_type][ret_type][q_type] = utility\n",
    "\n",
    "    return recalls, fprs, ksss, precisions, losses, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_test, fprs_test, ksss_test, precisions_test, losses_test, utilities_test = predict_and_evaluate(hazard_probabilities_test, extremes_test, best_w, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confronto in-sample vs out-of-sample\n",
    "\n",
    "Confrontiamo allora le performance su training e test set.\n",
    "\n",
    "Bisogna tenere a mente, però, che il training set ed il testing set condividono parte delle due crisi (finanziaria e debito EU), e quindi le performance potrebbero essere addirittura migliori sul testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Plot di $W(1|t)$ e $W_{emp}(1|t)$\n",
    "\n",
    "Vediamo le hazard probability empiriche e fittate, sia sul training che sul testing set, coi returns negativi e threshold 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 60\n",
    "x = np.arange(1, max_t + 1)\n",
    "\n",
    "ret_type = 'neg'\n",
    "q_type = '99'\n",
    "theoretical_hazard_test = dict()\n",
    "\n",
    "# per ogni distribuzione\n",
    "for dist_type in distribution_type:\n",
    "    theoretical_hazard_test[dist_type] = dict()\n",
    "    \n",
    "    # per ogni stock\n",
    "    for s_type in stock_type:\n",
    "        if dist_type == 'weibull':\n",
    "            theoretical_hazard_test[dist_type][s_type] = weibull_hazard(\n",
    "                x,\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['shape'],\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['scale'],\n",
    "            )\n",
    "        elif dist_type == 's-exp':\n",
    "            theoretical_hazard_test[dist_type][s_type] = sexp_hazard(\n",
    "                x,\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['shape'],\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['a'],\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['b'],\n",
    "            )\n",
    "        elif dist_type == 'q-exp':\n",
    "            theoretical_hazard_test[dist_type][s_type] = qexp_hazard(\n",
    "                x,\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['shape'],\n",
    "                best_params[dist_type][s_type][ret_type][q_type]['lambda'],\n",
    "            )\n",
    "\n",
    "empirical_hazard_test = {\n",
    "    s_type: np.array([\n",
    "        get_empirical_hazard_prob(recurrence_intervals_test[s_type][ret_type][q_type]['n_days'].values, t, 1)\n",
    "        for t in x\n",
    "    ])\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle righe train vs test, sulle colonne le due azioni\n",
    "fig, ax = pl.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "\n",
    "dist_colors = {\n",
    "    'weibull': 'orchid',\n",
    "    's-exp': 'orangered',\n",
    "    'q-exp': 'mediumblue'\n",
    "}\n",
    "\n",
    "dist_labels = {\n",
    "    'weibull': r'Weibull',\n",
    "    's-exp': r's-exp',\n",
    "    'q-exp': r'q-exp',\n",
    "}\n",
    "\n",
    "datasets = ['training', 'testing']\n",
    "\n",
    "for j, s_type in enumerate(stock_type):\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        if i == 0:\n",
    "            y_emp = empirical_hazard[s_type]\n",
    "        elif i == 1:\n",
    "            y_emp = empirical_hazard_test[s_type]\n",
    "        \n",
    "        for dist_type in distribution_type:\n",
    "            if i == 0:\n",
    "                y = theoretical_hazard[dist_type][s_type]\n",
    "            elif i == 1:\n",
    "                y = theoretical_hazard_test[dist_type][s_type]\n",
    "                \n",
    "            ax[i, j].plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=dist_colors[dist_type],\n",
    "                label=dist_labels[dist_type]\n",
    "            )\n",
    "            \n",
    "        ax[i, j].plot(\n",
    "            x,\n",
    "            y_emp,\n",
    "            label=r'Empirical',\n",
    "            color='black',\n",
    "            linestyle='-',\n",
    "            marker='o',\n",
    "            markersize=1,\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        \n",
    "        ax[i, j].legend(fontsize=14)\n",
    "        ax[i, j].set_xlabel(r'$t$', fontsize=16)\n",
    "        ax[i, j].set_ylabel(r'$W(1 | t)$', fontsize=16)\n",
    "        ax[i, j].set_title(f'Hazard prob for q99 | {s_type} | {ret_type} returns | {dataset}', fontsize=16)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Recall, FPR, KSS score, Precision al variare di $w_t$\n",
    "\n",
    "Vediamo ora come i valori di performance sul training e testing set, per il threshold ottimale $w_t^*$.\n",
    "\n",
    "Prima calcolo i risultati ottenuti con il threshold migliore sul training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------#\n",
    "# recall per w_t ottimale\n",
    "best_recalls = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: recalls[dist_type][s_type][ret_type][q_type][best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# fpr per w_t ottimale\n",
    "best_fprs = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: fprs[dist_type][s_type][ret_type][q_type][best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# kss per w_t ottimale\n",
    "best_ksss = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: ksss[dist_type][s_type][ret_type][q_type][best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# precision per w_t ottimale\n",
    "best_precisions = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: precisions[dist_type][s_type][ret_type][q_type][best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "#-----------------------------------#\n",
    "# utility per w_t ottimale\n",
    "best_utilities = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: {\n",
    "                q_type: utilities[dist_type][s_type][ret_type][q_type][best_indexes[dist_type][s_type][ret_type][q_type]]\n",
    "                for q_type in quantile_type\n",
    "            }\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi mi creo una funzione che crei la tabella, per una singola distribuzione, per un singolo tipo di return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_table(train_performance, test_performance, dist_type: str, s_type: str, ret_type: str):\n",
    "    \"\"\"Get a table similar to those on page 366 of the paper.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_performance: List[Dict]\n",
    "        performance on the train set, list of dicts with order \n",
    "        - recall\n",
    "        - fpr\n",
    "        - kss\n",
    "        - precision\n",
    "        - utility\n",
    "        each of which is a tree with first-level keys the return_type,\n",
    "        second-level keys the quantile_type\n",
    "    \n",
    "    test_performance: List[Dict]\n",
    "        performance on the test set, list of dicts with order \n",
    "        - recall\n",
    "        - fpr\n",
    "        - kss\n",
    "        - precision\n",
    "        - utility\n",
    "        each of which is a tree with first-level keys the return_type,\n",
    "        second-level keys the quantile_type\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    table: pd.DataFrame\n",
    "        a DataFrame with quantile_type on the columns, and rows\n",
    "        - fpr (in/out)\n",
    "        - recall (in/out)\n",
    "        - utility (in/out)\n",
    "        - kss (in/out)\n",
    "    \"\"\"\n",
    "    rec_train, fpr_train, kss_train, prec_train, u_train = train_performance\n",
    "    rec_test, fpr_test, kss_test, prec_test, u_test = test_performance\n",
    "    \n",
    "    columns = quantile_type\n",
    "    index = pd.Index([\n",
    "        'in: FPR', 'out: FPR', 'in: Recall', 'out: Recall',\n",
    "        'in: Prec', 'out: Prec',\n",
    "        'in: U', 'out: U', 'in: KSS', 'out: KSS',\n",
    "    ], name=dist_type)\n",
    "    \n",
    "    table = pd.DataFrame(\n",
    "        data=np.zeros((len(index), len(columns)), dtype=np.float64),\n",
    "        columns=columns,\n",
    "        index=index\n",
    "    )\n",
    "    \n",
    "    for q_type in quantile_type:\n",
    "        table.loc['in: FPR', q_type] = fpr_train[dist_type][s_type][ret_type][q_type]\n",
    "        table.loc['out: FPR', q_type] = fpr_test[dist_type][s_type][ret_type][q_type]\n",
    "        \n",
    "        table.loc['in: Recall', q_type] = rec_train[dist_type][s_type][ret_type][q_type]\n",
    "        table.loc['out: Recall', q_type] = rec_test[dist_type][s_type][ret_type][q_type]\n",
    "        \n",
    "        table.loc['in: Prec', q_type] = prec_train[dist_type][s_type][ret_type][q_type]\n",
    "        table.loc['out: Prec', q_type] = prec_test[dist_type][s_type][ret_type][q_type]\n",
    "        \n",
    "        table.loc['in: U', q_type] = u_train[dist_type][s_type][ret_type][q_type]\n",
    "        table.loc['out: U', q_type] = u_test[dist_type][s_type][ret_type][q_type]\n",
    "        \n",
    "        table.loc['in: KSS', q_type] = kss_train[dist_type][s_type][ret_type][q_type]\n",
    "        table.loc['out: KSS', q_type] = kss_test[dist_type][s_type][ret_type][q_type]\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e la uso per calcolare le tabelle per ogni distribuzione.\n",
    "\n",
    "Infine, concateno le tabelle per lo stesso tipo di return per ottenere le tabelle multi-dimensionali di pagina 366 del paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_train = {\n",
    "    dist_type: {\n",
    "        s_type: (\n",
    "            best_recalls[dist_type][s_type],\n",
    "            best_fprs[dist_type][s_type],\n",
    "            best_ksss[dist_type][s_type],\n",
    "            best_precisions[dist_type][s_type],\n",
    "            best_utilities[dist_type][s_type]\n",
    "        )\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "performance_test = {\n",
    "    dist_type: {\n",
    "        s_type: (\n",
    "            recalls_test[dist_type][s_type],\n",
    "            fprs_test[dist_type][s_type],\n",
    "            ksss_test[dist_type][s_type],\n",
    "            precisions_test[dist_type][s_type],\n",
    "            utilities_test[dist_type][s_type]\n",
    "        )\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "performance_train = (\n",
    "    best_recalls,\n",
    "    best_fprs,\n",
    "    best_ksss,\n",
    "    best_precisions,\n",
    "    best_utilities,\n",
    ")\n",
    "\n",
    "performance_test = (\n",
    "    recalls_test,\n",
    "    fprs_test,\n",
    "    ksss_test,\n",
    "    precisions_test,\n",
    "    utilities_test,\n",
    ")\n",
    "\n",
    "result_tables = {\n",
    "    dist_type: {\n",
    "        s_type: {\n",
    "            ret_type: get_results_table(\n",
    "                performance_train,\n",
    "                performance_test,\n",
    "                dist_type,\n",
    "                s_type,\n",
    "                ret_type\n",
    "            )\n",
    "            for ret_type in return_type\n",
    "        }\n",
    "        for s_type in stock_type\n",
    "    }\n",
    "    for dist_type in distribution_type\n",
    "}\n",
    "\n",
    "result_panels = {\n",
    "    s_type: {\n",
    "        ret_type: pd.concat([\n",
    "                result_tables[dist_type][s_type][ret_type] for dist_type in distribution_type\n",
    "            ],\n",
    "            axis='columns',\n",
    "            keys=distribution_type\n",
    "        )\n",
    "        for ret_type in return_type\n",
    "    }\n",
    "    for s_type in stock_type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusioni\n",
    "\n",
    "Abbiamo replicato il paper, stavolta sulle azioni S&P500, dove abbiamo preso due azioni, la meno volatile e la più volatile, dal 2005 ad oggi.\n",
    "\n",
    "Il criterio di inclusione in training o testing set è stato quello di includere nel training almeno metà delle due crisi successive al 2005, quella finanziaria del 2007-2008 e quella del debito dell'eurozona negli anni 2010-2014.\n",
    "\n",
    "È stata svolta una ottimizzazione per la ricerca del threshold ottimale per la *hazard probability* $W(\\Delta t | t)$, e trasformato quindi il problema in un problema di classificazione.\n",
    "\n",
    "Vediamo i risultati di questa analisi, cominciando dalla meno volatile.\n",
    "\n",
    "### 9.1 Azione meno volatile\n",
    "\n",
    "Vediamo i returns negativi per primi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['min_vol']['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che la recall è molto alta sia sul training che sul testing set, mentre la precision è molto bassa, attorno al 10%, per tutte e tre le distribuzioni. Si vede che il threshold della EVT non è adeguato.\n",
    "\n",
    "Vediamo allora sui returns positivi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['min_vol']['pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche qui, idem.\n",
    "\n",
    "Vediamo i returns assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['min_vol']['abs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lievemente più alte la precision per il quantile 95 e 97.5, ma comunque basse basse.\n",
    "\n",
    "### 9.2 Azione più volatile\n",
    "\n",
    "Vediamo come è andata sull'azione più volatile. Cominciamo anche qui dai returns negativi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['max_vol']['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le performance sono migliori, con una precision che oscilla tra il 10 ed il 22%, del tutto comparabile agli esperimenti con rete neurale che abbiamo già fatto. La Recall resta molto alta, attorno all'87%.\n",
    "\n",
    "Vediamo i returns positivi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['max_vol']['pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem, ma sul $q_{95}$ si raggiunge una precision del 25%.\n",
    "\n",
    "Infine quelli assoluti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_panels['max_vol']['abs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche qui idem, si tocca un massimo di 26% di recall.\n",
    "\n",
    "### 9.3 Plot precision-recall per le due azioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sulle colonne le azioni, sulle righe le distribuzioni\n",
    "fig, ax = pl.subplots(nrows=3, ncols=2, figsize=(20, 15))\n",
    "\n",
    "for j, s_type in enumerate(stock_type):\n",
    "    for i, dist_type in enumerate(distribution_type):\n",
    "        ax[i, j].scatter(\n",
    "            precisions[dist_type][s_type]['neg']['95'],\n",
    "            recalls[dist_type][s_type]['neg']['95'],\n",
    "            edgecolors='white',\n",
    "            color=dist_colors[dist_type],\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        ax[i, j].set_title(f\"{str.title(dist_type)} | {s_type}\", fontsize=16)\n",
    "        ax[i, j].set_xlim([0, 1.05])\n",
    "        ax[i, j].set_ylim([0, 1.05])\n",
    "    \n",
    "ax[-1, 0].set_xlabel('Precision', fontsize=14)\n",
    "ax[-1, 1].set_xlabel('Precision', fontsize=14)\n",
    "ax[0, 0].set_ylabel('Recall', fontsize=14)\n",
    "ax[1, 0].set_ylabel('Recall', fontsize=14)\n",
    "ax[2, 0].set_ylabel('Recall', fontsize=14)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo riflette esattamente lo stesso problema avuto con i primi esperimenti di Deep Learning.\n",
    "\n",
    "Tuttavia, in questo caso, si può agire sul threshold $\\theta$ della *loss function*, che va a pesare il ratio dei falsi allarmi rispetto a alla recall.\n",
    "In questo modo, si può privilegiare un più basso tasso di falsi allarmi e perdere qualche evento estremo.\n",
    "\n",
    "Le performance serviranno da base per il lavoro con il nuovo modello di Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSA NON FUNZIONA\n",
    "\n",
    "La EVT non dà un buon threshold, anzi finisce sotto la media dei corrispondenti returns. Purtroppo non so cosa farci."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
