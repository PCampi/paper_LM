{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ANN_LSTM_sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-VbbtgVOqGWb",
        "colab": {}
      },
      "source": [
        "ENV = 'colab'  # 'colab'\n",
        "if ENV == 'colab':\n",
        "    !pip install -q PyDrive imbalanced-learn ipdb hyperopt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-qdJhI2pqGWf"
      },
      "source": [
        "# Rete neurale per extreme returns su 2 azioni - con sentiment\n",
        "\n",
        "Questo notebook contiene la parte di rete neurale per confronto con l'analisi statistica. Qui faremo *solo* l'ottimizzazione degli iperparametri, l'addestramento finale con i parametri ottimali trovati sarà fatta in un altro notebook.\n",
        "\n",
        "Il flusso è il seguente:\n",
        "\n",
        "- [x] utilizzo del dataset *S&P500* con la massima ampiezza storica disponibile (2005 - 2018)\n",
        "- [x] calcolo dei log returns\n",
        "- [x] selezione di due stocks, quelle con la minima e la massima volatilità in nel training set considerato\n",
        "- [x] creazione estremi al 95%\n",
        "- [x] oversampling con due possibili strategie: replicare le istanze positive, o replicarle con aggiunta di rumore gaussiano\n",
        "- [x] addestramento rete con hyperparameter optimization\n",
        "- [x] ripetizione di ottimizzazione iperparametri per tutte e due le azioni con aggiunta di sentiment\n",
        "\n",
        "Nell'altro notebook dovrò fare:\n",
        "- [ ] utilizzo stesse metriche (ROC, KSS, Precision, Recall, Utility) che nel paper\n",
        "- [ ] confronto con i risultati del modello probabilistico\n",
        "- [ ] conclusioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yIWuJMSBqGWg",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "import pickle\n",
        "import copy\n",
        "import pprint\n",
        "import uuid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas.testing as pt\n",
        "import sklearn.metrics as sm\n",
        "import sklearn.preprocessing as skpp\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import hyperopt as hy\n",
        "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
        "\n",
        "import keras\n",
        "\n",
        "import matplotlib.pyplot as pl\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "import seaborn as sns\n",
        "\n",
        "import ipdb\n",
        "\n",
        "%pdb on"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ImRJwq-Fq4aj",
        "colab": {}
      },
      "source": [
        "if ENV == 'colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "joUuYwuYqGWj"
      },
      "source": [
        "Un po' di dichiarazioni utili per il seguito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRPdSilJqGWk",
        "colab": {}
      },
      "source": [
        "stock_type = ['min_vol', 'max_vol']\n",
        "return_type = ['pos', 'neg', 'abs']\n",
        "q_type = '95'\n",
        "rs = 42  # random state\n",
        "MAX_EPOCHS = 1000\n",
        "\n",
        "stock_codes = {\n",
        "    'min_vol': '9CE4C7',\n",
        "    'max_vol': 'E28F22'\n",
        "}  # già trovate in Paper-azioni.ipynb\n",
        "\n",
        "stock_colors = {\n",
        "    'min_vol': 'palegoldenrod',\n",
        "    'max_vol': 'coral',\n",
        "}\n",
        "\n",
        "# i giorni sono i primi disponibili in quel mese nei dati\n",
        "split_dates = {\n",
        "    'subprime-crisis': datetime.datetime(2007, 1, 3), # subprime crisis\n",
        "    'subprime-crisis-start': datetime.datetime(2007, 1, 3), # subprime crisis\n",
        "    'subprime-crisis-halfway': datetime.datetime(2008, 9, 2),\n",
        "    'subprime-crisis-end': datetime.datetime(2010, 1, 4),\n",
        "    'eu-debt': datetime.datetime(2011, 1, 3), # EU sovereign debt crisis\n",
        "    'eu-debt-halfway': datetime.datetime(2012, 1, 3), # EU sovereign debt crisis\n",
        "    'last_train': datetime.datetime(2017, 1, 3), \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIgaOP67qGWm"
      },
      "source": [
        "## 1. Importazione dei dati \n",
        "\n",
        "Per importare i dati dobbiamo caricarli, e poi usare la stategia \"taglia-e-cuci\" usata in `Paper-azioni.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R3FzgUr5qGWn",
        "colab": {}
      },
      "source": [
        "if ENV == 'colab':\n",
        "    data_path = '/gdrive/My Drive/OptiRisk Thesis/data'\n",
        "    base_path = '/gdrive/My Drive/OptiRisk Thesis/experiments/11_final_experiment'\n",
        "else:\n",
        "    data_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/data\"\n",
        "    base_path = \"/Users/pietro/Google Drive/OptiRisk Thesis/experiments/11_final_experiment\"\n",
        "\n",
        "prices_path = os.path.join(data_path, 'prices', 'adjusted_prices_volume.csv')\n",
        "ta_dir = os.path.join(data_path, 'technical_features', 'features_all_years')\n",
        "impact_path = os.path.join(data_path, 'sentiment', 'impactFinal.csv')\n",
        "print(f\"BASE path: {base_path}\")\n",
        "print(f\"TA dir: {ta_dir}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TsmulRB3qGWp"
      },
      "source": [
        "Conversione delle date e settaggio dell'index del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aX80lWk8qGWq",
        "colab": {}
      },
      "source": [
        "prices = pd.read_csv(prices_path)\n",
        "prices.loc[:, 'date'] = pd.to_datetime(prices['date'], format=\"%Y%m%d\")\n",
        "prices.index = prices['date']\n",
        "prices.drop(columns=['date'], inplace=True)\n",
        "prices.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VlBiVnJIqGWs"
      },
      "source": [
        "Trasformiamola un una serie temporale, ogni riga una data, ogni colonna un'azione.\n",
        "\n",
        "I prezzi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HyNlWyFnqGWt",
        "colab": {}
      },
      "source": [
        "prices_ts = prices.pivot(columns='ravenpackId', values='close')\n",
        "prices_ts_no_nan = prices_ts.dropna(axis='columns', how='any', inplace=False)\n",
        "prices_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B-TCT_OnqGWv"
      },
      "source": [
        "I volumi:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EegGob7qGWv",
        "colab": {}
      },
      "source": [
        "volume_ts = prices.pivot(columns='ravenpackId', values='volume')\n",
        "volume_ts_no_nan = volume_ts.loc[:, prices_ts_no_nan.columns]\n",
        "pt.assert_index_equal(prices_ts_no_nan.columns, volume_ts_no_nan.columns, check_names=False)\n",
        "volume_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hNWkwALBDN",
        "colab_type": "text"
      },
      "source": [
        "Ora carico l'impact score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGVl7dDLBDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "impact = pd.read_csv(impact_path)\n",
        "impact.loc[:, 'date'] = pd.to_datetime(impact['date'], format=\"%Y%m%d\")\n",
        "impact.index = impact['date']\n",
        "impact.drop(columns=['date'], inplace=True)\n",
        "impact.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg6LiL7tLBDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "impact_ts = impact.pivot(columns='companyId', values=['PosImpact', 'NegImpact'])\n",
        "impact_ts_no_nan = impact_ts.dropna(axis='columns', how='any', inplace=False)\n",
        "impact_ts_no_nan.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D11Ov9SBqGWy"
      },
      "source": [
        "Ora calcoliamo i log-returns, le direzioni, i volumi e gli impact:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-hSvXqxqGW0",
        "colab": {}
      },
      "source": [
        "log_returns = np.log(prices_ts_no_nan).diff(periods=1).iloc[1:, :]\n",
        "directions_ts_no_nan = prices_ts_no_nan.diff(periods=1).iloc[1:, :]\n",
        "prices_ts_no_nan = prices_ts_no_nan.iloc[1:, :]\n",
        "volume_ts_no_nan = volume_ts_no_nan.iloc[1:, :]\n",
        "\n",
        "impact_ts_no_nan = impact_ts_no_nan.loc[prices_ts_no_nan.index]\n",
        "\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, volume_ts_no_nan.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, log_returns.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, directions_ts_no_nan.index)\n",
        "pt.assert_index_equal(prices_ts_no_nan.index, impact_ts_no_nan.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h3N3bQbJkr3c"
      },
      "source": [
        "Mi conviene creare una funzione che standardizzi le features, visto che poi ne avrò più di una (es: returns + volume)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rqZ2KaSJk2Kt",
        "colab": {}
      },
      "source": [
        "def only_train_notime(feature: pd.Series) -> pd.Series:\n",
        "    \"\"\"Just return the training part of a Series.\"\"\"\n",
        "    f = feature[np.logical_or(\n",
        "        feature.index < split_dates['subprime-crisis-halfway'],\n",
        "        np.logical_and(\n",
        "            feature.index >= split_dates['eu-debt-halfway'],\n",
        "            feature.index < split_dates['last_train']\n",
        "        )\n",
        "    )]\n",
        "\n",
        "    return f\n",
        "\n",
        "def standardize(feature: pd.Series) -> pd.Series:\n",
        "    \"\"\"Standardize a feature by computing the statistics on the training set.\"\"\"\n",
        "    # prendo solo la parte di training, perdendo ogni riferimento alla\n",
        "    # sequenza temporale\n",
        "    tmp_feature_train = only_train_notime(feature)\n",
        "\n",
        "    scaler = skpp.RobustScaler()\n",
        "    scaler.fit(tmp_feature_train.values.reshape(-1, 1))\n",
        "\n",
        "    result = pd.Series(\n",
        "        data=scaler.transform(feature.values.reshape(-1, 1)).flatten(),\n",
        "        index=feature.index\n",
        "    )\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVoCHZZurG4j"
      },
      "source": [
        "Ora creo i thresholds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soghP5HhqGW3",
        "colab": {}
      },
      "source": [
        "# ora creo i dati per i returns (non standardizzati), i thresholds e i volumi (standardizzati)\n",
        "lr_train_notime = dict()\n",
        "lr_test_notime = dict()\n",
        "returns_train_notime = dict()\n",
        "\n",
        "# aggiungiamo i dati in modalità taglia-e-cuci\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    # training set\n",
        "    lr_current = log_returns.loc[:, s_code]\n",
        "    lr_train_notime[s_type] = only_train_notime(lr_current)\n",
        "    \n",
        "    # returns train, tutti POSITIVI\n",
        "    returns_train_notime[s_type] = {\n",
        "        'pos': lr_train_notime[s_type][lr_train_notime[s_type] > 0.0],\n",
        "        'neg': -(lr_train_notime[s_type][lr_train_notime[s_type] < 0.0]),\n",
        "        'abs': lr_train_notime[s_type].abs()\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "# ora creo i threshold\n",
        "thresholds = {\n",
        "    s_type: {\n",
        "        ret_type: {\n",
        "            q_type: returns_train_notime[s_type][ret_type].quantile(0.95)\n",
        "        }\n",
        "        for ret_type in return_type\n",
        "    }\n",
        "    for s_type in stock_type\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lr51ts-sjf6b"
      },
      "source": [
        "ed infine creo i DataFrame e gli arrays che contengono tutti gli estremi e tutti i dati.\n",
        "\n",
        "Le features che qui utilizziamo sono:\n",
        "\n",
        "- log-returns standardizzati\n",
        "- volume scambiato standardizzato\n",
        "- tutte le features di TA che ci sono nel white paper di Douglas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzG8axsrrSSa",
        "colab": {}
      },
      "source": [
        "feature_names = [\n",
        "    'adx', 'aroon_down', 'aroon_up', 'atr', 'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'cci', 'cmo', 'ema5', 'ema10', 'ema15', 'macd', 'rsi', 'sma5', 'sma10', 'sma15',\n",
        "]\n",
        "\n",
        "feature_paths = [os.path.join(ta_dir, name + '.h5') for name in feature_names]\n",
        "\n",
        "features = dict()\n",
        "first_allowable_dates = dict()  # date in cui posso prendere le feature e i returns\n",
        "\n",
        "to_standardize = {\n",
        "    'sma5', 'sma10', 'sma15',\n",
        "    'ema5', 'ema10', 'ema15',\n",
        "    'macd',\n",
        "    'bb_lower', 'bb_middle', 'bb_upper',\n",
        "    'roc', 'atr', 'cci', 'adx',\n",
        "    }\n",
        "\n",
        "to_divide = {\n",
        "    'rsi': 100.0,\n",
        "    'aroon_down': 100.0,\n",
        "    'aroon_up': 100.0,\n",
        "    'cmo': 100.0,\n",
        "}\n",
        "\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    print(f\"Stock type: {s_type}\")\n",
        "    print(\"-\"*30)\n",
        "    features[s_type] = dict()\n",
        "\n",
        "    for feature_name, feature_path in zip(feature_names, feature_paths):\n",
        "        feature = pd.read_hdf(feature_path)\n",
        "\n",
        "        if feature_name in to_standardize:\n",
        "            print(f\"standardizing {feature_name}\")\n",
        "            feature_transformed = standardize(feature.loc[:, s_code])\n",
        "            features[s_type][feature_name] = feature_transformed\n",
        "        elif feature_name in to_divide.keys():\n",
        "            print(f\"dividing {feature_name}\")\n",
        "            features[s_type][feature_name] = feature.loc[:, s_code] / to_divide[feature_name]\n",
        "        else:\n",
        "            raise ValueError(f\"unknown feature {feature_name}\")\n",
        "\n",
        "    # impact positive e negative\n",
        "    print(\"adding positive and negative sentiment impact\")\n",
        "    features[s_type]['pos_impact'] = impact_ts_no_nan.loc[:, ('PosImpact', s_code)]\n",
        "    features[s_type]['neg_impact'] = impact_ts_no_nan.loc[:, ('NegImpact', s_code)]\n",
        "    \n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90lESpICqGW9",
        "colab": {}
      },
      "source": [
        "extremes_all = dict()  # keys: s_type, q_type\n",
        "data_all = dict()  # keys: s_type\n",
        "volumes = dict()  # keys: s_type\n",
        "directions_all = dict()  # keys: s_type\n",
        "\n",
        "for s_type, s_code in stock_codes.items():\n",
        "    # i returns\n",
        "    lr = log_returns.loc[:, s_code]\n",
        "    lr_transformed = standardize(lr)\n",
        "\n",
        "    # i volumi\n",
        "    stock_volume = volume_ts_no_nan.loc[:, s_code]\n",
        "    volume_transformed = standardize(stock_volume)\n",
        "    volumes[s_type] = volume_transformed\n",
        "\n",
        "    # le features tecniche\n",
        "    all_features = [lr_transformed, volume_transformed] + \\\n",
        "              [features[s_type][name] for name in feature_names] + \\\n",
        "              [features[s_type]['pos_impact'], features[s_type]['neg_impact']]\n",
        "\n",
        "    # tutte le features in un unico DataFrame\n",
        "    tmp_df = pd.concat(\n",
        "        all_features,\n",
        "        axis=1,\n",
        "        keys=['log_return', 'volume'] + feature_names + ['pos_impact', 'neg_impact']\n",
        "    )\n",
        "\n",
        "    tmp_df = tmp_df.dropna(axis='index', how='any')\n",
        "    \n",
        "    data_all[s_type] = tmp_df\n",
        "    extremes_all[s_type] = dict()\n",
        "    \n",
        "    ext = np.logical_or(\n",
        "        lr >= thresholds[s_type]['pos'][q_type],\n",
        "        lr <= -thresholds[s_type]['neg'][q_type],\n",
        "    )\n",
        "    \n",
        "    extremes_all[s_type][q_type] = pd.Series(data=ext, index=log_returns.index)\n",
        "    \n",
        "    # le direzioni\n",
        "    direction = (directions_ts_no_nan.loc[:, s_code] > 0.0).astype(np.int8)\n",
        "    directions_all[s_type] = direction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SbKxpt4zqGW8"
      },
      "source": [
        "## 2. Creazione dataset train-test per TensorFlow\n",
        "\n",
        "Ora che ho i thresholds, posso creare il dataset vero e proprio, cioè:\n",
        "\n",
        "- X: cubo dati\n",
        "- y: estremo si/no\n",
        "\n",
        "Per prima cosa, creo delle funzioni che mi creano i dati:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sg7SbAHBqGXA",
        "colab": {}
      },
      "source": [
        "# testata, funziona con array, Series e DataFrame\n",
        "def rolling_window(data: np.ndarray,\n",
        "                   start: int,\n",
        "                   end: int,\n",
        "                   lookback: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a rolling window view of data, starting at index start, finishing\n",
        "    at index end, with loockback days of bptt.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: series, dataframe or array\n",
        "        the data, containing one row for each time point and one column for each feature\n",
        "        \n",
        "    start: int\n",
        "        starting index in the data\n",
        "        \n",
        "    end: int\n",
        "        index where the whole thing ends, data[end] is **excluded**\n",
        "        \n",
        "    lookback: int\n",
        "        length of the lookback period\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    X: np.ndarray\n",
        "        array of shape(n_points, lookback, n_features)\n",
        "    \"\"\"\n",
        "    assert lookback < data.shape[0]  # lookback sano\n",
        "    assert start - lookback + 1 >= 0  # lookback sano\n",
        "    \n",
        "    n_features = data.shape[1]\n",
        "    n_points = end - start\n",
        "    \n",
        "    X = np.zeros((n_points, lookback, n_features), dtype = data.dtype)\n",
        "    \n",
        "    # range strano per l'indicizzazione numpy\n",
        "    for i, t in enumerate(range(start + 1, end + 1)):\n",
        "        X[i, :, :] = data[t - lookback:t, :]\n",
        "        \n",
        "    return X\n",
        "\n",
        "\n",
        "# testata, funziona hehehe\n",
        "def rolling_window_xyd(data: Union[pd.Series, pd.DataFrame],\n",
        "                      targets: List[pd.Series],\n",
        "                      start: int,\n",
        "                      end: int,\n",
        "                      lookback: int) -> Tuple[np.ndarray, List[np.ndarray], pd.Series]:\n",
        "    \"\"\"\n",
        "    Create X, y and dates in a single shot.\n",
        "    The returned dates are relative to the y array(s).\n",
        "    \"\"\"\n",
        "    if isinstance(data, pd.Series):\n",
        "        my_data = data.values.reshape(-1, 1)\n",
        "    elif isinstance(data, pd.DataFrame):\n",
        "        my_data = data.values\n",
        "    else:\n",
        "        raise TypeError(\"data should be a pandas Series or Dataframe\")\n",
        "\n",
        "    X = rolling_window(my_data, start, end, lookback)\n",
        "    \n",
        "    if not isinstance(targets, list):\n",
        "        raise TypeError(\"target must be a list of pandas Series\")\n",
        "    if not all(isinstance(t, pd.Series) for t in targets):\n",
        "        raise TypeError(\"all targets should be pandas Series\")\n",
        "    if not all(isinstance(t.index, pd.DatetimeIndex) for t in targets):\n",
        "        raise TypeError(\"index of target should be a pandas DatetimeIndex\")\n",
        "        \n",
        "    y = [t.values[start + 1:end + 1] for t in targets]\n",
        "    dates = pd.Series(data=targets[0].index[start + 1: end + 1])\n",
        "        \n",
        "    return X, y, dates\n",
        "\n",
        "\n",
        "# TESTATO: funziona\n",
        "def create_Xyd(returns: Union[pd.Series, pd.DataFrame],\n",
        "               extremes: pd.Series,\n",
        "               directions: pd.Series,\n",
        "               lookback: int) -> Tuple[\n",
        "    np.ndarray, np.ndarray, List[np.ndarray], List[np.ndarray], pd.Series, pd.Series\n",
        "]:\n",
        "    \"\"\"\n",
        "    Create the X, y and dates arrays for the ANN.\n",
        "    \"\"\"\n",
        "    test_start_1 = returns.index.get_loc(split_dates['subprime-crisis-halfway'])\n",
        "    test_end_1 = returns.index.get_loc(split_dates['eu-debt-halfway'])\n",
        "    test_start_2 = returns.index.get_loc(split_dates['last_train'])\n",
        "\n",
        "    # TRAIN\n",
        "    tmp_X_train_1, tmp_y_train_1, tmp_dates_train_1 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=lookback - 1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_start_1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "\n",
        "    tmp_X_train_2, tmp_y_train_2, tmp_dates_train_2 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_end_1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_start_2,\n",
        "        lookback=lookback\n",
        "    )\n",
        "    \n",
        "    assert len(tmp_y_train_1) == len(tmp_y_train_2)\n",
        "    \n",
        "    X_train = np.concatenate([tmp_X_train_1, tmp_X_train_2])\n",
        "    y_train = [np.concatenate([tmp_y_train_1[i], tmp_y_train_2[i]]) for i in range(len(tmp_y_train_1))]\n",
        "    dates_train = pd.concat([tmp_dates_train_1, tmp_dates_train_2], axis=0, ignore_index=True).values\n",
        "    assert X_train.shape[0] == dates_train.shape[0]\n",
        "    assert all(yy.shape[0] == X_train.shape[0] for yy in y_train)\n",
        "\n",
        "    # TEST\n",
        "    tmp_X_test_1, tmp_y_test_1, tmp_dates_test_1 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_start_1,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=test_end_1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "    \n",
        "    tmp_X_test_2, tmp_y_test_2, tmp_dates_test_2 = rolling_window_xyd(\n",
        "        returns,\n",
        "        [extremes, directions],\n",
        "        start=test_start_2,  # sempre lookback - 1 se il primo iniziale\n",
        "        end=returns.shape[0] - 1,\n",
        "        lookback=lookback\n",
        "    )\n",
        "  \n",
        "    X_test = np.concatenate([tmp_X_test_1, tmp_X_test_2])\n",
        "    y_test = [np.concatenate([tmp_y_test_1[i], tmp_y_test_2[i]]) for i in range(len(tmp_y_test_1))]\n",
        "    dates_test = pd.concat([tmp_dates_test_1, tmp_dates_test_2], axis=0, ignore_index=True).values\n",
        "    assert X_test.shape[0] == dates_test.shape[0]\n",
        "    assert all(yy.shape[0] == X_test.shape[0] for yy in y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "\n",
        "\n",
        "def split_stratified(X: np.ndarray,\n",
        "                     y: List[np.ndarray],\n",
        "                     dates: np.ndarray,\n",
        "                     test_size=0.2,\n",
        "                     random_state=rs,\n",
        "                     verbose=False):\n",
        "    \"\"\"\n",
        "    Split a dataset in a stratified fashion on the target variable y[0].\n",
        "    \"\"\"\n",
        "    assert X.ndim == 3\n",
        "    # divido in train-validation, lo faccio prendendo gli indici dagli estremi y/n con un\n",
        "    # ShuffleSplit che divide a caso\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    \n",
        "    n_samples = X.shape[0]\n",
        "    n_features = X.shape[2]\n",
        "    \n",
        "    XX = np.zeros(n_samples, dtype=np.int8)\n",
        "    \n",
        "    if verbose:\n",
        "        for i in range(len(y)):\n",
        "            vals, counts = np.unique(y[i], return_counts=True)\n",
        "            for v, c in zip(vals, counts):\n",
        "                print(f\"y[{i}] has {c} elements of class {v}\")\n",
        "    \n",
        "    train_index, test_index = next(splitter.split(XX, y[0]))\n",
        "    \n",
        "    X_train = X[train_index]\n",
        "    X_validation = X[test_index]\n",
        "    \n",
        "    y_train = [yy[train_index] for yy in y]\n",
        "    y_validation = [yy[test_index] for yy in y]\n",
        "    \n",
        "    dates_train = dates[train_index]\n",
        "    dates_validation = dates[test_index]\n",
        "\n",
        "    return X_train, X_validation, y_train, y_validation, dates_train, dates_validation\n",
        "\n",
        "\n",
        "def oversample_mtl(X: np.ndarray, y: List[np.ndarray], random_state=rs, dt=np.float32):\n",
        "    \"\"\"Oversample a dataset on the positive 1 class.\"\"\"\n",
        "    assert X.dtype == dt\n",
        "    assert X.ndim == 3\n",
        "    assert isinstance(y, list) and all(yy.ndim == 1 for yy in y) and all(yy.dtype == dt for yy in y)\n",
        "    \n",
        "    # oversample\n",
        "    ro = RandomOverSampler(random_state=random_state)\n",
        "    nx = X.shape[0]\n",
        "    indexes = np.arange(nx).reshape(nx, 1)\n",
        "    \n",
        "    indexes_resampled, y_resampled = ro.fit_resample(indexes, y[0])\n",
        "    ir = indexes_resampled.flatten()\n",
        "    \n",
        "    X_resampled = X[ir]\n",
        "    y_resampled = [yy[ir] for yy in y]\n",
        "    \n",
        "    return X_resampled, y_resampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UVy3gkvqGXG"
      },
      "source": [
        "## 3. Addestramento rete\n",
        "\n",
        "Visto che serve l'ottimizzazione degli iperparametri, è importante avere una funzione da ottimizzare con Hyperopt. Aggiungiamo le funzioni che servono.\n",
        "\n",
        "La prima crea il modello:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gyNkr3CjOH6t",
        "colab": {}
      },
      "source": [
        "def create_model_mtl(space: Dict[str, Any],\n",
        "                     bptt: int,\n",
        "                     n_features: int) -> keras.models.Model:\n",
        "    \"\"\"Create a model using the parameters in the search space.\"\"\"\n",
        "    l = space['layers']\n",
        "\n",
        "    input_dropout = float(l['input_dropout'])\n",
        "    assert input_dropout >= 0.0 and input_dropout <= 1.0\n",
        "\n",
        "    n_layers = int(l['num_layers']['how_many'])\n",
        "    assert n_layers <= 2 and n_layers > 0\n",
        "\n",
        "#     n_cells_1 = int(l['num_layers']['n_cells_1'])\n",
        "#     assert n_cells_1 >= 1\n",
        "\n",
        "    # creo il modello\n",
        "    model_input = keras.Input(shape=(bptt, n_features), name='model_input')\n",
        "\n",
        "    if n_layers == 1:\n",
        "        if input_dropout > 0.0:\n",
        "            x = keras.layers.LSTM(n_features, dropout=input_dropout)(model_input)\n",
        "        else:\n",
        "            x = keras.layers.LSTM(n_features)(model_input)\n",
        "    elif n_layers == 2:\n",
        "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
        "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
        "        x = keras.layers.LSTM(n_cells_2)(x)\n",
        "    elif n_layers == 3:\n",
        "        n_cells_2 = int(l['num_layers']['n_cells_2'])\n",
        "        n_cells_3 = int(l['num_layers']['n_cells_3'])\n",
        "        x = keras.layers.LSTM(n_features, return_sequences=True)(model_input)\n",
        "        x = keras.layers.LSTM(n_cells_2, return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(n_cells_3)(x)\n",
        "\n",
        "    output_is_extreme = keras.layers.Dense(\n",
        "        2, activation='softmax', name='extreme')(x)\n",
        "    output_is_up_down = keras.layers.Dense(\n",
        "        2, activation='softmax', name='up_down')(x)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=model_input,\n",
        "        outputs=[output_is_extreme, output_is_up_down],\n",
        "        name='MTL_model')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WNGl_yGeOH6v"
      },
      "source": [
        "La seconda valuta le performance e trova i TP, FP, TN, FN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFpAsD-WOH6v",
        "colab": {}
      },
      "source": [
        "def get_tptnfpfn(y_true, y_pred, labels=[0, 1]) -> Dict[str, int]:\n",
        "    \"\"\"Get the tp, tn, fp, fn count for y_true and y_pred.\"\"\"\n",
        "    cm = sm.confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    tp = cm[1, 1]\n",
        "    tn = cm[0, 0]\n",
        "    fp = cm[0, 1]\n",
        "    fn = cm[1, 0]\n",
        "\n",
        "    return {\n",
        "        'tp': tp,\n",
        "        'fp': fp,\n",
        "        'tn': tn,\n",
        "        'fn': fn,\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_performance(tp, tn, fp, fn, weighted=False):\n",
        "    support_1 = tp + fn\n",
        "    support_0 = tn + fp\n",
        "    \n",
        "    if tp + fp > 0:\n",
        "        prec_1 = tp / (tp + fp)\n",
        "    else:\n",
        "        prec_1 = np.nan\n",
        "        \n",
        "    if tn + fn > 0:\n",
        "        prec_0 = tn / (tn + fn)\n",
        "    else:\n",
        "        prec_0 = np.nan\n",
        "    \n",
        "    rec_1 = tp / (tp + fn)\n",
        "    rec_0 = tn / (tn + fp)\n",
        "    \n",
        "    if any(np.isnan(x) for x in [prec_1, prec_0, rec_1, rec_0]):\n",
        "        return np.nan\n",
        "    else:\n",
        "        f1_1 = 2.0 * (prec_1 * rec_1) / (prec_1 + rec_1)\n",
        "        f1_0 = 2.0 * (prec_0 * rec_0) / (prec_0 + rec_0)\n",
        "        \n",
        "        if weighted: # pesa di più la classe di maggioranza\n",
        "            f1_tot = np.average([f1_0, f1_1], weights=[support_0, support_1])\n",
        "        else:\n",
        "            f1_tot = np.mean([f1_0, f1_1])\n",
        "            \n",
        "        return f1_tot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z5c6x_4IOH6y"
      },
      "source": [
        "La terza mi crea il dataframe dei risultati che poi posso analizzare:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OiCeLJ4lOH60",
        "colab": {}
      },
      "source": [
        "def get_new_results() -> pd.DataFrame:\n",
        "    return pd.DataFrame(\n",
        "        data=None,\n",
        "        columns=[\n",
        "            'dataset',\n",
        "            'optimizer',\n",
        "            'start_time',\n",
        "            'experiment_id',\n",
        "            'trial',\n",
        "            'bptt',\n",
        "            'lr',\n",
        "            'n_layers',\n",
        "            'n_cells_1',\n",
        "            'n_cells_2',\n",
        "            'n_cells_3',\n",
        "            'dropout',\n",
        "            'loss_extreme',\n",
        "            'loss_up_down',\n",
        "            'loss_volume',\n",
        "            'f1_validation_extreme',\n",
        "            'f1_validation_up_down',\n",
        "            'tp_extreme',\n",
        "            'tn_extreme',\n",
        "            'fp_extreme',\n",
        "            'fn_extreme',\n",
        "            'tp_up_down',\n",
        "            'tn_up_down',\n",
        "            'fp_up_down',\n",
        "            'fn_up_down',\n",
        "            'train_epochs',\n",
        "            'es_min_delta',\n",
        "            'es_patience',\n",
        "            'use_class_weight',\n",
        "            'output',\n",
        "        ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rEkG4X9sOH63"
      },
      "source": [
        "Infine l'ultima esegue l'esperimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WR-j2l13OH64",
        "colab": {}
      },
      "source": [
        "def run_experiment(space,\n",
        "                   stock_type: str,\n",
        "                   max_epochs: int,\n",
        "                   data: Union[pd.Series, pd.DataFrame],\n",
        "                   extremes: pd.Series,\n",
        "                   directions: pd.Series,\n",
        "                   n_runs: int,\n",
        "                   verbose: int,\n",
        "                   results_path: str):\n",
        "    \"\"\"Fai girare un singolo esperimento.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    space: \n",
        "        hyperopt search space\n",
        "        \n",
        "    stock_type:\n",
        "        type of stock to use\n",
        "    \n",
        "    max_epochs: int\n",
        "        number of max epochs to tun the model for\n",
        "        \n",
        "    data: pd.Series of shape (n_timepoints,), or pd.DataFrame of shape (n_timepoints, n_features)\n",
        "        data containing returns, volume and all other things, where every row is\n",
        "        a timepoint and every column a different feature\n",
        "        \n",
        "    extremes: pd.Series of shape (n_timepoints,)\n",
        "        target for the extremes, binary 1/0, \n",
        "        \n",
        "    directions: pd.Series of shape (n_timepoints,)\n",
        "        target for the directions\n",
        "        \n",
        "    n_runs: int\n",
        "        how many times to run a model with the same structure to get a reliable\n",
        "        estimate of the loss function\n",
        "        \n",
        "    verbose: int\n",
        "        verbosity for Keras\n",
        "        \n",
        "    results_path: str or path\n",
        "        path where to save the intermediate runs\n",
        "    \"\"\"\n",
        "    sigmoid_or_softmax = 'softmax'\n",
        "    if data.ndim == 1:\n",
        "        n_features = 1\n",
        "    else:\n",
        "        n_features = data.shape[1]\n",
        "\n",
        "    lookback = bptt = int(space['bptt'])\n",
        "    batch_size = data.shape[0]\n",
        "    \n",
        "    # 1. creazione dataset per questo lookback\n",
        "    X_trv, X_test, y_trv, y_test, dates_trv, dates_test = create_Xyd(\n",
        "        data.astype(np.float32),\n",
        "        extremes.astype(np.float32),\n",
        "        directions.astype(np.float32),\n",
        "        lookback=lookback\n",
        "    )\n",
        "\n",
        "    # divido in train-validation\n",
        "    X_train, X_validation, y_train, y_validation, dates_train, dates_validation = split_stratified(\n",
        "        X_trv,\n",
        "        y_trv,\n",
        "        dates_trv,\n",
        "        test_size=0.2,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # bilancio con oversampling della classe di minoranza (1)\n",
        "    X_train_bal, y_train_bal = oversample_mtl(X_train, y_train)  # bal = balanced\n",
        "    \n",
        "    # 2. creo il file dei risultati\n",
        "    if not os.path.exists(results_path):\n",
        "        print(f\"Creating new result file at {results_path}\")\n",
        "        results = get_new_results()\n",
        "    else:\n",
        "        print(f\"Loading result file at {results_path}\")\n",
        "        results = pd.read_csv(results_path)\n",
        "\n",
        "    # 3. creo le variabili che servono per il training (dati e parametri)\n",
        "    print(space)\n",
        "    try:\n",
        "        use_class_weight = space['use_class_weight']\n",
        "        if use_class_weight:\n",
        "            print(\"Using class weight for training\")\n",
        "    except KeyError:\n",
        "        use_class_weight = False\n",
        "        \n",
        "    \n",
        "    y_train_bal_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_train_bal]\n",
        "    y_validation_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_validation]\n",
        "    y_test_cat = [keras.utils.to_categorical(yy, num_classes=2) for yy in y_test]\n",
        "\n",
        "    # 4. inizializza le loss a 0 e crea i tempi di inizio e l'id esperimento\n",
        "    train_losses: List[float] = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    optimizer_name = space['optimizer']['name']\n",
        "    assert optimizer_name in {'adam', 'adadelta'}\n",
        "\n",
        "    start_time = int(round(time.time()))\n",
        "    experiment_id = str(uuid.uuid4())\n",
        "\n",
        "    # 5. addestra e testa il modello per n_runs volte\n",
        "    for i in range(n_runs):\n",
        "        model = create_model_mtl(space, lookback, n_features)\n",
        "\n",
        "        # 5.1 crea l'optimizer\n",
        "        if optimizer_name == 'adam':\n",
        "            learning_rate = space['optimizer']['lr']\n",
        "            optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "        elif optimizer_name == 'adadelta':\n",
        "            optimizer = 'adadelta'\n",
        "            learning_rate = 1.0\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid optimizer name {optimizer_name}\")\n",
        "\n",
        "        if i == 0:\n",
        "            print(\n",
        "                f\"Hyper parameters: bptt={bptt}, optimizer={optimizer_name}, learning_rate={learning_rate}\"\n",
        "            )\n",
        "            model.summary()\n",
        "\n",
        "        # 5.2 compila il modello\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
        "        )\n",
        "\n",
        "        # 5.3 parametri per l'Early Stopping\n",
        "        min_delta = float(space['early_stop']['min_delta'])\n",
        "        patience = int(space['early_stop']['patience'])\n",
        "\n",
        "        early_stop_cb = keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            min_delta=min_delta,\n",
        "            patience=patience,\n",
        "            restore_best_weights=True)\n",
        "\n",
        "        print(f\"Iteration {i}\")\n",
        "        print(\"Fitting model\")\n",
        "\n",
        "#         if use_class_weight:\n",
        "#             print(f\"Class weights: {class_weights}\")\n",
        "#             history: keras.callbacks.History = model.fit(\n",
        "#                 x=X_train,\n",
        "#                 y=y_t,\n",
        "#                 epochs=max_epochs,\n",
        "#                 batch_size=batch_size,\n",
        "#                 validation_data=(X_validation, y_v),\n",
        "#                 callbacks=[early_stop_cb],\n",
        "#                 shuffle=True,\n",
        "#                 verbose=verbose,\n",
        "#                 class_weight=class_weights)\n",
        "#         else:\n",
        "#             history: keras.callbacks.History = model.fit(  # type: ignore\n",
        "#                 x=X_train,\n",
        "#                 y=y_t,\n",
        "#                 epochs=max_epochs,\n",
        "#                 batch_size=batch_size,\n",
        "#                 validation_data=(X_validation, y_v),\n",
        "#                 callbacks=[early_stop_cb],\n",
        "#                 shuffle=True,\n",
        "#                 verbose=verbose)\n",
        "\n",
        "        # 5.4 addestra il modello\n",
        "        history: keras.callbacks.History = model.fit(  # type: ignore\n",
        "            x=X_train_bal,\n",
        "            y=y_train_bal_cat,\n",
        "            epochs=max_epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_validation, y_validation_cat),\n",
        "            callbacks=[early_stop_cb],\n",
        "            shuffle=True,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # 5.5 valutalo su training e validation\n",
        "        train_epochs = len(history.history['loss'])\n",
        "\n",
        "        min_index = np.argmin(history.history['val_loss'])\n",
        "\n",
        "        curr_train_loss_total = history.history['loss'][min_index]\n",
        "        curr_train_loss_extreme = history.history['extreme_loss'][min_index]\n",
        "        curr_train_loss_up_down = history.history['up_down_loss'][min_index]\n",
        "        train_losses.append(curr_train_loss_total)\n",
        "\n",
        "        curr_validation_loss_total = history.history['val_loss'][min_index]\n",
        "        curr_validation_loss_extreme = history.history['val_extreme_loss'][\n",
        "            min_index]\n",
        "        curr_validation_loss_up_down = history.history['val_up_down_loss'][\n",
        "            min_index]\n",
        "\n",
        "        # 5.6 valutalo su test set\n",
        "        current_test_loss = model.evaluate(X_test, y_test_cat, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        curr_test_loss_total, curr_test_loss_extreme, curr_test_loss_up_down = current_test_loss\n",
        "        test_losses.append(curr_test_loss_total)\n",
        "\n",
        "        # 5.7 salva train, validation e test set performance\n",
        "        # train\n",
        "        print(\"Evaluating the model on the training set\")\n",
        "        probs = model.predict(X_train_bal, batch_size=batch_size)\n",
        "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
        "        \n",
        "        y_true_extreme, y_true_up_down = y_train_bal\n",
        "\n",
        "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
        "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
        "\n",
        "        row = pd.Series({\n",
        "            'dataset': 'train',\n",
        "            'optimizer': optimizer_name,\n",
        "            'start_time': start_time,\n",
        "            'experiment_id': experiment_id,\n",
        "            'trial': i,\n",
        "            'bptt': bptt,\n",
        "            'lr': learning_rate,\n",
        "            'n_layers': space['layers']['num_layers']['how_many'],\n",
        "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
        "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
        "            'dropout': space['layers']['input_dropout'],\n",
        "            'loss_extreme': curr_train_loss_extreme,\n",
        "            'loss_up_down': curr_train_loss_up_down,\n",
        "            'loss_volume': np.nan,\n",
        "            'f1_validation_extreme': np.nan,\n",
        "            'f1_validation_up_down': np.nan,\n",
        "            'tp_extreme': r_extreme['tp'],\n",
        "            'tn_extreme': r_extreme['tn'],\n",
        "            'fp_extreme': r_extreme['fp'],\n",
        "            'fn_extreme': r_extreme['fn'],\n",
        "            'tp_up_down': r_up_down['tp'],\n",
        "            'tn_up_down': r_up_down['tn'],\n",
        "            'fp_up_down': r_up_down['fp'],\n",
        "            'fn_up_down': r_up_down['fn'],\n",
        "            'train_epochs': train_epochs,\n",
        "            'es_min_delta': min_delta,\n",
        "            'es_patience': patience,\n",
        "            'use_class_weight': use_class_weight,\n",
        "            'output': sigmoid_or_softmax,\n",
        "        })\n",
        "        results = results.append(row, ignore_index=True)\n",
        "\n",
        "        # validation\n",
        "        print(\"Evaluating the model on the validation set\")\n",
        "        probs = model.predict(X_validation, batch_size=batch_size)\n",
        "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
        "        \n",
        "        y_true_extreme, y_true_up_down = y_validation\n",
        "        \n",
        "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
        "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
        "\n",
        "        # ora voglio ottimizzare rispetto all'F1 score della classe +1 -->\n",
        "        # utilizzo questo come metrica\n",
        "        f1_extreme = compute_performance(r_extreme['tp'], r_extreme['tn'],\n",
        "                                         r_extreme['fp'], r_extreme['fn'])\n",
        "        f1_up_down = compute_performance(r_up_down['tp'], r_up_down['tn'],\n",
        "                                         r_up_down['fp'], r_up_down['fn'])\n",
        "\n",
        "#         # se l'f1 è zero, allora l'esperimento è fallito\n",
        "#         if np.isnan(f1_extreme) or np.isnan(f1_up_down):\n",
        "#             return {\n",
        "#                 'status': STATUS_FAIL,\n",
        "#                 'loss': 1e+30,\n",
        "#             }\n",
        "\n",
        "        val_losses.append(curr_validation_loss_total)\n",
        "\n",
        "        row = pd.Series({\n",
        "            'dataset': 'validation',\n",
        "            'optimizer': optimizer_name,\n",
        "            'start_time': start_time,\n",
        "            'experiment_id': experiment_id,\n",
        "            'trial': i,\n",
        "            'bptt': bptt,\n",
        "            'lr': learning_rate,\n",
        "            'n_layers': space['layers']['num_layers']['how_many'],\n",
        "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
        "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
        "            'dropout': space['layers']['input_dropout'],\n",
        "            'loss_extreme': curr_validation_loss_extreme,\n",
        "            'loss_up_down': curr_validation_loss_up_down,\n",
        "            'loss_volume': np.nan,\n",
        "            'f1_validation_extreme': f1_extreme,\n",
        "            'f1_validation_up_down': f1_up_down,\n",
        "            'tp_extreme': r_extreme['tp'],\n",
        "            'tn_extreme': r_extreme['tn'],\n",
        "            'fp_extreme': r_extreme['fp'],\n",
        "            'fn_extreme': r_extreme['fn'],\n",
        "            'tp_up_down': r_up_down['tp'],\n",
        "            'tn_up_down': r_up_down['tn'],\n",
        "            'fp_up_down': r_up_down['fp'],\n",
        "            'fn_up_down': r_up_down['fn'],\n",
        "            'train_epochs': train_epochs,\n",
        "            'es_min_delta': min_delta,\n",
        "            'es_patience': patience,\n",
        "            'use_class_weight': use_class_weight,\n",
        "            'output': sigmoid_or_softmax,\n",
        "        })\n",
        "        results = results.append(row, ignore_index=True)\n",
        "\n",
        "        # testing\n",
        "        print(\"Evaluating the model on the test set\")\n",
        "        probs = model.predict(X_test, batch_size=batch_size)\n",
        "        y_pred_extreme, y_pred_up_down = [np.argmax(p, axis=1) for p in probs]\n",
        "        \n",
        "        y_true_extreme, y_true_up_down = y_test\n",
        "\n",
        "        r_extreme = get_tptnfpfn(y_true_extreme, y_pred_extreme)\n",
        "        r_up_down = get_tptnfpfn(y_true_up_down, y_pred_up_down)\n",
        "\n",
        "        row = pd.Series({\n",
        "            'dataset': 'test',\n",
        "            'optimizer': optimizer_name,\n",
        "            'start_time': start_time,\n",
        "            'experiment_id': experiment_id,\n",
        "            'trial': i,\n",
        "            'bptt': bptt,\n",
        "            'lr': learning_rate,\n",
        "            'n_layers': space['layers']['num_layers']['how_many'],\n",
        "            'n_cells_1': space['layers']['num_layers']['n_cells_1'],\n",
        "            'n_cells_2': space['layers']['num_layers']['n_cells_2'],\n",
        "            'dropout': space['layers']['input_dropout'],\n",
        "            'loss_extreme': curr_test_loss_extreme,\n",
        "            'loss_up_down': curr_test_loss_up_down,\n",
        "            'loss_volume': np.nan,\n",
        "            'f1_validation_extreme': np.nan,\n",
        "            'f1_validation_up_down': np.nan,\n",
        "            'tp_extreme': r_extreme['tp'],\n",
        "            'tn_extreme': r_extreme['tn'],\n",
        "            'fp_extreme': r_extreme['fp'],\n",
        "            'fn_extreme': r_extreme['fn'],\n",
        "            'tp_up_down': r_up_down['tp'],\n",
        "            'tn_up_down': r_up_down['tn'],\n",
        "            'fp_up_down': r_up_down['fp'],\n",
        "            'fn_up_down': r_up_down['fn'],\n",
        "            'train_epochs': train_epochs,\n",
        "            'es_min_delta': min_delta,\n",
        "            'es_patience': patience,\n",
        "            'use_class_weight': use_class_weight,\n",
        "            'output': sigmoid_or_softmax,\n",
        "        })\n",
        "        results = results.append(row, ignore_index=True)\n",
        "\n",
        "        # scrivo sul CSV\n",
        "        print(\"Scrivo i risultati\")\n",
        "        results.to_csv(results_path, index=None)\n",
        "\n",
        "        # cancello la memoria\n",
        "        keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    # 6. media delle loss\n",
        "    train_loss_mean = np.mean(train_losses)\n",
        "    train_loss_variance = np.var(train_losses)\n",
        "\n",
        "    val_loss_mean = np.mean(val_losses)\n",
        "    val_loss_variance = np.var(val_losses)\n",
        "\n",
        "    test_loss_mean = np.mean(test_losses)\n",
        "    test_loss_variance = np.var(test_losses)\n",
        "\n",
        "    # 7. return del dict di loss\n",
        "    return {\n",
        "        'status': STATUS_OK,\n",
        "        'loss': val_loss_mean,\n",
        "        'loss_variance': val_loss_variance,\n",
        "        'true_loss': test_loss_mean,\n",
        "        'true_loss_variance': test_loss_variance,\n",
        "        'train_loss_mean': train_loss_mean,\n",
        "        'train_loss_variance': train_loss_variance,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T_tVJY9rOH66"
      },
      "source": [
        "### 3.1 Azione con minima volatilità - ottimizzazione iperparametri\n",
        "\n",
        "Cominciamo con l'azione meno volatile. Per prima cosa, bisogna trovare la struttura migliore della rete, quindi usiamo l'ottimizzazione degli iperparametri per farlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BxKDWjIAqGXG",
        "colab": {}
      },
      "source": [
        "s_type = 'min_vol'\n",
        "results_path = os.path.join(base_path, 'results', f\"results_{s_type}_with_sentiment.csv\")\n",
        "print(f\"RESULTS path: {results_path}\")\n",
        "\n",
        "def objective_fn(space, **kwargs):\n",
        "    return run_experiment(\n",
        "        space,\n",
        "        stock_type=s_type,\n",
        "        max_epochs=MAX_EPOCHS,\n",
        "        data=data_all[s_type],\n",
        "        extremes=extremes_all[s_type]['95'],\n",
        "        directions=directions_all[s_type],\n",
        "        n_runs=5,\n",
        "        verbose=0,\n",
        "        results_path=results_path\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-RDEDjiqOH69"
      },
      "source": [
        "Ora creo lo spazio di ricerca per questa azione:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uEMQYlzQOH6-",
        "colab": {}
      },
      "source": [
        "optimizer_space = hp.choice('opt_name', [\n",
        "    {\n",
        "        'name': 'adam',\n",
        "        'lr': hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
        "    },\n",
        "   {\n",
        "       'name': 'adadelta',\n",
        "   },\n",
        "])\n",
        "\n",
        "layer_space = {\n",
        "    'num_layers': {\n",
        "        'how_many': 2,\n",
        "        'n_cells_1': 21, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
        "        'n_cells_2': hp.quniform('number_of_cells_2', low=5, high=20, q=1),\n",
        "    },\n",
        "    'input_dropout': hp.uniform('dropout_kill_rate', low=0.0, high=0.4),\n",
        "}\n",
        "\n",
        "early_stop_space = {\n",
        "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
        "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
        "}\n",
        "\n",
        "opt_space = {\n",
        "    'optimizer': optimizer_space,\n",
        "    'layers': layer_space,\n",
        "    'bptt': hp.quniform('bptt_len', low=10, high=120, q=1),\n",
        "    'early_stop': early_stop_space,\n",
        "    'use_class_weight': False,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y3bOciYgOH7A"
      },
      "source": [
        "Ora facciamo la ricerca degli iperparametri:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UtmthHGGOH7B",
        "colab": {}
      },
      "source": [
        "print(\"COMINCIAMO\\n\")\n",
        "n_trials = 50\n",
        "\n",
        "load_trials = False\n",
        "trials_filename = os.path.join(base_path, 'results', f\"trials_{s_type}_with_sentiment_2_layers.pickle\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_with_sentiment_2_layers.pickle\")\n",
        "\n",
        "if load_trials:\n",
        "    print(\"Loading trials from last execution\")\n",
        "    try:\n",
        "        with open(trials_filename, 'rb') as infile:\n",
        "            trials = pickle.load(infile)\n",
        "            print(f\"len(trials) = {len(trials)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Trials file not found, starting with new trial object\")\n",
        "        trials = hy.Trials()\n",
        "else:\n",
        "    print(\"Starting new experiment with new trials object\")\n",
        "    trials = hy.Trials()\n",
        "\n",
        "\n",
        "best = fmin(\n",
        "    objective_fn,\n",
        "    opt_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=len(trials) + n_trials,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "with open(trials_filename, 'wb') as outfile:\n",
        "    pickle.dump(trials, outfile)\n",
        "\n",
        "with open(best_filename, 'wb') as outfile:\n",
        "    pickle.dump(best, outfile)\n",
        "\n",
        "print(\"FINITO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okVkBuNFqGXN"
      },
      "source": [
        "### 3.3 Azione massima volatilità - ottimizzazione iperparametri\n",
        "\n",
        "Ora, facciamo la stessa cosa per l'azione con più volatilità. Ricorda che qui stiamo solo cercando la configurazione ottimale della rete e degli iperparametri, mentre il vero training-testing lo farò in un altro notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98LZmDXv__8c",
        "colab": {}
      },
      "source": [
        "s_type = 'max_vol'\n",
        "results_path = os.path.join(base_path, 'results', f\"results_{s_type}_with_sentiment.csv\")\n",
        "print(f\"RESULTS path: {results_path}\")\n",
        "\n",
        "def objective_fn(space, **kwargs):\n",
        "    return run_experiment(\n",
        "        space,\n",
        "        stock_type=s_type,\n",
        "        max_epochs=MAX_EPOCHS,\n",
        "        data=data_all[s_type],\n",
        "        extremes=extremes_all[s_type]['95'],\n",
        "        directions=directions_all[s_type],\n",
        "        n_runs=5,\n",
        "        verbose=0,\n",
        "        results_path=results_path\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MyxjqnBnqGXT",
        "colab": {}
      },
      "source": [
        "optimizer_space = hp.choice('opt_name', [\n",
        "    {\n",
        "        'name': 'adam',\n",
        "        'lr': hp.uniform('lr_adam', low=1e-5, high=1e-2)\n",
        "    },\n",
        "   {\n",
        "       'name': 'adadelta',\n",
        "   },\n",
        "])\n",
        "\n",
        "layer_space = {\n",
        "    'num_layers': {\n",
        "        'how_many': 2,\n",
        "        'n_cells_1': 21, # hp.quniform('number_of_cells', low=10, high=100, q=2), # 96\n",
        "        'n_cells_2': hp.quniform('number_of_cells_2', low=5, high=20, q=1),\n",
        "    },\n",
        "    'input_dropout': hp.uniform('dropout_kill_rate', low=0.0, high=0.4),\n",
        "}\n",
        "\n",
        "early_stop_space = {\n",
        "    'patience': hp.quniform('early_stop_patience', low=5, high=25, q=1),\n",
        "    'min_delta': hp.quniform('early_stop_min_delta', low=1e-4, high=1e-2, q=2e-4)\n",
        "}\n",
        "\n",
        "opt_space = {\n",
        "    'optimizer': optimizer_space,\n",
        "    'layers': layer_space,\n",
        "    'bptt': hp.quniform('bptt_len', low=10, high=120, q=1),\n",
        "    'early_stop': early_stop_space,\n",
        "    'use_class_weight': False,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ErGLxD7fAPRF",
        "colab": {}
      },
      "source": [
        "print(\"COMINCIAMO\\n\")\n",
        "n_trials = 50\n",
        "\n",
        "load_trials = True\n",
        "trials_filename = os.path.join(base_path, 'results', f\"trials_{s_type}_with_sentiment_2_layers.pickle\")\n",
        "best_filename = os.path.join(base_path, 'results', f\"best_{s_type}_with_sentiment_2_layers.pickle\")\n",
        "\n",
        "if load_trials:\n",
        "    print(\"Loading trials from last execution\")\n",
        "    try:\n",
        "        with open(trials_filename, 'rb') as infile:\n",
        "            trials = pickle.load(infile)\n",
        "            print(f\"len(trials) = {len(trials)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Trials file not found, starting with new trial object\")\n",
        "        trials = hy.Trials()\n",
        "else:\n",
        "    print(\"Starting new experiment with new trials object\")\n",
        "    trials = hy.Trials()\n",
        "\n",
        "\n",
        "best = fmin(\n",
        "    objective_fn,\n",
        "    opt_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=len(trials) + n_trials,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "with open(trials_filename, 'wb') as outfile:\n",
        "    pickle.dump(trials, outfile)\n",
        "\n",
        "with open(best_filename, 'wb') as outfile:\n",
        "    pickle.dump(best, outfile)\n",
        "\n",
        "print(\"FINITO\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TzTD8h5jAaL6"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P5gX8eBRZeU4"
      },
      "source": [
        "Ora vedo come variare il threshold per ottenere le curve ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5nXyquVVsRW",
        "colab": {}
      },
      "source": [
        "def loss_function(theta, recall, fpr):\n",
        "    \"\"\"The loss function L = theta * (1 - recall) + (1 - theta) * fpr\"\"\"\n",
        "    assert theta >= 0.0 and theta <= 1.0\n",
        "    \n",
        "    return theta * (1 - recall) + (1 - theta) * fpr\n",
        "\n",
        "\n",
        "def utility_function(theta, loss):\n",
        "    \"\"\"The utility function U = min(theta, 1 - theta) - loss\"\"\"\n",
        "    return min(theta, 1 - theta) - loss\n",
        "\n",
        "\n",
        "def to_binary(prob: np.ndarray, thresh: float):\n",
        "    assert thresh <= 1.0 and thresh >= 0.0\n",
        "    \n",
        "    return (prob >= thresh).astype(np.int8)\n",
        "\n",
        "\n",
        "def recall_fpr_kss_precision(y_true, y_pred):\n",
        "    \"\"\"Compute recall, fpr and KSS score.\"\"\"\n",
        "    tp = np.sum(np.logical_and(y_true, y_pred))\n",
        "    tn = np.sum(np.logical_and(\n",
        "        np.logical_not(y_true),\n",
        "        np.logical_not(y_pred)\n",
        "    ))\n",
        "    fp = np.sum(np.logical_and(\n",
        "        np.logical_not(y_true),\n",
        "        y_pred\n",
        "    ))\n",
        "    fn = np.sum(np.logical_and(\n",
        "        y_true,\n",
        "        np.logical_not(y_pred)\n",
        "    ))\n",
        "    \n",
        "    recall = tp / (tp + fn)  # TP / (TP + FN)\n",
        "    fpr = fp / (fp + tn)  # FP / (FP + TN)\n",
        "    precision = tp / (tp + fp)\n",
        "    \n",
        "    kss = recall - fpr\n",
        "    \n",
        "    return recall, fpr, kss, precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s0ylM6MWZjHg",
        "colab": {}
      },
      "source": [
        "w_t = np.arange(0, 1, 1e-3)\n",
        "theta = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fBtZN0LrZ4_R",
        "colab": {}
      },
      "source": [
        "def optimize_wt(w, theta, probabilities, y_true, verbose=False):\n",
        "    \"\"\"Get the best threshold for the class 1 probability.\"\"\"\n",
        "    recalls = np.zeros((w.shape[0], ), dtype=np.float64)\n",
        "    fprs = copy.deepcopy(recalls)\n",
        "    ksss = copy.deepcopy(recalls)\n",
        "    precisions = copy.deepcopy(recalls)\n",
        "    losses = copy.deepcopy(recalls)\n",
        "    utilities = copy.deepcopy(recalls)\n",
        "\n",
        "    for i, thresh in enumerate(w):\n",
        "        if i % 200 == 0 and verbose:\n",
        "            print(f\"iteration {i} / {len(w_t)}\")\n",
        "\n",
        "        y_pred = to_binary(probabilities, thresh).astype(np.int8)\n",
        "        recall, fpr, kss, precision = recall_fpr_kss_precision(y_true, y_pred)\n",
        "        loss = loss_function(theta, recall, fpr)\n",
        "        utility = utility_function(theta, loss)\n",
        "\n",
        "        recalls[i] = recall\n",
        "        precisions[i] = precision\n",
        "        ksss[i] = kss\n",
        "        fprs[i] = fpr\n",
        "        losses[i] = loss\n",
        "        utilities[i] = utility\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Finished!\")\n",
        "\n",
        "    return recalls, fprs, ksss, precisions, losses, utilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_i6fKRXteWS0",
        "colab": {}
      },
      "source": [
        "probabilities_extreme_train = model.predict(X_train, batch_size=batch_size)[0][:, 1]\n",
        "probabilities_up_down_train = model.predict(X_train, batch_size=batch_size)[1][:, 1]\n",
        "\n",
        "probabilities_extreme_validation = model.predict(X_validation, batch_size=batch_size)[0][:, 1]\n",
        "probabilities_up_down_validation = model.predict(X_validation, batch_size=batch_size)[1][:, 1]\n",
        "\n",
        "# per le ROC sugli estremi\n",
        "recalls_train, fprs_train, ksss_train, precisions_train, losses_train, utilities_train = \\\n",
        "optimize_wt(w_t, theta, probabilities_extreme_train, y_train[0].astype(np.int8))\n",
        "\n",
        "recalls_validation, fprs_validation, ksss_validation, \\\n",
        "precisions_validation, losses_validation, utilities_validation = \\\n",
        "optimize_wt(w_t, theta, probabilities_extreme_validation, y_validation[0].astype(np.int8))\n",
        "\n",
        "# per le ROC sul su-giù\n",
        "recalls_train_ud, fprs_train_ud, ksss_train_ud, precisions_train_ud, losses_train_ud, utilities_train_ud = \\\n",
        "optimize_wt(w_t, theta, probabilities_up_down_train, y_train[1].astype(np.int8))\n",
        "\n",
        "recalls_validation_ud, fprs_validation_ud, ksss_validation_ud, \\\n",
        "precisions_validation_ud, losses_validation_ud, utilities_validation_ud = \\\n",
        "optimize_wt(w_t, theta, probabilities_up_down_validation, y_validation[1].astype(np.int8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcQpqDonZ9BB",
        "colab": {}
      },
      "source": [
        "fig, ax = pl.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
        "fig.suptitle(f\"{s_type} stock\", fontsize=16)\n",
        "\n",
        "# primo plot: EXTREMES\n",
        "# train set\n",
        "i_sorted = np.argsort(fprs_train)\n",
        "\n",
        "x_extreme = fprs_train[i_sorted]\n",
        "y_extreme = recalls_train[i_sorted]\n",
        "ax[0].plot(\n",
        "    x_extreme,\n",
        "    y_extreme,\n",
        "    color='navy',\n",
        "    label='train'\n",
        ")\n",
        "\n",
        "i_sweet = np.argmax(utilities_train)\n",
        "best_x = fprs_train[i_sweet]\n",
        "best_y = recalls_train[i_sweet]\n",
        "\n",
        "ax[0].plot(\n",
        "    best_x,\n",
        "    best_y,\n",
        "    marker='s',\n",
        "    markersize=5,\n",
        "    color='navy',\n",
        "    label='train - best'\n",
        ")\n",
        "\n",
        "# validation set\n",
        "i_sorted = np.argsort(fprs_validation)\n",
        "\n",
        "x = fprs_validation[i_sorted]\n",
        "y = recalls_validation[i_sorted]\n",
        "ax[0].plot(\n",
        "    x,\n",
        "    y,\n",
        "    color='forestgreen',\n",
        "    label='validation'\n",
        ")\n",
        "\n",
        "i_sweet = np.argmax(utilities_validation)\n",
        "best_x = fprs_validation[i_sweet]\n",
        "best_y = recalls_validation[i_sweet]\n",
        "\n",
        "ax[0].plot(\n",
        "    best_x,\n",
        "    best_y,\n",
        "    marker='s',\n",
        "    markersize=5,\n",
        "    color='forestgreen',\n",
        "    label='validation - best'\n",
        ")\n",
        "\n",
        "ax[0].set_title(\"Extreme prediction\")\n",
        "\n",
        "# ------------------------------------------------------- #\n",
        "# secondo plot: UP-DOWN\n",
        "# train set\n",
        "i_sorted = np.argsort(fprs_train_ud)\n",
        "\n",
        "x = fprs_train_ud[i_sorted]\n",
        "y = recalls_train_ud[i_sorted]\n",
        "ax[1].plot(\n",
        "    x,\n",
        "    y,\n",
        "    color='navy',\n",
        "    label='train'\n",
        ")\n",
        "\n",
        "i_sweet = np.argmax(utilities_train)\n",
        "best_x = fprs_train_ud[i_sweet]\n",
        "best_y = recalls_train_ud[i_sweet]\n",
        "\n",
        "ax[1].plot(\n",
        "    best_x,\n",
        "    best_y,\n",
        "    marker='s',\n",
        "    markersize=5,\n",
        "    color='navy',\n",
        "    label='train - best'\n",
        ")\n",
        "\n",
        "# validation set\n",
        "i_sorted = np.argsort(fprs_validation_ud)\n",
        "\n",
        "x = fprs_validation_ud[i_sorted]\n",
        "y = recalls_validation_ud[i_sorted]\n",
        "ax[1].plot(\n",
        "    x,\n",
        "    y,\n",
        "    color='forestgreen',\n",
        "    label='validation'\n",
        ")\n",
        "\n",
        "i_sweet = np.argmax(utilities_validation)\n",
        "best_x = fprs_validation_ud[i_sweet]\n",
        "best_y = recalls_validation_ud[i_sweet]\n",
        "\n",
        "ax[1].plot(\n",
        "    best_x,\n",
        "    best_y,\n",
        "    marker='s',\n",
        "    markersize=5,\n",
        "    color='forestgreen',\n",
        "    label='validation - best'\n",
        ")\n",
        "\n",
        "ax[1].set_title(\"Up-Down prediction\")\n",
        "\n",
        "# la linea del random classifier\n",
        "for a in ax:\n",
        "    a.plot([0, 1], [0, 1], color='black', linewidth=0.5)\n",
        "    a.legend(loc='lower right', fontsize=14)\n",
        "    a.set_xlim([0, 1.1])\n",
        "    a.set_ylim([0, 1.1])\n",
        "    a.set_xlabel('FPR', fontsize=16)\n",
        "    a.set_ylabel('Recall', fontsize=16)\n",
        "\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}